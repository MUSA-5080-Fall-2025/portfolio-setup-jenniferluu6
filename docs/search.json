[
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html",
    "href": "weekly-notes/week-07-notes.html",
    "title": "Week 7 notes",
    "section": "",
    "text": "The (n-1) Rule: One neighborhood is automatically chosen as the reference category (omitted)! R picks the first alphabetically unless you specify otherwise.\n\n\nReference Category: Allston (automatically chosen - alphabetically first) Structural Variables: - Living Area: Each additional sq ft adds this amount (same for all neighborhoods) - Bedrooms: Effect of one more full bathroom (same for all neighborhoods) Neighborhood Dummies: - Positive coefficient = This neighborhood is MORE expensive than r ref_neighborhood - Negative coefficient = This neighborhood is LESS expensive than r ref_neighborhood - All else equal (same size, same bathrooms)\nSigns of Non-Linearity:\n\nCurved residual plots\nDiminishing returns\nAccelerating effects\nU-shaped or inverted-U patterns\nTheoretical reasons\n\n⚠️ Can’t Interpret Coefficients Directly!\nWith Age², the effect of age is no longer constant. You need to calculate the marginal effect. Marginal effect of Age = β₁ + 2×β₂×Age This means the effect changes at every age!\n\n\n\n\n\n\n\n\n\nNote“Everything is related to everything else, but near things are more related than distant things”\n\n\n\n- Waldo Tobler, 1970\n\n\n\nCrime nearby matters more than crime across the city\nParks within walking distance affect value\nYour immediate neighborhood defines your market\n\n\n\n\n\n\nCount or sum events within a defined distance\nExample: Number of crimes within 500 feet\n\n\n\nAverage distance to k closest events\nExample: Average distance to 3 nearest violent crimes\n\n\n\nStraight-line distance to important locations\nExample: Distance to downtown, nearest T station Fixed Effects = Categorical variables that capture all unmeasured characteristics of a group\nIn hedonic models:\n\nEach neighborhood gets its own dummy variable\nCaptures everything unique about that neighborhood we didn’t explicitly measure\n\nThree common validation approaches:\n\nTrain/Test Split - 80/20 split, simple but unstable\nk-Fold Cross-Validation - Split into k folds, train on k-1, test on 1, repeat\nLOOCV - Leave one observation out at a time (special case of k-fold)\n\nWhy CV?\n\nTells us how well model predicts NEW data\nMore honest than in-sample R²\nHelps detect overfitting\n\n\n\n\n\n\n\nWhat happened?\n\nRandom split created 10 folds\nAll “West End” sales ended up in ONE fold (the test fold)\nTraining folds never saw “West End”\nModel can’t predict for a category it never learned\n\n\n\n\n\n\n\nImportantThe Issue\n\n\n\nWhen neighborhoods have very few sales (&lt;10), random CV splits can put all instances in the same fold, breaking the model.\n\n\n\n\n\n4326 = WGS84 (lat/lon in degrees) - fine for display\nESRI:102286 = MA State Plane (feet) - good for distance calculations\n\n\n\n\n\n\n\n\nAdd 50 variables at once\nIgnore errors\nForget st_drop_geometry() for non-spatial operations\nSkip sparse category check\n\n\n\n\n\n\n\n\n\n“Factor has new levels” → Group sparse categories\n“Computationally singular” → Remove collinear variables\nVery high RMSE → Check outliers, scale\nCV takes forever → Simplify model or reduce folds\nNegative R² → Model worse than mean, rethink variables\n\n\n\nExample Scenarios: - Housing: Does square footage matter more in wealthy neighborhoods? - Education: Do tutoring effects vary by initial skill level? - Public Health: Do pollution effects differ by age?"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-07-notes.html#key-concepts-learned",
    "title": "Week 7 notes",
    "section": "",
    "text": "The (n-1) Rule: One neighborhood is automatically chosen as the reference category (omitted)! R picks the first alphabetically unless you specify otherwise.\n\n\nReference Category: Allston (automatically chosen - alphabetically first) Structural Variables: - Living Area: Each additional sq ft adds this amount (same for all neighborhoods) - Bedrooms: Effect of one more full bathroom (same for all neighborhoods) Neighborhood Dummies: - Positive coefficient = This neighborhood is MORE expensive than r ref_neighborhood - Negative coefficient = This neighborhood is LESS expensive than r ref_neighborhood - All else equal (same size, same bathrooms)\nSigns of Non-Linearity:\n\nCurved residual plots\nDiminishing returns\nAccelerating effects\nU-shaped or inverted-U patterns\nTheoretical reasons\n\n⚠️ Can’t Interpret Coefficients Directly!\nWith Age², the effect of age is no longer constant. You need to calculate the marginal effect. Marginal effect of Age = β₁ + 2×β₂×Age This means the effect changes at every age!\n\n\n\n\n\n\n\n\n\nNote“Everything is related to everything else, but near things are more related than distant things”\n\n\n\n- Waldo Tobler, 1970\n\n\n\nCrime nearby matters more than crime across the city\nParks within walking distance affect value\nYour immediate neighborhood defines your market\n\n\n\n\n\n\nCount or sum events within a defined distance\nExample: Number of crimes within 500 feet\n\n\n\nAverage distance to k closest events\nExample: Average distance to 3 nearest violent crimes\n\n\n\nStraight-line distance to important locations\nExample: Distance to downtown, nearest T station Fixed Effects = Categorical variables that capture all unmeasured characteristics of a group\nIn hedonic models:\n\nEach neighborhood gets its own dummy variable\nCaptures everything unique about that neighborhood we didn’t explicitly measure\n\nThree common validation approaches:\n\nTrain/Test Split - 80/20 split, simple but unstable\nk-Fold Cross-Validation - Split into k folds, train on k-1, test on 1, repeat\nLOOCV - Leave one observation out at a time (special case of k-fold)\n\nWhy CV?\n\nTells us how well model predicts NEW data\nMore honest than in-sample R²\nHelps detect overfitting\n\n\n\n\n\n\n\nWhat happened?\n\nRandom split created 10 folds\nAll “West End” sales ended up in ONE fold (the test fold)\nTraining folds never saw “West End”\nModel can’t predict for a category it never learned\n\n\n\n\n\n\n\nImportantThe Issue\n\n\n\nWhen neighborhoods have very few sales (&lt;10), random CV splits can put all instances in the same fold, breaking the model.\n\n\n\n\n\n4326 = WGS84 (lat/lon in degrees) - fine for display\nESRI:102286 = MA State Plane (feet) - good for distance calculations\n\n\n\n\n\n\n\n\nAdd 50 variables at once\nIgnore errors\nForget st_drop_geometry() for non-spatial operations\nSkip sparse category check\n\n\n\n\n\n\n\n\n\n“Factor has new levels” → Group sparse categories\n“Computationally singular” → Remove collinear variables\nVery high RMSE → Check outliers, scale\nCV takes forever → Simplify model or reduce folds\nNegative R² → Model worse than mean, rethink variables\n\n\n\nExample Scenarios: - Housing: Does square footage matter more in wealthy neighborhoods? - Education: Do tutoring effects vary by initial skill level? - Public Health: Do pollution effects differ by age?"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#three-approaches-to-spatial-features",
    "href": "weekly-notes/week-07-notes.html#three-approaches-to-spatial-features",
    "title": "Week 7 notes",
    "section": "",
    "text": "Count or sum events within a defined distance\nExample: Number of crimes within 500 feet\n\n\n\nAverage distance to k closest events\nExample: Average distance to 3 nearest violent crimes\n\n\n\nStraight-line distance to important locations\nExample: Distance to downtown, nearest T station Fixed Effects = Categorical variables that capture all unmeasured characteristics of a group\nIn hedonic models:\n\nEach neighborhood gets its own dummy variable\nCaptures everything unique about that neighborhood we didn’t explicitly measure\n\nThree common validation approaches:\n\nTrain/Test Split - 80/20 split, simple but unstable\nk-Fold Cross-Validation - Split into k folds, train on k-1, test on 1, repeat\nLOOCV - Leave one observation out at a time (special case of k-fold)\n\nWhy CV?\n\nTells us how well model predicts NEW data\nMore honest than in-sample R²\nHelps detect overfitting"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#the-problem-sparse-categories",
    "href": "weekly-notes/week-07-notes.html#the-problem-sparse-categories",
    "title": "Week 7 notes",
    "section": "",
    "text": "What happened?\n\nRandom split created 10 folds\nAll “West End” sales ended up in ONE fold (the test fold)\nTraining folds never saw “West End”\nModel can’t predict for a category it never learned\n\n\n\n\n\n\n\nImportantThe Issue\n\n\n\nWhen neighborhoods have very few sales (&lt;10), random CV splits can put all instances in the same fold, breaking the model.\n\n\n\n\n\n4326 = WGS84 (lat/lon in degrees) - fine for display\nESRI:102286 = MA State Plane (feet) - good for distance calculations\n\n\n\n\n\n\n\n\nAdd 50 variables at once\nIgnore errors\nForget st_drop_geometry() for non-spatial operations\nSkip sparse category check\n\n\n\n\n\n\n\n\n\n“Factor has new levels” → Group sparse categories\n“Computationally singular” → Remove collinear variables\nVery high RMSE → Check outliers, scale\nCV takes forever → Simplify model or reduce folds\nNegative R² → Model worse than mean, rethink variables\n\n\n\nExample Scenarios: - Housing: Does square footage matter more in wealthy neighborhoods? - Education: Do tutoring effects vary by initial skill level? - Public Health: Do pollution effects differ by age?"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#coding-techniques",
    "href": "weekly-notes/week-07-notes.html#coding-techniques",
    "title": "Week 7 notes",
    "section": "",
    "text": "4326 = WGS84 (lat/lon in degrees) - fine for display\nESRI:102286 = MA State Plane (feet) - good for distance calculations"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#questions-challenges",
    "href": "weekly-notes/week-07-notes.html#questions-challenges",
    "title": "Week 7 notes",
    "section": "",
    "text": "Add 50 variables at once\nIgnore errors\nForget st_drop_geometry() for non-spatial operations\nSkip sparse category check"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#reflection",
    "href": "weekly-notes/week-07-notes.html#reflection",
    "title": "Week 7 notes",
    "section": "",
    "text": "Example Scenarios: - Housing: Does square footage matter more in wealthy neighborhoods? - Education: Do tutoring effects vary by initial skill level? - Public Health: Do pollution effects differ by age?"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 4 notes",
    "section": "",
    "text": "Geographic bias in algorithms:\n\nTraining data may under-represent certain areas\nSpatial autocorrelation violates independence assumptions\nService delivery algorithms may reinforce geographic inequities\n\n\n\n\n\n\nThree basic types:\n\nPoints → Locations (schools, hospitals, crime incidents)\nLines → Linear features (roads, rivers, transit routes)\n\nPolygons → Areas (census tracts, neighborhoods, service areas)\n\nEach feature has:\n\nGeometry → Shape and location\nAttributes → Data about that feature (population, income, etc.)\n\nKey principle: Spatial data is just data.frame + geometry column\nCommon spatial data formats:\n\nShapefiles (.shp + supporting files)\nGeoJSON (.geojson)\nKML/KMZ (Google Earth)\nDatabase connections (PostGIS)\n\n\n\n\nThe Earth is round, maps are flat\nProblems:\n\nCan’t preserve area, distance, and angles simultaneously\nDifferent projections optimize different properties\nWrong projection → wrong analysis results!\n\nExample: Measuring areas in latitude/longitude gives wrong answers\n\n\n\nGeographic Coordinate Systems (GCS):\n\nLatitude/longitude coordinates\nUnits: decimal degrees\nGood for: Global datasets, web mapping\nBad for: Area/distance calculations\n\nProjected Coordinate Systems (PCS):\n\nX/Y coordinates on a flat plane\nUnits: meters, feet, etc.\nGood for: Local analysis, accurate measurements\nBad for: Large areas, global datasets\n\n\n\n\nWGS84 (EPSG:4326)\n\nGPS standard, global coverage\nGeographic system (lat/lon)\nGood for web mapping, data sharing\n\nWeb Mercator (EPSG:3857)\n\nWeb mapping standard\nProjected system\nHeavily distorts areas near poles\n\nState Plane / UTM zones\n\nLocal accuracy\nDifferent zones for different regions\nOptimized for specific geographic areas\n\nAlbers Equal Area\n\nPreserves area\nGood for demographic/statistical analysis\n\n\n\n\nThe .predicate tells st_filter() what spatial relationship to look for:\nr predicate-structure # Basic structure: st_filter(data_to_filter, reference_geometry, .predicate = relationship)\nDifferent questions need different relationships:\n\n“Which counties border Allegheny?” → st_touches\n“Which tracts are IN Allegheny?” → st_within\n\n“Which tracts overlap a metro area?” → st_intersects\n\nDefault: If no .predicate specified, uses st_intersects\n\n\n\n\n\n\n\n\n\n\n\nPredicate\nDefinition\nPolicy Use Case\n\n\n\n\nst_intersects()\nAny overlap at all\n“Counties affected by flooding”\n\n\nst_touches()\nShare boundary, no interior overlap\n“Neighboring counties”\n\n\nst_within()\nCompletely inside\n“Schools within district boundaries”\n\n\nst_contains()\nCompletely contains\n“Districts containing hospitals”\n\n\nst_overlaps()\nPartial overlap\n“Overlapping service areas”\n\n\nst_disjoint()\nNo spatial relationship\n“Counties separate from urban areas”\n\n\n\nMost common: st_intersects() (any overlap) and st_touches() (neighbors)\nImportant: Units depend on coordinate reference system!\nThe Key Difference st_filter() with predicates: Selects complete features (keeps or removes entire rows)\nst_intersection() and st_union(): Modifies geometries (creates new shapes)"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "title": "Week 4 notes",
    "section": "",
    "text": "Geographic bias in algorithms:\n\nTraining data may under-represent certain areas\nSpatial autocorrelation violates independence assumptions\nService delivery algorithms may reinforce geographic inequities\n\n\n\n\n\n\nThree basic types:\n\nPoints → Locations (schools, hospitals, crime incidents)\nLines → Linear features (roads, rivers, transit routes)\n\nPolygons → Areas (census tracts, neighborhoods, service areas)\n\nEach feature has:\n\nGeometry → Shape and location\nAttributes → Data about that feature (population, income, etc.)\n\nKey principle: Spatial data is just data.frame + geometry column\nCommon spatial data formats:\n\nShapefiles (.shp + supporting files)\nGeoJSON (.geojson)\nKML/KMZ (Google Earth)\nDatabase connections (PostGIS)\n\n\n\n\nThe Earth is round, maps are flat\nProblems:\n\nCan’t preserve area, distance, and angles simultaneously\nDifferent projections optimize different properties\nWrong projection → wrong analysis results!\n\nExample: Measuring areas in latitude/longitude gives wrong answers\n\n\n\nGeographic Coordinate Systems (GCS):\n\nLatitude/longitude coordinates\nUnits: decimal degrees\nGood for: Global datasets, web mapping\nBad for: Area/distance calculations\n\nProjected Coordinate Systems (PCS):\n\nX/Y coordinates on a flat plane\nUnits: meters, feet, etc.\nGood for: Local analysis, accurate measurements\nBad for: Large areas, global datasets\n\n\n\n\nWGS84 (EPSG:4326)\n\nGPS standard, global coverage\nGeographic system (lat/lon)\nGood for web mapping, data sharing\n\nWeb Mercator (EPSG:3857)\n\nWeb mapping standard\nProjected system\nHeavily distorts areas near poles\n\nState Plane / UTM zones\n\nLocal accuracy\nDifferent zones for different regions\nOptimized for specific geographic areas\n\nAlbers Equal Area\n\nPreserves area\nGood for demographic/statistical analysis\n\n\n\n\nThe .predicate tells st_filter() what spatial relationship to look for:\nr predicate-structure # Basic structure: st_filter(data_to_filter, reference_geometry, .predicate = relationship)\nDifferent questions need different relationships:\n\n“Which counties border Allegheny?” → st_touches\n“Which tracts are IN Allegheny?” → st_within\n\n“Which tracts overlap a metro area?” → st_intersects\n\nDefault: If no .predicate specified, uses st_intersects\n\n\n\n\n\n\n\n\n\n\n\nPredicate\nDefinition\nPolicy Use Case\n\n\n\n\nst_intersects()\nAny overlap at all\n“Counties affected by flooding”\n\n\nst_touches()\nShare boundary, no interior overlap\n“Neighboring counties”\n\n\nst_within()\nCompletely inside\n“Schools within district boundaries”\n\n\nst_contains()\nCompletely contains\n“Districts containing hospitals”\n\n\nst_overlaps()\nPartial overlap\n“Overlapping service areas”\n\n\nst_disjoint()\nNo spatial relationship\n“Counties separate from urban areas”\n\n\n\nMost common: st_intersects() (any overlap) and st_touches() (neighbors)\nImportant: Units depend on coordinate reference system!\nThe Key Difference st_filter() with predicates: Selects complete features (keeps or removes entire rows)\nst_intersection() and st_union(): Modifies geometries (creates new shapes)"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#questions-challenges",
    "href": "weekly-notes/week-04-notes.html#questions-challenges",
    "title": "Week 4 notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nA note about that (.)\nThe dot (.) is a placeholder that represents the data being passed through the pipe (%&gt;%).\npa_counties &lt;- pa_counties %&gt;% mutate( area_sqkm = as.numeric(st_area(.)) / 1000000 )\nThe . refers to pa_counties - the data frame being passed through the pipe. So this is equivalent to:\npa_counties &lt;- pa_counties %&gt;% mutate( area_sqkm = as.numeric(st_area(pa_counties)) / 1000000 )"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#connections-to-policy",
    "href": "weekly-notes/week-04-notes.html#connections-to-policy",
    "title": "Week 4 notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\nTypical spatial analysis steps:\n\nLoad data → Get spatial boundaries and attribute data\nCheck projections → Transform to appropriate CRS\nJoin datasets → Combine spatial and non-spatial data\nSpatial operations → Buffers, intersections, distance calculations\nAggregation → Summarize across spatial units\nVisualization → Maps and charts\nInterpretation → Policy recommendations\n\nUse st_filter() when:\n\n“Which census tracts touch hospital service areas?”\nYou want to select/identify features based on location\nYou need complete features with their original boundaries\nYou’re counting: “How many tracts are near hospitals?”\n\nUse st_intersection() when:\n\n“What is the area of overlap between tracts and service zones?”\nYou need to calculate areas, populations, or other measures within specific boundaries\nYou’re doing spatial overlay analysis\nYou need to clip data to a study area"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#reflection",
    "href": "weekly-notes/week-04-notes.html#reflection",
    "title": "Week 4 notes",
    "section": "Reflection",
    "text": "Reflection\nMajor takeaway is the focus in coordinate systems."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html",
    "href": "weekly-notes/week-05-notes.html",
    "title": "Week 5 notes",
    "section": "",
    "text": "The Statistical Learning Framework: What are we actually doing?\nTwo goals: Understanding relationships vs Making predictions\nBuilding your first model with PA census data\nModel evaluation: How do we know if it’s any good?\nChecking assumptions: When can we trust the model?\nImproving predictions: Transformations, multiple variables\n\nWe observe data: counties, income, population, education, etc.\nWe believe there’s some relationship between these variables."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-05-notes.html#key-concepts-learned",
    "title": "Week 5 notes",
    "section": "",
    "text": "The Statistical Learning Framework: What are we actually doing?\nTwo goals: Understanding relationships vs Making predictions\nBuilding your first model with PA census data\nModel evaluation: How do we know if it’s any good?\nChecking assumptions: When can we trust the model?\nImproving predictions: Transformations, multiple variables\n\nWe observe data: counties, income, population, education, etc.\nWe believe there’s some relationship between these variables."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#formalizing-the-relationship",
    "href": "weekly-notes/week-05-notes.html#formalizing-the-relationship",
    "title": "Week 5 notes",
    "section": "Formalizing the Relationship",
    "text": "Formalizing the Relationship\nFor any quantitative response Y and predictors X₁, X₂, … Xₚ:\n\\[Y = f(X) + \\epsilon\\]\nWhere:\n\nf = the systematic information X provides about Y\nε = random error (irreducible)"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#how-do-we-estimate-f",
    "href": "weekly-notes/week-05-notes.html#how-do-we-estimate-f",
    "title": "Week 5 notes",
    "section": "How Do We Estimate f?",
    "text": "How Do We Estimate f?\nTwo broad approaches:\nParametric Methods\n\nMake an assumption about the functional form (e.g., linear)\nReduces problem to estimating a few parameters\nEasier to interpret\nThis is what we’ll focus on\n\nNon-Parametric Methods\n\nDon’t assume a specific form\nMore flexible\nRequire more data\nHarder to interpret\nParametric (blue): We assume f is linear, then estimate β₀ and β₁\nNon-parametric (green): We let the data determine the shape of f\n\nThe method: Ordinary Least Squares (OLS)"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#why-linear-regression",
    "href": "weekly-notes/week-05-notes.html#why-linear-regression",
    "title": "Week 5 notes",
    "section": "Why Linear Regression?",
    "text": "Why Linear Regression?\nAdvantages:\n\nSimple and interpretable\nWell-understood properties\nWorks remarkably well for many problems\nFoundation for more complex methods\n\nLimitations:\n\nAssumes linearity (we’ll test this)\nSensitive to outliers\nMakes several assumptions (we’ll check these)"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#interpreting-coefficients",
    "href": "weekly-notes/week-05-notes.html#interpreting-coefficients",
    "title": "Week 5 notes",
    "section": "Interpreting Coefficients",
    "text": "Interpreting Coefficients\nIntercept (β₀) = $62,855\n\nExpected income when population = 0\nNot usually meaningful in practice\n\nSlope (β₁) = $0.02\n\nFor each additional person, income increases by $0.02\nMore useful: For every 1,000 people, income increases by ~$20\n\nIs this relationship real?\n\np-value &lt; 0.001 → Very unlikely to see this if true β₁ = 0\nWe can reject the null hypothesis"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#statistical-significance",
    "href": "weekly-notes/week-05-notes.html#statistical-significance",
    "title": "Week 5 notes",
    "section": "Statistical Significance",
    "text": "Statistical Significance\nThe logic:\n\nNull hypothesis (H₀): β₁ = 0 (no relationship)\nOur estimate: β₁ = 0.02\nQuestion: Could we get 0.02 just by chance if H₀ is true?\n\nt-statistic: How many standard errors away from 0?\n\nBigger |t| = more confidence the relationship is real\n\np-value: Probability of seeing our estimate if H₀ is true\n\nSmall p → reject H₀, conclude relationship exists\n\nKey Metrics (Averaged Across 10 Folds)\n\nRMSE: Typical prediction error (~$12,578)\nR²: % of variation explained (0.564)\nMAE: Average absolute error (~$8,860) - easier to interpret"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#why-this-matters-for-prediction",
    "href": "weekly-notes/week-05-notes.html#why-this-matters-for-prediction",
    "title": "Week 5 notes",
    "section": "Why This Matters for Prediction",
    "text": "Why This Matters for Prediction\nLinearity violations hurt predictions, not just inference:\n\nIf the true relationship is curved and you fit a straight line, you’ll systematically underpredict in some regions and overpredict in others\nBiased predictions in predictable ways (not random errors!)\nResidual plots should show random scatter - any pattern means your model is missing something systematic\n\nInterpretation:\n\np &gt; 0.05: Constant variance assumption OK\np &lt; 0.05: Evidence of heteroscedasticity\n\nIf detected, solutions:\n\nTransform Y (try log(income))\nRobust standard errors\nAdd missing variables\nAccept it (point predictions still OK for prediction goals)\n\n\nCoding Techniques\nThe model: Predicted healthcare needs using costs as proxy\nTechnically: Probably had good R², low prediction error (good “fit”)\nEthically: Learned and amplified existing discrimination\nR² = 0.208\n“21% of variation in income is explained by population”"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#questions-challenges",
    "href": "weekly-notes/week-05-notes.html#questions-challenges",
    "title": "Week 5 notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#assumption-2-constant-variance",
    "href": "weekly-notes/week-05-notes.html#assumption-2-constant-variance",
    "title": "Week 5 notes",
    "section": "Assumption 2: Constant Variance",
    "text": "Assumption 2: Constant Variance\nHeteroscedasticity: Variance changes across X\nImpact: Standard errors are wrong → p-values misleading"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#what-heteroskedasticity-tells-you",
    "href": "weekly-notes/week-05-notes.html#what-heteroskedasticity-tells-you",
    "title": "Week 5 notes",
    "section": "What Heteroskedasticity Tells You",
    "text": "What Heteroskedasticity Tells You\nOften a symptom of model misspecification:\n\nModel fits well for some values (e.g., small counties) but poorly for others (large counties)\nMay indicate missing variables that matter more at certain X values\nAsk: “What’s different about observations with large residuals?”\n\nExample: Population alone predicts income well in rural counties, but large urban counties need additional variables (education, industry) to predict accurately."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#reflection",
    "href": "weekly-notes/week-05-notes.html#reflection",
    "title": "Week 5 notes",
    "section": "Reflection",
    "text": "Reflection\nWhat we assume: Residuals are normally distributed\nWhy it matters:\n\nLess critical for point predictions (unbiased regardless)\nImportant for confidence intervals and prediction intervals\nNeeded for valid hypothesis tests (t-tests, F-tests)"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#assumption-3-no-multicollinearity",
    "href": "weekly-notes/week-05-notes.html#assumption-3-no-multicollinearity",
    "title": "Week 5 notes",
    "section": "Assumption 3: No Multicollinearity",
    "text": "Assumption 3: No Multicollinearity\nFor multiple regression: Predictors shouldn’t be too correlated\nWhy it matters: Coefficients become unstable, hard to interpret"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#assumption-4-no-influential-outliers",
    "href": "weekly-notes/week-05-notes.html#assumption-4-no-influential-outliers",
    "title": "Week 5 notes",
    "section": "Assumption 4: No Influential Outliers",
    "text": "Assumption 4: No Influential Outliers\nNot all outliers are problems - only those with high leverage AND large residuals"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Jennifer Luu, but I go by Jun (pronounced like June)! I am discovering my path of blending my lived experience and background in construction engineering with urban spatial analytics.\nI think the most effective way to tell you my story is by going through the cities I’ve lived and learned in. If you see a city you’re thinking about visiting, let me know! I would love to share recommendations or even help you plan your trip!\n\nMy Journey\nLeesburg, VA\nI grew up 1-1.5h outside of Washington D.C. It was in this politics-centered city where my flame of affordable housing ignited.\nBlacksburg, VA\nMy interest in working in affordable housing led me to Virginia Tech, where I majored in Construction Engineering and Management. GO HOKIES!\nChicago, IL\nI wanted to integrate myself into a city that with a strong history of housing reform and policy. It was here that I started to think about how my work in different construction projects were affecting the community that I was in. I began a personal research project in an attempt to quantify these effects.\nLos Angeles, CA\nDuring my time in LA, my research interests both expanded and deepened. I was able to see how housing directly affected healthcare and education.\nNew York City, NY\nI had the amazing opportunity to work on my dream project- the thing that I had worked for all through undergrad: renovation of low-income housing in East Brooklyn. I could go on about this, but I will spare you the reading. (Feel free to reach out to me if you want to talk about it!)\nPhiladelphia, PA\nIt all led to right here, The University of Pennsylvania. Though I loved working in affordable housing construction and it felt like that was what I was running towards since I was 16, I realized something else that I wanted. It is extremely important to do the work in the field, but I wanted to do research to support future projects.\nThanks for reading! I hope you’ll follow me to see where I go next! Maybe it’ll be another city, but maybe I’ll stay right here in Philly."
  },
  {
    "objectID": "assignments/assignment_3/Luu_Jun_Slides.html#the-problem-the-goal",
    "href": "assignments/assignment_3/Luu_Jun_Slides.html#the-problem-the-goal",
    "title": "Model Citizens Consulting",
    "section": "The Problem & The Goal",
    "text": "The Problem & The Goal\nWhy This Matters\nImproving the Office of Property Assessment’s Automated Valuation Model (AVM) can potentially:\n\nMore stable values aligned with market rates for properties being sold.\nAlleviate burden from gentrification in previously disinvested neighborhoods."
  },
  {
    "objectID": "assignments/assignment_3/Luu_Jun_Slides.html#the-problem-the-goal-1",
    "href": "assignments/assignment_3/Luu_Jun_Slides.html#the-problem-the-goal-1",
    "title": "Model Citizens Consulting",
    "section": "The Problem & The Goal",
    "text": "The Problem & The Goal\nWhat We Want\n\nEquitable taxation for residents.\nFinancial stability for Philadelphia.\nHelp set the future for more reliable open data."
  },
  {
    "objectID": "assignments/assignment_3/Luu_Jun_Slides.html#where-are-expensive-homes",
    "href": "assignments/assignment_3/Luu_Jun_Slides.html#where-are-expensive-homes",
    "title": "Model Citizens Consulting",
    "section": "Where Are Expensive Homes?",
    "text": "Where Are Expensive Homes?"
  },
  {
    "objectID": "assignments/assignment_3/Luu_Jun_Slides.html#findings",
    "href": "assignments/assignment_3/Luu_Jun_Slides.html#findings",
    "title": "Model Citizens Consulting",
    "section": "Findings",
    "text": "Findings\n\nHigher Prices: Center City, University City, the riverfront, and affluent Northwest pockets.\nPotentially due to easy access to transit and amenities.\nLower Prices: North of Broad Street into parts of West and North Philadelphia.\nPotentially reflecting long-term disinvestment.\nSale price is place-dependent in Philadelphia, mostly due to neighborhood qualities."
  },
  {
    "objectID": "assignments/assignment_3/Luu_Jun_Slides.html#what-drives-prices",
    "href": "assignments/assignment_3/Luu_Jun_Slides.html#what-drives-prices",
    "title": "Model Citizens Consulting",
    "section": "What Drives Prices?",
    "text": "What Drives Prices?\n\n\n\n\nLarger homes = an increase in price, but only up to a certain point."
  },
  {
    "objectID": "assignments/assignment_3/Luu_Jun_Slides.html#model-comparison-performance",
    "href": "assignments/assignment_3/Luu_Jun_Slides.html#model-comparison-performance",
    "title": "Model Citizens Consulting",
    "section": "Model Comparison & Performance",
    "text": "Model Comparison & Performance\n\n\nRMSE = 138,279.40 → Predicted sale price differs by about ± $138,279 from actual market sale price.\nR² = 0.746 → Explains 75% of variance in home prices."
  },
  {
    "objectID": "assignments/assignment_3/Luu_Jun_Slides.html#hardest-to-predict",
    "href": "assignments/assignment_3/Luu_Jun_Slides.html#hardest-to-predict",
    "title": "Model Citizens Consulting",
    "section": "Hardest To Predict",
    "text": "Hardest To Predict"
  },
  {
    "objectID": "assignments/assignment_1.html",
    "href": "assignments/assignment_1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Virginia Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment_1.html#scenario",
    "href": "assignments/assignment_1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Virginia Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment_1.html#learning-objectives",
    "href": "assignments/assignment_1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment_1.html#submission-instructions",
    "href": "assignments/assignment_1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment_1.html#data-retrieval",
    "href": "assignments/assignment_1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\ncounty_acs &lt;- get_acs(\n  variables = c(\n    median_household_income = \"B19013_001\", \n    total_population = \"B01003_001\"),\n  year = 2022, \n  geography = \"county\", \n  state = my_state, \n  survey = \"acs5\", \n  cache = TRUE, \n  output = \"wide\")\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\ncounty_clean = county_acs %&gt;%\n  mutate(\n    NAME = NAME %&gt;%\n      str_remove(\" County,\") %&gt;%\n      str_remove(\" Virginia\") %&gt;%\n      str_remove(\" city,\") # I had some counties that were named city as well as county \n  )\n\n# Display the first few rows\nhead(county_clean)\n\n# A tibble: 6 × 6\n  GEOID NAME     median_household_inc…¹ median_household_inc…² total_populationE\n  &lt;chr&gt; &lt;chr&gt;                     &lt;dbl&gt;                  &lt;dbl&gt;             &lt;dbl&gt;\n1 51001 Accomack                  52694                   5883             33367\n2 51003 Albemar…                  97708                   3686            112513\n3 51005 Allegha…                  52546                   3958             15159\n4 51007 Amelia                    63438                  15114             13309\n5 51009 Amherst                   64454                   4514             31426\n6 51011 Appomat…                  60041                   7091             16253\n# ℹ abbreviated names: ¹​median_household_incomeE, ²​median_household_incomeM\n# ℹ 1 more variable: total_populationM &lt;dbl&gt;"
  },
  {
    "objectID": "assignments/assignment_1.html#data-quality-assessment",
    "href": "assignments/assignment_1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\ncounty_reliability &lt;- county_clean %&gt;%\n  mutate(\n    hh_income_moe_pct = (median_household_incomeM / median_household_incomeE) * 100,\n    hh_income_moe_cat = case_when(\n      hh_income_moe_pct &lt; 5 ~ \"High Confidence\",\n      hh_income_moe_pct &lt; 10 ~ \"Moderate Confidence\",\n      TRUE ~ \"Low Confidence\"\n    )\n  )\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\n\nreliability_summary &lt;- county_reliability %&gt;%\n  count(hh_income_moe_cat, name = \"reliability_count\") %&gt;%\n  mutate(percentage= reliability_count/sum(reliability_count)*100)"
  },
  {
    "objectID": "assignments/assignment_1.html#high-uncertainty-counties",
    "href": "assignments/assignment_1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\nhighest_moe &lt;- county_reliability %&gt;%\n  arrange(desc(hh_income_moe_pct))\n\ntop_moe_counties &lt;- slice_head(highest_moe, n=5)\n\n# Format as table with kable() - include appropriate column names and caption\nhighest_moe_table &lt;- top_moe_counties %&gt;%\n  select(  \n    \"County\" = NAME,\n    \"Median Income\" = median_household_incomeE,\n    \"Margin Error (%)\" = hh_income_moe_pct,\n    \"Reliability\" = hh_income_moe_cat \n  )\n\nkable(highest_moe_table, caption = \"Top 5 Counties in Virginia by Median Household Income MOE\", booktabs = TRUE, digits = 2, align = \"c\")\n\n\nTop 5 Counties in Virginia by Median Household Income MOE\n\n\nCounty\nMedian Income\nMargin Error (%)\nReliability\n\n\n\n\nKing and Queen\n70147\n26.88\nLow Confidence\n\n\nNorton\n36974\n25.41\nLow Confidence\n\n\nAmelia\n63438\n23.82\nLow Confidence\n\n\nBath\n55699\n21.37\nLow Confidence\n\n\nLexington\n93651\n20.95\nLow Confidence\n\n\n\n\n\nData Quality Commentary:\nThe counties listed above have a high margin of error when reviewing their median household income. This can happen for many different reasons including lower population size, high income variability, and or smaller counties. This data could misrepresent the population and lead to biased decision making."
  },
  {
    "objectID": "assignments/assignment_1.html#focus-area-selection",
    "href": "assignments/assignment_1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\nselected_counties = c(\"Loudoun\", \"Essex\", \"Falls Church\")\n\nfiltered_counties &lt;- county_reliability %&gt;%\n  filter(NAME %in% selected_counties)\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n\nselected_counties_table &lt;- filtered_counties %&gt;%\n  select(  \n    \"County\" = NAME,\n    \"Median Income\" = median_household_incomeE,\n    \"Margin Error (%)\" = hh_income_moe_pct,\n    \"Reliability\" = hh_income_moe_cat \n  )\n\nkable(selected_counties_table, caption = \"Median Household Income of Selected Counties Virginia\", booktabs = TRUE, digits = 2, align = \"c\")\n\n\nMedian Household Income of Selected Counties Virginia\n\n\n\n\n\n\n\n\nCounty\nMedian Income\nMargin Error (%)\nReliability\n\n\n\n\nEssex\n52335\n15.29\nLow Confidence\n\n\nLoudoun\n170463\n2.05\nHigh Confidence\n\n\nFalls Church\n164536\n8.01\nModerate Confidence\n\n\n\n\n\nComment on the output: There seems like there may be a relationship between the MOE and Median Income."
  },
  {
    "objectID": "assignments/assignment_1.html#tract-level-demographics",
    "href": "assignments/assignment_1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\nrace_ethnicity &lt;- c(\n  white_alone = \"B03002_003\",\n  black = \"B03002_004\",\n  hispanic_latino = \"B03002_012\",\n  total_pop = \"B03002_001\"\n  )\n  \n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\ntract_acs &lt;- get_acs(\n  variables = race_ethnicity,\n  year = 2022, \n  geography = \"tract\", \n  state = my_state, \n  survey = \"acs5\", \n  county = selected_counties,\n  cache = TRUE, \n  output = \"wide\")\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\ntract_acs &lt;- tract_acs %&gt;%\n  mutate(\n    white_alone_pct = (white_aloneE / total_popE) * 100,\n    black_pct = (blackE / total_popE) * 100,\n    hispanic_latino_pct = (hispanic_latinoE / total_popE) * 100\n  )\n\n# Add readable tract and county name columns using str_extract() or similar\ntract_acs &lt;- tract_acs %&gt;%\n  mutate(\n  tract = str_extract(NAME, \"Tract [^;]+\"),\n  county = str_extract(NAME, \"(?&lt;=; )[^;]+(?=;)\") %&gt;%\n    str_remove(\"city\") %&gt;%\n    str_remove(\" County\")\n  )"
  },
  {
    "objectID": "assignments/assignment_1.html#demographic-analysis",
    "href": "assignments/assignment_1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\ntop_hispanic_tract &lt;- tract_acs %&gt;%\n  arrange(desc(hispanic_latino_pct)) %&gt;% \n  slice_head(n = 1) \n\nprint(top_hispanic_tract)\n\n# A tibble: 1 × 15\n  GEOID       NAME      white_aloneE white_aloneM blackE blackM hispanic_latinoE\n  &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;            &lt;dbl&gt;\n1 51107611602 Census T…          574          202    196    151             2325\n# ℹ 8 more variables: hispanic_latinoM &lt;dbl&gt;, total_popE &lt;dbl&gt;,\n#   total_popM &lt;dbl&gt;, white_alone_pct &lt;dbl&gt;, black_pct &lt;dbl&gt;,\n#   hispanic_latino_pct &lt;dbl&gt;, tract &lt;chr&gt;, county &lt;chr&gt;\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\ndemographics_summary &lt;- tract_acs %&gt;%\n  group_by(county) %&gt;%\n  summarize(\n    tracts_count = n(),\n    avg_white_pct = mean(white_alone_pct, na.rm = TRUE),\n    avg_black_pct = mean(black_pct, na.rm = TRUE),\n    avg_hispanic_latino_pct = mean(hispanic_latino_pct, na.rm = TRUE)\n  )\n\n# Create a nicely formatted table of your results using kable()\n\ndemographics_summary_table &lt;- demographics_summary %&gt;%\n  select( \n    \"County\" = county,\n    \"Number of Tracts\" = tracts_count,\n    \"Average White Only Population (%)\" = avg_white_pct,\n    \"Average Black Population (%)\" = avg_black_pct,\n    \"Average Hispanic/Latino Population (%)\" = avg_hispanic_latino_pct,    \n  )\n\nkable(demographics_summary_table, caption = \"Average Demographics by County\", booktabs = TRUE, digits = 2, align = \"c\")\n\n\nAverage Demographics by County\n\n\n\n\n\n\n\n\n\nCounty\nNumber of Tracts\nAverage White Only Population (%)\nAverage Black Population (%)\nAverage Hispanic/Latino Population (%)\n\n\n\n\nEssex\n3\n53.82\n37.84\n4.38\n\n\nFalls Church\n3\n69.04\n4.18\n11.34\n\n\nLoudoun\n75\n54.07\n7.28\n14.21"
  },
  {
    "objectID": "assignments/assignment_1.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment_1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\ndemographics_reliability &lt;- tract_acs %&gt;%\n  mutate(\n    white_alone_moe_pct = (white_aloneM / white_aloneE) * 100,\n    black_moe_pct = (blackM / blackE) * 100,\n    hispanic_latino_moe_pct = (hispanic_latinoM / hispanic_latinoE) * 100,\n    \n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\n        high_moe_flag = (white_alone_moe_pct &gt; 15) | \n                        (black_moe_pct &gt; 15) | \n                        (hispanic_latino_moe_pct &gt; 15)\n  )\n\n# Create summary statistics showing how many tracts have data quality issues\nmoe_summary &lt;- demographics_reliability %&gt;%\n  summarise(\n    total_tracts = n(),                        # total number of tracts\n    tracts_high_moe = sum(high_moe_flag),      # number of tracts flagged as high MOE\n    pct_high_moe = (tracts_high_moe / total_tracts) * 100  # percentage of tracts with high MOE\n  )\n\nmoe_summary_table &lt;- moe_summary %&gt;%\n  select(  \n  \"Total Number of Tracts\" = total_tracts,\n  \"Tracts with High MOE\" = tracts_high_moe,\n  \"Tracts with High MOE (%)\" = pct_high_moe\n  )\n\nkable(moe_summary_table, caption = \"Tracts with Data Quality Issues\", booktabs = TRUE, digits = 2, align = \"c\")\n\n\nTracts with Data Quality Issues\n\n\n\n\n\n\n\nTotal Number of Tracts\nTracts with High MOE\nTracts with High MOE (%)\n\n\n\n\n81\n81\n100"
  },
  {
    "objectID": "assignments/assignment_1.html#pattern-analysis",
    "href": "assignments/assignment_1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n\nmoe_tract &lt;- demographics_reliability %&gt;%\n  group_by(high_moe_flag) %&gt;%\n  summarize(\n    avg_pop = mean(total_popE, na.rm = TRUE),\n    avg_white_alone = mean(white_aloneE, na.rm = TRUE),\n    avg_black = mean(blackE, na.rm = TRUE),\n    avg_hispanic = mean(hispanic_latinoE, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\nmoe_tract_table &lt;- moe_tract %&gt;%\n  select(\n    \"Total\" = avg_pop,\n    \"White\" = avg_white_alone,\n    \"Black\" = avg_black,\n    \"Hispanic/Latino\" = avg_hispanic\n  )\nkable(moe_tract_table, caption = \"Average Population of Tracts with High MOE\", booktabs = TRUE, digits = 2, align = \"c\") # Since all my tracts have high MOE\n\n\nAverage Population of Tracts with High MOE\n\n\nTotal\nWhite\nBlack\nHispanic/Latino\n\n\n\n\n5505.57\n2955.37\n441.56\n744.53\n\n\n\n\n\nPattern Analysis: I found that there were varying margins of error when it came to income levels, but the demographic data had consistently higher margins of error. The data was particularly unreliable for Black and Hispanic/Latino populations, in some cases exceeding 100% margins of error. This is likely due to small population sizes within certain counties, which makes estimates less certain. For income levels, counties in southern Virginia showed lower confidence in their margins of error, and these counties also tended to have lower household incomes. By contrast, counties with larger population sizes generally had higher reliability, both in household income estimates and demographic data."
  },
  {
    "objectID": "assignments/assignment_1.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment_1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\nAcross all analyses, there were a few systematic patterns that emerges in the reliability of demographic and socioeconomic data. Household income data showed lower confidence concentrated in southern Virginia counties that also exhibited lower household incomes. Demographic data consistently revealed higher margins of error, particularly for Black and Hispanic/Latino populations, in some cases exceeding 100% across all counties. Counties with larger population sizes tended to have more reliable estimates across both demographic and income indicators, highlighting a structural imbalance in data quality.\nFrom an equity perspective, these disparities mean that communities of Black and Hispanic/Latino residents face a greater risk of algorithmic bias when data is used for policy, funding, or service allocation. When data carries high uncertainty, any algorithm built on it risks incorrectly representing those groups. Similarly, lower-income communities in southern Virginia also face heightened risk, as unreliable income data undermines equitable targeting of economic support or development programs.\nThe root causes of these issues stem primarily from small sample sizes in survey-based data collection methods, which reduce reliability for smaller populations. Smaller minority populations, rural communities, and low-income counties are more likely to experience data quality issues and more dependent on accurate representation for equitable policy outcomes. These factors create a feedback loop where communities most in need of resources have the weakest statistical representation.\nTo address these systemic challenges, the Department should invest in data collection for small populations by increasing the percentage per county population data collected. The Department could also look at different data collection surveys in order decrease margin of error. These options could help the Department mitigate risks of algorithmic bias while ensuring that vulnerable communities are not disadvantaged by unreliable data. If they are unable to make these changes, I would urge them to include their margins of error when using any information from the data sets."
  },
  {
    "objectID": "assignments/assignment_1.html#specific-recommendations",
    "href": "assignments/assignment_1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\ncounty_reliability_table &lt;- county_reliability %&gt;%\n  select(\n    \"County\" = NAME,\n    \"Median Income\" = median_household_incomeE,\n    \"MOE (%)\" = hh_income_moe_pct,\n    \"Reliability\" = hh_income_moe_cat\n  )\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\ncounty_reliability_table &lt;- county_reliability_table %&gt;%\n  mutate(\n    \"Algorithm Recommendation\" = case_when(\n      Reliability == \"High Confidence\"   ~ \"Safe for algorithmic decisions\",\n      Reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      Reliability == \"Low Confidence\"    ~ \"Requires manual review or additional data\",\n      TRUE ~ NA_character_  # for any missing/unexpected values\n    )\n  )\n  \n# Format as a professional table with kable()\nkable(county_reliability_table, caption = \"Household Income Reliability by County\", booktabs = TRUE, digits = 2, align = \"c\") \n\n\nHousehold Income Reliability by County\n\n\n\n\n\n\n\n\n\nCounty\nMedian Income\nMOE (%)\nReliability\nAlgorithm Recommendation\n\n\n\n\nAccomack\n52694\n11.16\nLow Confidence\nRequires manual review or additional data\n\n\nAlbemarle\n97708\n3.77\nHigh Confidence\nSafe for algorithmic decisions\n\n\nAlleghany\n52546\n7.53\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nAmelia\n63438\n23.82\nLow Confidence\nRequires manual review or additional data\n\n\nAmherst\n64454\n7.00\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nAppomattox\n60041\n11.81\nLow Confidence\nRequires manual review or additional data\n\n\nArlington\n137387\n1.98\nHigh Confidence\nSafe for algorithmic decisions\n\n\nAugusta\n76124\n4.17\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBath\n55699\n21.37\nLow Confidence\nRequires manual review or additional data\n\n\nBedford\n74773\n4.38\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBland\n59901\n4.73\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBotetourt\n77680\n7.33\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nBrunswick\n52678\n5.92\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nBuchanan\n39591\n7.40\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nBuckingham\n59894\n14.55\nLow Confidence\nRequires manual review or additional data\n\n\nCampbell\n59022\n6.30\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCaroline\n83562\n8.56\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCarroll\n49113\n9.77\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCharles City\n65573\n5.26\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCharlotte\n51548\n17.84\nLow Confidence\nRequires manual review or additional data\n\n\nChesterfield\n95757\n2.22\nHigh Confidence\nSafe for algorithmic decisions\n\n\nClarke\n107475\n14.79\nLow Confidence\nRequires manual review or additional data\n\n\nCraig\n66286\n12.27\nLow Confidence\nRequires manual review or additional data\n\n\nCulpeper\n92359\n4.65\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCumberland\n56497\n14.25\nLow Confidence\nRequires manual review or additional data\n\n\nDickenson\n40143\n7.46\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nDinwiddie\n77225\n9.55\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nEssex\n52335\n15.29\nLow Confidence\nRequires manual review or additional data\n\n\nFairfax\n145165\n1.16\nHigh Confidence\nSafe for algorithmic decisions\n\n\nFauquier\n122785\n5.40\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nFloyd\n57146\n10.90\nLow Confidence\nRequires manual review or additional data\n\n\nFluvanna\n90766\n6.05\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nFranklin\n66275\n5.74\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nFrederick\n92443\n4.99\nHigh Confidence\nSafe for algorithmic decisions\n\n\nGiles\n61987\n6.19\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGloucester\n83750\n5.24\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGoochland\n105600\n8.09\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGrayson\n43348\n10.54\nLow Confidence\nRequires manual review or additional data\n\n\nGreene\n81338\n6.64\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGreensville\n51823\n13.84\nLow Confidence\nRequires manual review or additional data\n\n\nHalifax\n49145\n5.18\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nHanover\n104678\n3.36\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHenrico\n82424\n2.10\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHenry\n43694\n5.98\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nHighland\n57070\n14.45\nLow Confidence\nRequires manual review or additional data\n\n\nIsle of Wight\n91680\n4.09\nHigh Confidence\nSafe for algorithmic decisions\n\n\nJames City\n100711\n4.64\nHigh Confidence\nSafe for algorithmic decisions\n\n\nKing and Queen\n70147\n26.88\nLow Confidence\nRequires manual review or additional data\n\n\nKing George\n103264\n8.14\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nKing William\n79398\n11.86\nLow Confidence\nRequires manual review or additional data\n\n\nLancaster\n62674\n7.10\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLee\n41619\n12.35\nLow Confidence\nRequires manual review or additional data\n\n\nLoudoun\n170463\n2.05\nHigh Confidence\nSafe for algorithmic decisions\n\n\nLouisa\n76594\n9.58\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLunenburg\n54438\n14.45\nLow Confidence\nRequires manual review or additional data\n\n\nMadison\n74586\n8.68\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMathews\n79054\n18.95\nLow Confidence\nRequires manual review or additional data\n\n\nMecklenburg\n51265\n8.19\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMiddlesex\n69389\n9.39\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMontgomery\n65270\n5.45\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nNelson\n64028\n17.43\nLow Confidence\nRequires manual review or additional data\n\n\nNew Kent\n113120\n6.15\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nNorthampton\n54693\n13.07\nLow Confidence\nRequires manual review or additional data\n\n\nNorthumberland\n64655\n14.33\nLow Confidence\nRequires manual review or additional data\n\n\nNottoway\n62366\n13.91\nLow Confidence\nRequires manual review or additional data\n\n\nOrange\n87309\n9.40\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nPage\n56760\n7.45\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nPatrick\n49180\n10.87\nLow Confidence\nRequires manual review or additional data\n\n\nPittsylvania\n52619\n5.76\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nPowhatan\n108089\n4.56\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPrince Edward\n57304\n6.61\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nPrince George\n80318\n6.31\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nPrince William\n123193\n2.19\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPulaski\n59740\n6.43\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nRappahannock\n98663\n11.29\nLow Confidence\nRequires manual review or additional data\n\n\nRichmond\n62708\n18.57\nLow Confidence\nRequires manual review or additional data\n\n\nRoanoke\n80872\n2.34\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRockbridge\n61903\n5.71\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nRockingham\n73232\n3.18\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRussell\n44088\n8.87\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nScott\n44535\n6.38\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nShenandoah\n62149\n6.68\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSmyth\n45061\n7.21\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSouthampton\n67813\n10.69\nLow Confidence\nRequires manual review or additional data\n\n\nSpotsylvania\n105068\n4.52\nHigh Confidence\nSafe for algorithmic decisions\n\n\nStafford\n128036\n3.18\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSurry\n68655\n10.95\nLow Confidence\nRequires manual review or additional data\n\n\nSussex\n59195\n11.64\nLow Confidence\nRequires manual review or additional data\n\n\nTazewell\n46508\n7.10\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWarren\n79313\n8.86\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWashington\n59116\n4.31\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWestmoreland\n56647\n12.26\nLow Confidence\nRequires manual review or additional data\n\n\nWise\n47541\n6.43\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWythe\n53921\n7.47\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nYork\n105154\n3.38\nHigh Confidence\nSafe for algorithmic decisions\n\n\nAlexandria\n113179\n2.24\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBristol\n45250\n6.95\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nBuena Vista\n48783\n14.29\nLow Confidence\nRequires manual review or additional data\n\n\nCharlottesville\n67177\n7.63\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nChesapeake\n92703\n2.37\nHigh Confidence\nSafe for algorithmic decisions\n\n\nColonial Heights\n72216\n7.53\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCovington\n45737\n15.38\nLow Confidence\nRequires manual review or additional data\n\n\nDanville\n41484\n8.28\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nEmporia\n41442\n14.39\nLow Confidence\nRequires manual review or additional data\n\n\nFairfax\n128708\n9.39\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nFalls Church\n164536\n8.01\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nFranklin\n57537\n9.03\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nFredericksburg\n83445\n6.36\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGalax\n44612\n20.94\nLow Confidence\nRequires manual review or additional data\n\n\nHampton\n64430\n3.09\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHarrisonburg\n56050\n3.21\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHopewell\n50661\n10.43\nLow Confidence\nRequires manual review or additional data\n\n\nLexington\n93651\n20.95\nLow Confidence\nRequires manual review or additional data\n\n\nLynchburg\n56243\n4.99\nHigh Confidence\nSafe for algorithmic decisions\n\n\nManassas\n110559\n7.71\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nManassas Park\n91673\n16.15\nLow Confidence\nRequires manual review or additional data\n\n\nMartinsville\n39127\n9.27\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nNewport News\n63355\n3.31\nHigh Confidence\nSafe for algorithmic decisions\n\n\nNorfolk\n60998\n3.08\nHigh Confidence\nSafe for algorithmic decisions\n\n\nNorton\n36974\n25.41\nLow Confidence\nRequires manual review or additional data\n\n\nPetersburg\n46930\n6.60\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nPoquoson\n114503\n7.31\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nPortsmouth\n57154\n4.32\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRadford\n51039\n14.27\nLow Confidence\nRequires manual review or additional data\n\n\nRichmond\n59606\n3.96\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRoanoke\n51523\n4.47\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSalem\n68402\n6.98\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nStaunton\n59731\n8.95\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSuffolk\n87758\n4.22\nHigh Confidence\nSafe for algorithmic decisions\n\n\nVirginia Beach\n87544\n2.24\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWaynesboro\n52519\n8.79\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWilliamsburg\n66815\n7.32\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWinchester\n62495\n6.93\nModerate Confidence\nUse with caution - monitor outcomes\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: Albemarle, Arlington, Augusta, Bedford, Bland, Chesterfield, Culpeper, Fairfax, Frederick, Hanover, Henrico, Isle of Wight, James City, Loudoun, Powhatan, Prince William, Roanoke, Rockingham, Spotsylvania, Stafford, Washington, York, Alexandria, Chesapeake, Hampton, Harrisonburg, Lynchburg, Newport News, Norfolk, Portsmouth, Richmond, Roanoke, Suffolk, Virginia Beach.\n\nThe high confidence counties have data with low margins of errors. These counties are considered “safe” because the data is statistically sound, enabling reliable automated decision-making with minimal risk of error or bias.\n\nCounties requiring additional oversight: Alleghany, Amherst, Botetourt, Brunswick, Buchanan, Campbell, Caroline, Carroll, Charles City, Dickenson, Dinwiddie, Fauquier, Fluvanna, Franklin, Giles, Gloucester, Goochland, Greene, Halifax, Henry, King George, Lancaster, Louisa, Madison, Mecklenburg, Middlesex, Montgomery, New Kent, Orange, Page, Pittsylvania, Prince Edward, Prince George, Pulaski, Rockbridge, Russell, Scott, Shenandoah, Smyth, Tazewell, Warren, Wise, Wythe, Bristol, Charlottesville, Colonial Heights, Danville, Fairfax, Falls Church, Franklin, Fredericksburg, Manassas, Martinsville, Petersburg, Poquoson, Salem, Staunton, Waynesboro, Williamsburg, Winchester.\n\nModerate confidence county data should be used cautiously. The data is generally usable but carries enough uncertainty that algorithmic decisions could occasionally be misleading. Though these counties do not need to be manually reviewed every time, it is important to monitor any abnormalities in the data.\n\nCounties needing alternative approaches: Accomack, Amelia, Appomattox, Bath, Buckingham, Charlotte, Clarke, Craig, Cumberland, Essex, Floyd, Grayson, Greensville, Highland, King and Queen, King William, Lee, Lunenburg, Mathews, Nelson, Northampton, Northumberland, Nottoway, Patrick, Rappahannock, Richmond, Southampton, Surry, Sussex, Westmoreland, Buena Vista, Covington, Emporia, Galax, Hopewell, Lexington, Manassas Park, Norton, Radford.\n\nThese counties have a high margin of error meaning their data has higher variability with lower data points. This means that decisions that are made using this data set should be checked manually before implementation."
  },
  {
    "objectID": "assignments/assignment_1.html#questions-for-further-investigation",
    "href": "assignments/assignment_1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nAre there regional clusters of counties with consistently high margins of error, and how do these clusters relate to population size, demographics, rurality, or economic conditions?\nHow do margins of error and confidence levels change over time?\nDo certain demographic characteristics such as race and age contribute to higher MOE or lower confidence in algorithmic outputs?"
  },
  {
    "objectID": "assignments/assignment_1.html#submission-checklist",
    "href": "assignments/assignment_1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "assignments/lab-05.html",
    "href": "assignments/lab-05.html",
    "title": "Lab 5",
    "section": "",
    "text": "You’re early! Come back to see when I upload my Lab 5 project."
  },
  {
    "objectID": "assignments/lab-03.html",
    "href": "assignments/lab-03.html",
    "title": "Lab 3",
    "section": "",
    "text": "You’re early! Come back to see when I upload my Lab 3 project."
  },
  {
    "objectID": "assignments/lab-04.html",
    "href": "assignments/lab-04.html",
    "title": "Lab 4",
    "section": "",
    "text": "You’re early! Come back to see when I upload my Lab 4 project."
  },
  {
    "objectID": "assignments/assignment_2/Luu_Jun_Assignment2.html",
    "href": "assignments/assignment_2/Luu_Jun_Assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment_2/Luu_Jun_Assignment2.html#assignment-overview",
    "href": "assignments/assignment_2/Luu_Jun_Assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment_2/Luu_Jun_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "assignments/assignment_2/Luu_Jun_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data: - Pennsylvania county boundaries - Pennsylvania hospitals (from lecture data) - Pennsylvania census tracts\nYour Task:\nQuestions to answer: - How many hospitals are in your dataset? - How many census tracts? - What coordinate reference system is each dataset in?\n\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables: - Total population - Median household income - Population 65 years and over (you may need to sum multiple age categories)\nYour Task:\n\nacs_vars &lt;- load_variables(2022, \"acs5\", cache = TRUE)\n\n# Get demographic data from ACS\n\ncensus_api_key(Sys.getenv(\"368bf145f527c34904bbbc75ef3158887059279a\"))\n\nvariables &lt;- c(\n  median_household_income = \"B19013_001\",\n  total_population = \"B01003_001\",\n  male_65_75 = \"B01001A_014\",\n  male_75_85 = \"B01001A_015\",\n  male_85_over = \"B01001A_016\",\n  female_65_75 = \"B01001A_029\",\n  female_75_85 = \"B01001A_030\",\n  female_85_over = \"B01001A_031\"\n)\n\npa_tract_acs &lt;- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  variables = variables,\n  year = 2022,\n  survey = \"acs5\",\n  cache_table = TRUE, \n  output = \"wide\"\n)\n\npa_tract_acs &lt;- pa_tract_acs %&gt;%\n  mutate(\n    pop_65_overE = male_65_75E + male_75_85E + male_85_overE +\n                   female_65_75E + female_75_85E + female_85_overE\n  )\n\n# Join to tract boundaries\ntracts_with_data &lt;- pa_tracts %&gt;%\n  left_join(pa_tract_acs, by = \"GEOID\")\n\n# - What year of ACS data are you using? \nacs_year &lt;- 2022\ncat(\"ACS Year:\", acs_year, \"\\n\")\n\nACS Year: 2022 \n\n# How many tracts have missing income data? \nmissing_income &lt;- sum(is.na(tracts_with_data$median_household_incomeE))\ncat(\"Number of tracts with missing income data:\", missing_income, \"\\n\")\n\nNumber of tracts with missing income data: 62 \n\n# What is the median income across all PA census tracts? \nmedian_income_statewide &lt;- median(tracts_with_data$median_household_incomeE, na.rm = TRUE)\ncat(\"Median income across all PA tracts: $\", round(median_income_statewide, 0), \"\\n\")\n\nMedian income across all PA tracts: $ 70188 \n\n\nQuestions to answer: - What year of ACS data are you using? - How many tracts have missing income data? - What is the median income across all PA census tracts?\n\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria: 1. Low median household income (choose an appropriate threshold) 2. Significant elderly population (choose an appropriate threshold)\nYour Task:\n\n# Filter for vulnerable tracts based on your criteria\ntracts_with_data &lt;- tracts_with_data %&gt;%\n  mutate(\n    percent_65_over = (pop_65_overE / total_populationE) * 100\n  )\n\nincome_threshold &lt;- quantile(tracts_with_data$median_household_incomeE, probs = 0.25, na.rm = TRUE)\nage_threshold &lt;- 20\n\ntracts_with_data &lt;- tracts_with_data %&gt;%\n  mutate(\n    low_income = median_household_incomeE &lt;= income_threshold, \n    elderly = percent_65_over &gt;= age_threshold,    \n    vulnerable = low_income & elderly \n  )\n\n# 1. What income threshold did you choose and why?\ncat(\"Income threshold: $\", income_threshold, \n    \", which is the bottom quartile of income in Pennsylvania\\n\")\n\nIncome threshold: $ 55923.5 , which is the bottom quartile of income in Pennsylvania\n\n# 2. What elderly population threshold did you choose and why?\ncat(\"Elderly population threshold:\", age_threshold, \n    \"% , based on the Census Bureau. \\n\")\n\nElderly population threshold: 20 % , based on the Census Bureau. \n\n# 3. How many tracts meet your vulnerability criteria?\nnum_vulnerable &lt;- sum(tracts_with_data$vulnerable, na.rm = TRUE)\ncat(\"Number of vulnerable tracts:\", num_vulnerable, \"\\n\")\n\nNumber of vulnerable tracts: 197 \n\n# 4. What percentage of PA census tracts are considered vulnerable?\ntotal_tracts &lt;- nrow(tracts_with_data)\npct_vulnerable &lt;- (num_vulnerable / total_tracts) * 100\ncat(\"Percentage of vulnerable tracts in PA:\", round(pct_vulnerable, 2), \"%\\n\")\n\nPercentage of vulnerable tracts in PA: 5.72 %\n\n\nQuestions to answer: - What income threshold did you choose and why? - What elderly population threshold did you choose and why? - How many tracts meet your vulnerability criteria?\n- What percentage of PA census tracts are considered vulnerable by your definition?\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\nYour Task:\n\n# Transform to appropriate projected CRS\npa_counties_proj &lt;- st_transform(pa_counties, 3365)\npa_hospitals_proj &lt;- st_transform(pa_hospitals, 3365)\npa_tracts_proj &lt;- st_transform(tracts_with_data, 3365)\n\n# Calculate distance from each tract centroid to nearest hospital\nall_centroids &lt;- st_centroid(pa_tracts_proj)\n\nall_dist &lt;- st_distance(all_centroids, pa_hospitals_proj)\nall_centroids$nearest_hosp_ft &lt;- apply(all_dist, 1, min)\n\nall_centroids &lt;- all_centroids %&gt;%\n  mutate(nearest_hosp_mi = nearest_hosp_ft / 5280)\n\npa_centroids &lt;- pa_tracts_proj %&gt;%\n  left_join(\n    all_centroids %&gt;%\n      st_drop_geometry() %&gt;%\n      select(GEOID, nearest_hosp_mi),\n    by = \"GEOID\"\n  )\n\nnum_vul = sum(pa_centroids$vulnerable, na.rm = TRUE)\n\nvulnerable_tracts &lt;- pa_centroids %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(vulnerable == TRUE)\n\n# 1. What is the average distance to the nearest hospital for vulnerable tracts?\navg_dist &lt;- mean(vulnerable_tracts$nearest_hosp_mi, na.rm = TRUE)\ncat(\"Average distance to nearest hospital:\", round(avg_dist, 2), \"miles\\n\")\n\nAverage distance to nearest hospital: 5.46 miles\n\n# 2. What is the maximum distance to the nearest hospital?\nmax_dist &lt;- max(vulnerable_tracts$nearest_hosp_mi, na.rm = TRUE)\ncat(\"Maximum distance to nearest hospital:\", round(max_dist, 2), \"miles\\n\")\n\nMaximum distance to nearest hospital: 25.62 miles\n\n# 3. How many vulnerable tracts are more than 15 miles from the nearest hospital?\nnum_far &lt;- sum(vulnerable_tracts$nearest_hosp_mi &gt; 15, na.rm = TRUE)\ncat(\"Number of vulnerable tracts &gt; 15 miles from nearest hospital:\", num_far, \"\\n\")\n\nNumber of vulnerable tracts &gt; 15 miles from nearest hospital: 13 \n\n# 4. What percentage of vulnerable tracts are &gt;15 miles away?\nover_15mi &lt;- (num_far / num_vul) * 100\ncat(\"Percentage of vulnerable tracts &gt; 15 miles from nearest hospital:\", round(over_15mi, 2), \"%\\n\")\n\nPercentage of vulnerable tracts &gt; 15 miles from nearest hospital: 6.6 %\n\n\nRequirements: - Use an appropriate projected coordinate system for Pennsylvania - Calculate distances in miles - Explain why you chose your projection: I used EPSG: 26918 (UTM Zone 18N) as the projected coordinate system because it preserves distance accuracy across Pennsylvania.\nQuestions to answer: - What is the average distance to the nearest hospital for vulnerable tracts? - What is the maximum distance? - How many vulnerable tracts are more than 15 miles from the nearest hospital?\n\n\n\nStep 5: Identify Underserved Areas\nDefine “underserved” as vulnerable tracts that are more than 15 miles from the nearest hospital.\nYour Task:\n\n# Create underserved variable\nunderserved_tracts &lt;- pa_centroids %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(vulnerable == TRUE & nearest_hosp_mi &gt; 15)\n\npa_centroids &lt;- pa_centroids%&gt;%\n  mutate(\n    underserved = vulnerable & nearest_hosp_mi &gt; 15\n  )\n\nnum_underserved = sum(pa_centroids$underserved == TRUE, na.rm = TRUE)\n# 1. How many tracts are underserved?\ncat(\"Number of underserved tracts:\", num_underserved, \"\\n\")\n\nNumber of underserved tracts: 13 \n\n# 2. What percentage of vulnerable tracts are underserved?\npct_underserved &lt;- (num_underserved / num_vul) * 100\ncat(\"Percentage of vulnerable tracts that are underserved:\", round(pct_underserved, 2), \"%\\n\")\n\nPercentage of vulnerable tracts that are underserved: 6.6 %\n\n# 3. Does this surprise you? Why or why not?\ncat(\"This honestly does surprise me, because I thought that the number would be higher. I think that I could have decreased the elderly population percentage to 15%, but I will continue to use the information determined by the Census Bureau.\\n\")\n\nThis honestly does surprise me, because I thought that the number would be higher. I think that I could have decreased the elderly population percentage to 15%, but I will continue to use the information determined by the Census Bureau.\n\n\nQuestions to answer: - How many tracts are underserved? - What percentage of vulnerable tracts are underserved? - Does this surprise you? Why or why not?\n\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\nYour Task:\n\n# Spatial join tracts to counties\ncounty_tract_proj &lt;- pa_counties_proj %&gt;%\n  st_join(pa_centroids)\n\n\n# Aggregate statistics by county\ncounty_summary &lt;- county_tract_proj %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarise(\n    num_vul_tracts = sum(vulnerable, na.rm = TRUE),\n    num_und_tracts = sum(vulnerable & nearest_hosp_mi &gt; 15, na.rm = TRUE),\n    pct_und_tracts = round((num_und_tracts/num_vul_tracts)*100, 2),\n    avg_vul_dist = round(mean(nearest_hosp_mi[vulnerable], na.rm = TRUE), 2),\n    total_vul_pop = round(sum(total_populationE[vulnerable], na.rm = TRUE), 2)\n  )\n\n# 1. Which 5 counties have the highest percentage of underserved vulnerable tracts?\ntop5_pct_underserved &lt;- county_summary %&gt;%\n  slice_max(pct_und_tracts, n = 5)\n\nkable(top5_pct_underserved, caption = \"Top 5 counties with the highest percentage of underserved vulnerable tracts\")\n\n\nTop 5 counties with the highest percentage of underserved vulnerable tracts\n\n\n\n\n\n\n\n\n\n\nCOUNTY_NAM\nnum_vul_tracts\nnum_und_tracts\npct_und_tracts\navg_vul_dist\ntotal_vul_pop\n\n\n\n\nCOLUMBIA\n1\n1\n100\n16.91\n918\n\n\nDAUPHIN\n1\n1\n100\n19.16\n4018\n\n\nMONROE\n1\n1\n100\n17.68\n1299\n\n\nPERRY\n2\n2\n100\n17.53\n5800\n\n\nCAMERON\n4\n2\n50\n14.88\n8919\n\n\nCLINTON\n2\n1\n50\n11.03\n5124\n\n\nSULLIVAN\n2\n1\n50\n13.53\n4828\n\n\n\n\n# 2. Which counties have the most vulnerable people living far from hospitals?\ntop_5_und_counties &lt;- county_summary %&gt;%\n  arrange(desc(total_vul_pop)) %&gt;%\n  slice_head(n = 5)\n\nkable(top_5_und_counties, caption = \"Counties with the most vulnerable people living far from hospitals\")\n\n\nCounties with the most vulnerable people living far from hospitals\n\n\n\n\n\n\n\n\n\n\nCOUNTY_NAM\nnum_vul_tracts\nnum_und_tracts\npct_und_tracts\navg_vul_dist\ntotal_vul_pop\n\n\n\n\nWESTMORELAND\n22\n0\n0\n3.47\n60018\n\n\nFAYETTE\n16\n0\n0\n4.96\n49722\n\n\nALLEGHENY\n20\n0\n0\n2.61\n46795\n\n\nCLEARFIELD\n10\n3\n30\n12.80\n34060\n\n\nCAMBRIA\n13\n0\n0\n6.72\n31052\n\n\n\n\n# 3. Are there any patterns in where underserved counties are located?\ncounty_summary_map &lt;- pa_counties_proj %&gt;%\n  left_join(county_summary, by = \"COUNTY_NAM\")\n\n# Quick visualization\nggplot(county_summary_map) +\n  geom_sf(aes(fill = pct_und_tracts)) +\n  scale_fill_viridis_c(option = \"plasma\", na.value = \"grey90\") +\n  labs(\n    title = \"Percentage of Underserved Vulnerable Tracts by County\",\n    fill = \"% Underserved\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nRequired county-level statistics: - Number of vulnerable tracts - Number of underserved tracts\n- Percentage of vulnerable tracts that are underserved - Average distance to nearest hospital for vulnerable tracts - Total vulnerable population\nQuestions to answer: - Which 5 counties have the highest percentage of underserved vulnerable tracts? - Which counties have the most vulnerable people living far from hospitals? - Are there any patterns in where underserved counties are located?\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\nYour Task:\n\n# Create and format priority counties table\ncounty_priority &lt;- county_summary %&gt;%\n  mutate(\n    priority_score = round((.5*scale(pct_und_tracts))+(.5*scale(total_vul_pop)), 2),\n  ) %&gt;%\n  arrange(desc(priority_score)) %&gt;%\n  slice_head(n = 10)\n\ncounty_priority %&gt;%\n  select(\n    County = COUNTY_NAM,\n    `Vulnerable Tracts` = num_vul_tracts,\n    `Underserved Tracts` = num_und_tracts,\n    `Underserved (%)` = pct_und_tracts,\n    `Avg Distance (mi)` = avg_vul_dist,\n    `Vulnerable Population` = total_vul_pop,\n    `Priority Score` = priority_score\n  ) %&gt;%\n  kable(\n    caption = \"Top 10 Priority Counties for Healthcare Investment in Pennsylvania&lt;br&gt;(Based on Vulnerable Populations and Hospital Access)\",\n    format = \"html\",\n    align = c(\"l\", \"r\", \"r\", \"r\", \"r\", \"r\")\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE,\n    font_size = 13\n  ) %&gt;%\n  row_spec(0, bold = TRUE, background = \"#002855\", color = \"white\")\n\n\nTop 10 Priority Counties for Healthcare Investment in Pennsylvania\n(Based on Vulnerable Populations and Hospital Access)\n\n\nCounty\nVulnerable Tracts\nUnderserved Tracts\nUnderserved (%)\nAvg Distance (mi)\nVulnerable Population\nPriority Score\n\n\n\n\nWESTMORELAND\n22\n0\n0.00\n3.47\n60018\n1.62\n\n\nPERRY\n2\n2\n100.00\n17.53\n5800\n1.23\n\n\nFAYETTE\n16\n0\n0.00\n4.96\n49722\n1.20\n\n\nDAUPHIN\n1\n1\n100.00\n19.16\n4018\n1.16\n\n\nCLEARFIELD\n10\n3\n30.00\n12.80\n34060\n1.10\n\n\nALLEGHENY\n20\n0\n0.00\n2.61\n46795\n1.08\n\n\nMONROE\n1\n1\n100.00\n17.68\n1299\n1.05\n\n\nCOLUMBIA\n1\n1\n100.00\n16.91\n918\n1.03\n\n\nLUZERNE\n11\n1\n9.09\n3.98\n29629\n0.54\n\n\nWARREN\n9\n1\n11.11\n7.00\n27988\n0.51\n\n\n\n\n\nRequirements: - Use knitr::kable() or similar for formatting - Include descriptive column names - Format numbers appropriately (commas for population, percentages, etc.) - Add an informative caption - Sort by priority (you decide the metric)"
  },
  {
    "objectID": "assignments/assignment_2/Luu_Jun_Assignment2.html#part-2-comprehensive-visualization",
    "href": "assignments/assignment_2/Luu_Jun_Assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\n# Create county-level access map\npa_counties_summary &lt;- pa_counties_proj %&gt;%\n  left_join(county_summary, by = \"COUNTY_NAM\")\n\nggplot() +\n  geom_sf(\n    data = pa_counties_summary,\n    aes(fill = pct_und_tracts),\n    color = \"white\",\n    size = 0.2\n  ) +\n  geom_sf(\n    data = pa_hospitals_proj,\n    fill = \"red\",\n    color = \"white\",\n    size = 1.2,\n    shape = 21,\n    stroke = 0.3,\n    alpha = 1\n  ) +\n  scale_fill_viridis_c(\n    option = \"plasma\",\n    name = \"% Underserved\\n(Vulnerable Tracts)\",\n    labels = function(x) paste0(x, \"%\"),\n    na.value = \"grey90\"\n  ) +\n  labs(\n    title = \"Healthcare Access Challenges in Pennsylvania by County\",\n    subtitle = \"Counties shaded by the % of underserved tracts\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, margin = margin(b = 10)),\n    plot.caption = element_text(size = 9, color = \"gray40\", hjust = 0),\n    legend.title = element_text(size = 11, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\nRequirements: - Fill counties by percentage of vulnerable tracts that are underserved - Include hospital locations as points - Use an appropriate color scheme - Include clear title, subtitle, and caption - Use theme_void() or similar clean theme - Add a legend with formatted labels\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\n# Create detailed tract-level map\n\nggplot() +\n  geom_sf(\n    data = pa_counties_proj,\n    fill = \"grey95\",\n    color = \"white\",\n    size = 0.3\n  ) +\n  geom_sf(\n    data = pa_centroids,\n    aes(geometry = geometry),\n    color = \"grey85\",\n    size = 0.1,\n    alpha = 0.4\n  ) +\n  geom_sf(\n    data = pa_centroids %&gt;% filter(underserved == TRUE),\n    aes(geometry = geometry),\n    fill = \"blue\",\n    color = \"white\",\n    size = 0.15,\n    alpha = 0.8\n  ) +\n  geom_sf(\n    data = pa_hospitals_proj,\n    fill = \"red\",\n    color = \"white\",\n    size = 1.2,\n    shape = 21,\n    stroke = 0.3,\n    alpha = 1\n  ) +\n  labs(\n    title = \"Underserved Vulnerable Census Tracts in Pennsylvania\",\n    subtitle = \"Tracts with vulnerable populations located more than 15 miles from the nearest hospital\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, margin = margin(b = 10)),\n    plot.caption = element_text(size = 9, color = \"gray40\", hjust = 0),\n    legend.position = \"none\"\n  ) +\n  coord_sf(expand = FALSE)\n\n\n\n\n\n\n\n\nRequirements: - Show underserved vulnerable tracts in a contrasting color - Include county boundaries for context - Show hospital locations - Use appropriate visual hierarchy (what should stand out?) - Include informative title and subtitle\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\nYour Task:\n\n# Create distribution visualization\nggplot(pa_centroids %&gt;% filter(vulnerable), \n       aes(x = nearest_hosp_mi, y = total_populationE)) +\n  geom_point(alpha = 0.6, color = \"blue\", size = 1.2) +\n  scale_y_continuous(labels = comma) +\n  labs(\n    title = \"Distance to Nearest Hospital vs. Vulnerable Population Size\",\n    x = \"Distance to Nearest Hospital (mi)\",\n    y = \"Vulnerable Population per Tract\",\n    caption = \"Each point represents a vulnerable census tract in Pennsylvania. Tracts farther from hospitals\\nwith larger vulnerable populations indicate high-priority areas for healthcare investment.\"\n\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\nSuggested chart types: - Histogram or density plot of distances - Box plot comparing distances across regions - Bar chart of underserved tracts by county - Scatter plot of distance vs. vulnerable population size\nRequirements: - Clear axes labels with units - Appropriate title - Professional formatting - Brief interpretation (1-2 sentences as a caption or in text)"
  },
  {
    "objectID": "assignments/assignment_2/Luu_Jun_Assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "assignments/assignment_2/Luu_Jun_Assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\nChoose your own additional spatial dataset and conduct a supplementary analysis.\n\nChallenge Options\nChoose ONE of the following challenge exercises, or propose your own research question using OpenDataPhilly data (https://opendataphilly.org/datasets/).\nNote these are just loose suggestions to spark ideas - follow or make your own as the data permits and as your ideas evolve. This analysis should include bringing in your own dataset, ensuring the projection/CRS of your layers align and are appropriate for the analysis (not lat/long or geodetic coordinate systems). The analysis portion should include some combination of spatial and attribute operations to answer a relatively straightforward question\n\n\nEducation & Youth Services\nOption A: Educational Desert Analysis - Data: Schools, Libraries, Recreation Centers, Census tracts (child population) - Question: “Which neighborhoods lack adequate educational infrastructure for children?” - Operations: Buffer schools/libraries (0.5 mile walking distance), identify coverage gaps, overlay with child population density - Policy relevance: School district planning, library placement, after-school program siting\nOption B: School Safety Zones - Data: Schools, Crime Incidents, Bike Network - Question: “Are school zones safe for walking/biking, or are they crime hotspots?” - Operations: Buffer schools (1000ft safety zone), spatial join with crime incidents, assess bike infrastructure coverage - Policy relevance: Safe Routes to School programs, crossing guard placement\n\n\n\nEnvironmental Justice\nOption C: Green Space Equity - Data: Parks, Street Trees, Census tracts (race/income demographics) - Question: “Do low-income and minority neighborhoods have equitable access to green space?” - Operations: Buffer parks (10-minute walk = 0.5 mile), calculate tree canopy or park acreage per capita, compare by demographics - Policy relevance: Climate resilience, environmental justice, urban forestry investment —\n\n\nPublic Safety & Justice\nOption D: Crime & Community Resources - Data: Crime Incidents, Recreation Centers, Libraries, Street Lights - Question: “Are high-crime areas underserved by community resources?” - Operations: Aggregate crime counts to census tracts or neighborhoods, count community resources per area, spatial correlation analysis - Policy relevance: Community investment, violence prevention strategies —\n\n\nInfrastructure & Services\nOption E: Polling Place Accessibility - Data: Polling Places, SEPTA stops, Census tracts (elderly population, disability rates) - Question: “Are polling places accessible for elderly and disabled voters?” - Operations: Buffer polling places and transit stops, identify vulnerable populations, find areas lacking access - Policy relevance: Voting rights, election infrastructure, ADA compliance\n\n\n\nHealth & Wellness\nOption F: Recreation & Population Health - Data: Recreation Centers, Playgrounds, Parks, Census tracts (demographics) - Question: “Is lack of recreation access associated with vulnerable populations?” - Operations: Calculate recreation facilities per capita by neighborhood, buffer facilities for walking access, overlay with demographic indicators - Policy relevance: Public health investment, recreation programming, obesity prevention\n\n\n\nEmergency Services\nOption G: EMS Response Coverage - Data: Fire Stations, EMS stations, Population density, High-rise buildings - Question: “Are population-dense areas adequately covered by emergency services?” - Operations: Create service area buffers (5-minute drive = ~2 miles), assess population coverage, identify gaps in high-density areas - Policy relevance: Emergency preparedness, station siting decisions\n\n\n\nArts & Culture\nOption H: Cultural Asset Distribution - Data: Public Art, Museums, Historic sites/markers, Neighborhoods - Question: “Do all neighborhoods have equitable access to cultural amenities?” - Operations: Count cultural assets per neighborhood, normalize by population, compare distribution across demographic groups - Policy relevance: Cultural equity, tourism, quality of life, neighborhood identity\n\n\n\n\nData Sources\nOpenDataPhilly: https://opendataphilly.org/datasets/ - Most datasets available as GeoJSON, Shapefile, or CSV with coordinates - Always check the Metadata for a data dictionary of the fields.\nAdditional Sources: - Pennsylvania Open Data: https://data.pa.gov/ - Census Bureau (via tidycensus): Demographics, economic indicators, commute patterns - TIGER/Line (via tigris): Geographic boundaries\n\n\nRecommended Starting Points\nIf you’re feeling confident: Choose an advanced challenge with multiple data layers. If you are a beginner, choose something more manageable that helps you understand the basics\nIf you have a different idea: Propose your own question! Just make sure: - You can access the spatial data - You can perform at least 2 spatial operations\n\n\nYour Analysis\nYour Task:\n\nFind and load additional data\n\nDocument your data source\nCheck and standardize the CRS\nProvide basic summary statistics\n\n\n\n# Load your additional dataset\nrecreation_centers &lt;- st_read(\"https://opendata.arcgis.com/api/v3/datasets/9eb26a787a6e448ba426eea7f9f0d93a_0/downloads/data?format=geojson&spatialRefId=4326\")\n\nReading layer `PPR_Program_Sites' from data source \n  `https://opendata.arcgis.com/api/v3/datasets/9eb26a787a6e448ba426eea7f9f0d93a_0/downloads/data?format=geojson&spatialRefId=4326' \n  using driver `GeoJSON'\nSimple feature collection with 171 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.2563 ymin: 39.90444 xmax: -74.96944 ymax: 40.12284\nGeodetic CRS:  WGS 84\n\n#check crs\nst_crs(pa_centroids)\n\nCoordinate Reference System:\n  User input: EPSG:3365 \n  wkt:\nPROJCRS[\"NAD83(HARN) / Pennsylvania South (ftUS)\",\n    BASEGEOGCRS[\"NAD83(HARN)\",\n        DATUM[\"NAD83 (High Accuracy Reference Network)\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4152]],\n    CONVERSION[\"SPCS83 Pennsylvania South zone (US survey foot)\",\n        METHOD[\"Lambert Conic Conformal (2SP)\",\n            ID[\"EPSG\",9802]],\n        PARAMETER[\"Latitude of false origin\",39.3333333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-77.75,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",40.9666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",39.9333333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",1968500,\n            LENGTHUNIT[\"US survey foot\",0.304800609601219],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"US survey foot\",0.304800609601219],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"US survey foot\",0.304800609601219]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"US survey foot\",0.304800609601219]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United States (USA) - Pennsylvania - counties of Adams; Allegheny; Armstrong; Beaver; Bedford; Berks; Blair; Bucks; Butler; Cambria; Chester; Cumberland; Dauphin; Delaware; Fayette; Franklin; Fulton; Greene; Huntingdon; Indiana; Juniata; Lancaster; Lawrence; Lebanon; Lehigh; Mifflin; Montgomery; Northampton; Perry; Philadelphia; Schuylkill; Snyder; Somerset; Washington; Westmoreland; York.\"],\n        BBOX[39.71,-80.53,41.18,-74.72]],\n    ID[\"EPSG\",3365]]\n\nst_crs(recreation_centers)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n#only use philly data\nphila_centroids &lt;- pa_centroids %&gt;%\n  filter(COUNTYFP == \"101\")\n#transform crs\nrecreation_centers &lt;- st_transform(recreation_centers, st_crs(phila_centroids))\nsummary(recreation_centers)\n\n    OBJECTID       PARK_NAME          DPP_ASSET_ID  PROGRAM_TYPE      \n Min.   : 807.0   Length:171         Min.   :   4   Length:171        \n 1st Qu.: 993.5   Class :character   1st Qu.:1826   Class :character  \n Median :1036.0   Mode  :character   Median :1891   Mode  :character  \n Mean   :1027.6                      Mean   :1795                     \n 3rd Qu.:1078.5                      3rd Qu.:1952                     \n Max.   :1121.0                      Max.   :3479                     \n  SITE_CLASS          BUILDING             GYM            LABEL_NUMBER      \n Length:171         Length:171         Length:171         Length:171        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   COMMENTS         DATA_SOURCE                 geometry  \n Length:171         Length:171         POINT        :171  \n Class :character   Class :character   epsg:3365    :  0  \n Mode  :character   Mode  :character   +proj=lcc ...:  0  \n                                                          \n                                                          \n                                                          \n\n\nQuestions to answer: - What dataset did you choose and why? I chose the recreation center location data to look at the question, “Is lack of recreation access associated with vulnerable populations?” - What is the data source and date? The data source is OpenDataPhilly and the date created is March 2, 2022. - How many features does it contain? It contains 11 features. - What CRS is it in? Did you need to transform it? It is in WGS 84 and I did need to transform it.\n\n\nPose a research question\n\nWrite a clear research statement that your analysis will answer.”Is lack of recreation access associated with vulnerable populations?”\nExamples: - “Do vulnerable tracts have adequate public transit access to hospitals?” - “Are EMS stations appropriately located near vulnerable populations?” - “Do areas with low vehicle access have worse hospital access?”\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\nRequired operations (choose 2+): - Buffers - Spatial joins - Spatial filtering with predicates - Distance calculations - Intersections or unions - Point-in-polygon aggregation\nYour Task:\n\n# Your spatial analysis\nrec_buffer &lt;- st_buffer(recreation_centers, dist = 2640) #.5 miles\n#centroids of recreation centers\nphila_centroids &lt;- phila_centroids %&gt;%\n  mutate(\n    within_recreation_buffer = lengths(st_intersects(geometry, rec_buffer)) &gt; 0\n  )\n#seeing results for both vulnerable and not vulnerable \nrec_summary &lt;- phila_centroids %&gt;%\n  st_drop_geometry() %&gt;%\n  summarise(\n    total_tracts = n(),\n    tracts_with_access = sum(within_recreation_buffer, na.rm = TRUE),\n    total_vulnerable_tracts = sum(vulnerable, na.rm = TRUE),\n    vulnerable_with_access = sum(vulnerable & within_recreation_buffer, na.rm = TRUE),\n    vulnerable_without_access = sum(vulnerable & !within_recreation_buffer, na.rm = TRUE),\n    pct_vulnerable_with_access = round(vulnerable_with_access / total_vulnerable_tracts * 100, 1)\n  )\n\nrec_summary\n\n  total_tracts tracts_with_access total_vulnerable_tracts\n1          408                390                       2\n  vulnerable_with_access vulnerable_without_access pct_vulnerable_with_access\n1                      1                         1                         50\n\n\n\n#to see vulnerable on ggplot\nphila_centroids &lt;- phila_centroids %&gt;%\n  mutate(\n    vul_access_cat = case_when(\n      !vulnerable ~ \"Not vulnerable\",\n      vulnerable & within_recreation_buffer ~ \"Vulnerable w/ access\",\n      vulnerable & !within_recreation_buffer ~ \"Vulnerable w/o access\"\n    )\n  )\n\n\nggplot() +\n  # Plot tracts colored by vulnerability + access category\n  geom_sf(\n    data = phila_centroids,\n    aes(fill = vul_access_cat),\n    color = \"white\",\n    size = 0.1\n  ) +\n  \n  # Recreation center buffer used\n  geom_sf(\n    data = rec_buffer,\n    fill = \"blue\",\n    alpha = 0.2,\n    color = NA\n  ) +\n  \n  # Recreation centers\n  geom_sf(\n    data = recreation_centers,\n    color = \"blue\",\n    size = 1,\n    shape = 21,\n    fill = \"white\",\n    stroke = 0.6\n  ) +\n  \n  # County boundary outline\n  geom_sf(\n    data = st_union(phila_centroids),\n    fill = NA,\n    color = \"gray\",\n    linewidth = 0.6\n  ) +\n  \n  # colors for each category\n  scale_fill_manual(\n    values = c(\n      \"Not vulnerable\" = \"gray\",\n      \"Vulnerable w/o access\" = \"red\",\n      \"Vulnerable w/ access\" = \"green\"\n    ),\n    name = \"Tract Category\"\n  ) +\n  \n  labs(\n    title = \"Recreation Access for Vulnerable Populations in Philadelphia\",\n  ) +\n  \n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5),\n    plot.caption = element_text(size = 9, hjust = 0.5),\n    legend.position = \"right\",\n    legend.title = element_text(size = 11),\n    legend.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\nAnalysis requirements: - Clear code comments explaining each step - Appropriate CRS transformations - Summary statistics or counts - At least one map showing your findings - Brief interpretation of results (3-5 sentences)\nYour interpretation:\nI hadn’t taken a look at the vulnerable tracts in Philadelphia specifically until part 3, which was interesting. There were a lot less vulnerable tracts in Philadelphia than I had originally assumed. Reviewing the recreation center locations, there are no centers in the vulnerable tracts. This could be due to lack of funding in these vulnerable tracts."
  },
  {
    "objectID": "assignments/assignment_2/Luu_Jun_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "assignments/assignment_2/Luu_Jun_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nTake a few moments to clean up your markdown document and then write a line or two or three about how you may have incorporated feedback that you recieved after your first assignment.\nI hid the Census API key and data-loading code in a setup chunk, so it runs without cluttering the document, and instead I printed the key outputs (like tables and summaries) in the console or in formatted tables. This made the final report cleaner, easier to read, and more professional while keeping the analysis fully reproducible."
  },
  {
    "objectID": "assignments/assignment_2/Luu_Jun_Assignment2.html#submission-requirements",
    "href": "assignments/assignment_2/Luu_Jun_Assignment2.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it’s a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\n\nFile naming: LastName_FirstName_Assignment2.html and LastName_FirstName_Assignment2.qmd"
  },
  {
    "objectID": "assignments/assignment_3/Luu_Jun_Appendix.html",
    "href": "assignments/assignment_3/Luu_Jun_Appendix.html",
    "title": "Philadelphia Housing Model—Technical Appendix",
    "section": "",
    "text": "Code\n# Import relevant libraries.\nlibrary(car)\nlibrary(dplyr)\nlibrary(ggcorrplot)\nlibrary(ggplot2)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(sf)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(tidycensus)\nlibrary(tidyr)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(tmap)\nlibrary(units)\n\noptions(scipen = 999)\n\n\n\n\nCode\n# Property data.\nproperties_path &lt;- \"data/philly_properties.csv\"\nproperties &lt;- read.csv(properties_path)\n\n# Capture dimensions.\nog_property_dimension &lt;- dim(properties)\n\n# Set Census API key.\ncensus_api_key(\"3aaee31789e10b674a531e9f236c35d5394b19ed\")\n\n\n\n\nCode\n# All variables are character strings, remove white space then convert numeric character variables to numeric classes for chosen variables.\n\nproperties &lt;- properties %&gt;%\n  mutate(across(where(is.character), str_trim),\n         across(c(fireplaces, garage_spaces, number_of_bathrooms, number_stories,\n                  sale_price, total_livable_area, year_built), as.numeric)) %&gt;%\n  rename(fireplace_num = fireplaces,\n         garage_num = garage_spaces,\n         bath_num = number_of_bathrooms,\n         story_num = number_stories,\n         square_feet = total_livable_area,\n         basement_type = basements,\n         ac_binary = central_air,\n         fuel_type = fuel,\n         )\n\n\n\n\nCode\n# Filter to residential properties and 2023-2024 sales.\n# Note: Category code #1 is for residential.\nresidential_prop &lt;- properties %&gt;%\n  filter(.,\n         category_code == 1,\n         startsWith(sale_date, \"2023\") | startsWith(sale_date, \"2024\"))\n\n# Drop empty variables or variables not needed for model.\nresidential_prop &lt;- residential_prop %&gt;%\n  select(c(basement_type, ac_binary, fireplace_num, fuel_type, garage_num,\n           bath_num, story_num, sale_date, sale_price,\n           square_feet, year_built, shape)\n         )\n\n# Make empty character column values NA.\nresidential_prop &lt;- residential_prop %&gt;%\n  mutate(across(where(is.character), ~na_if(., \"\")))\n\n\n\n\nCode\n# Drop prices that are less than $10,000 as a catch-all (might not be as reflective for rural areas). Avoiding dropping prices based on percent of assessed value since property assessments can be biased against minoritized communities. Ideal drop would add deed type to drop any family or forced transfers.\nresidential_prop &lt;- residential_prop %&gt;%\n  filter(sale_price &gt; 10000,\n         square_feet &gt; 0)\n\n\n\n\nCode\n# Create building age column, change central air to binary, and adjust basement and fuel types.\n# Create log value for the sale price.\nresidential_prop &lt;- residential_prop %&gt;%\n  mutate(ln_sale_price = log(sale_price),\n         age = 2025 - year_built,\n         ln_square_feet = log(square_feet),\n         ac_binary = case_when(\n           ac_binary == \"Y\" ~ 1,\n           ac_binary == \"N\" ~ 0),\n         basement_type = case_when(\n           basement_type == \"0\" ~ \"None\",\n           basement_type == \"A\" ~ \"Full Finished\",\n           basement_type == \"B\" ~ \"Full Semi-Finished\",\n           basement_type == \"C\" ~ \"Full Unfinished\",\n           basement_type == \"D\" ~ \"Full Unknown Finish\",\n           basement_type == \"E\" ~ \"Partial Finished\",\n           basement_type == \"F\" ~ \"Partial Semi-Finished\",\n           basement_type == \"G\" ~ \"Partial Unfinished\",\n           basement_type == \"H\" ~ \"Partial Unknown Finish\",\n           basement_type == \"I\" ~ \"Unknown Size Finished\",\n           basement_type == \"J\" ~ \"Unknown Size Unfinished\"),\n         fuel_type = case_when(\n           fuel_type == \"A\" ~ \"Natural Gas\",\n           fuel_type == \"B\" ~ \"Oil Heat\",\n           fuel_type == \"C\" ~ \"Electric\",\n           fuel_type == \"D\" ~ \"Coal\",\n           fuel_type == \"E\" ~ \"Solar\",\n           fuel_type == \"F\" ~ \"Wood\",\n           fuel_type == \"G\" ~ \"Other\",\n           fuel_type == \"H\" ~ \"None\")\n         )\n\n\n\n\nCode\n# Turn categorical data into factors so OLS regression doesn't handle the data as a list of strings.\nresidential_prop$basement_type &lt;- as.factor(residential_prop$basement_type)\nresidential_prop$fuel_type &lt;- as.factor(residential_prop$fuel_type)\n\n# Change the reference categories for baseline comparison.\nresidential_prop$basement_type &lt;- relevel(residential_prop$basement_type, ref = \"None\")\nresidential_prop$fuel_type &lt;- relevel(residential_prop$fuel_type, ref = \"Natural Gas\")\n\n# Place fuel type with 10 or less counts into other category.\nresidential_prop &lt;- residential_prop %&gt;%\n  mutate(fuel_type = fct_lump_min(fuel_type, min = 11, other_level = \"Other\"))\n\n\n\n\nCode\n# Fixed effect temporal market fluctuations. Based on sale date, splitting the years into quarters (Q1, Q2, Q3, Q4). Potential fixed effect.\nresidential_prop &lt;- residential_prop %&gt;%\n  mutate(\n    quarters_fe = quarter(as_datetime(sale_date))\n    )\n\n# Make it a factor.\nresidential_prop$quarters_fe &lt;- factor(residential_prop$quarters_fe)\n\n\n\n\nCode\n# Capture dimensions.\nafter_property_dimension &lt;- dim(residential_prop)\n\n# Convert residential property to geodataframe. Use EPSG 2272 for South Pennsylvania in feet.\n# Drop shape when finished creating geometry.\nresidential_prop_gdf &lt;- residential_prop %&gt;%\n  mutate(geometry = st_as_sfc(shape)) %&gt;%\n  st_as_sf(crs = 2272) %&gt;%\n  rename(geometry_point = geometry) %&gt;%\n  select(-c(shape))"
  },
  {
    "objectID": "assignments/assignment_3/Luu_Jun_Appendix.html#primary-philadelphia-sales-data-opendataphilly",
    "href": "assignments/assignment_3/Luu_Jun_Appendix.html#primary-philadelphia-sales-data-opendataphilly",
    "title": "Philadelphia Housing Model—Technical Appendix",
    "section": "",
    "text": "Code\n# Import relevant libraries.\nlibrary(car)\nlibrary(dplyr)\nlibrary(ggcorrplot)\nlibrary(ggplot2)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(sf)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(tidycensus)\nlibrary(tidyr)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(tmap)\nlibrary(units)\n\noptions(scipen = 999)\n\n\n\n\nCode\n# Property data.\nproperties_path &lt;- \"data/philly_properties.csv\"\nproperties &lt;- read.csv(properties_path)\n\n# Capture dimensions.\nog_property_dimension &lt;- dim(properties)\n\n# Set Census API key.\ncensus_api_key(\"3aaee31789e10b674a531e9f236c35d5394b19ed\")\n\n\n\n\nCode\n# All variables are character strings, remove white space then convert numeric character variables to numeric classes for chosen variables.\n\nproperties &lt;- properties %&gt;%\n  mutate(across(where(is.character), str_trim),\n         across(c(fireplaces, garage_spaces, number_of_bathrooms, number_stories,\n                  sale_price, total_livable_area, year_built), as.numeric)) %&gt;%\n  rename(fireplace_num = fireplaces,\n         garage_num = garage_spaces,\n         bath_num = number_of_bathrooms,\n         story_num = number_stories,\n         square_feet = total_livable_area,\n         basement_type = basements,\n         ac_binary = central_air,\n         fuel_type = fuel,\n         )\n\n\n\n\nCode\n# Filter to residential properties and 2023-2024 sales.\n# Note: Category code #1 is for residential.\nresidential_prop &lt;- properties %&gt;%\n  filter(.,\n         category_code == 1,\n         startsWith(sale_date, \"2023\") | startsWith(sale_date, \"2024\"))\n\n# Drop empty variables or variables not needed for model.\nresidential_prop &lt;- residential_prop %&gt;%\n  select(c(basement_type, ac_binary, fireplace_num, fuel_type, garage_num,\n           bath_num, story_num, sale_date, sale_price,\n           square_feet, year_built, shape)\n         )\n\n# Make empty character column values NA.\nresidential_prop &lt;- residential_prop %&gt;%\n  mutate(across(where(is.character), ~na_if(., \"\")))\n\n\n\n\nCode\n# Drop prices that are less than $10,000 as a catch-all (might not be as reflective for rural areas). Avoiding dropping prices based on percent of assessed value since property assessments can be biased against minoritized communities. Ideal drop would add deed type to drop any family or forced transfers.\nresidential_prop &lt;- residential_prop %&gt;%\n  filter(sale_price &gt; 10000,\n         square_feet &gt; 0)\n\n\n\n\nCode\n# Create building age column, change central air to binary, and adjust basement and fuel types.\n# Create log value for the sale price.\nresidential_prop &lt;- residential_prop %&gt;%\n  mutate(ln_sale_price = log(sale_price),\n         age = 2025 - year_built,\n         ln_square_feet = log(square_feet),\n         ac_binary = case_when(\n           ac_binary == \"Y\" ~ 1,\n           ac_binary == \"N\" ~ 0),\n         basement_type = case_when(\n           basement_type == \"0\" ~ \"None\",\n           basement_type == \"A\" ~ \"Full Finished\",\n           basement_type == \"B\" ~ \"Full Semi-Finished\",\n           basement_type == \"C\" ~ \"Full Unfinished\",\n           basement_type == \"D\" ~ \"Full Unknown Finish\",\n           basement_type == \"E\" ~ \"Partial Finished\",\n           basement_type == \"F\" ~ \"Partial Semi-Finished\",\n           basement_type == \"G\" ~ \"Partial Unfinished\",\n           basement_type == \"H\" ~ \"Partial Unknown Finish\",\n           basement_type == \"I\" ~ \"Unknown Size Finished\",\n           basement_type == \"J\" ~ \"Unknown Size Unfinished\"),\n         fuel_type = case_when(\n           fuel_type == \"A\" ~ \"Natural Gas\",\n           fuel_type == \"B\" ~ \"Oil Heat\",\n           fuel_type == \"C\" ~ \"Electric\",\n           fuel_type == \"D\" ~ \"Coal\",\n           fuel_type == \"E\" ~ \"Solar\",\n           fuel_type == \"F\" ~ \"Wood\",\n           fuel_type == \"G\" ~ \"Other\",\n           fuel_type == \"H\" ~ \"None\")\n         )\n\n\n\n\nCode\n# Turn categorical data into factors so OLS regression doesn't handle the data as a list of strings.\nresidential_prop$basement_type &lt;- as.factor(residential_prop$basement_type)\nresidential_prop$fuel_type &lt;- as.factor(residential_prop$fuel_type)\n\n# Change the reference categories for baseline comparison.\nresidential_prop$basement_type &lt;- relevel(residential_prop$basement_type, ref = \"None\")\nresidential_prop$fuel_type &lt;- relevel(residential_prop$fuel_type, ref = \"Natural Gas\")\n\n# Place fuel type with 10 or less counts into other category.\nresidential_prop &lt;- residential_prop %&gt;%\n  mutate(fuel_type = fct_lump_min(fuel_type, min = 11, other_level = \"Other\"))\n\n\n\n\nCode\n# Fixed effect temporal market fluctuations. Based on sale date, splitting the years into quarters (Q1, Q2, Q3, Q4). Potential fixed effect.\nresidential_prop &lt;- residential_prop %&gt;%\n  mutate(\n    quarters_fe = quarter(as_datetime(sale_date))\n    )\n\n# Make it a factor.\nresidential_prop$quarters_fe &lt;- factor(residential_prop$quarters_fe)\n\n\n\n\nCode\n# Capture dimensions.\nafter_property_dimension &lt;- dim(residential_prop)\n\n# Convert residential property to geodataframe. Use EPSG 2272 for South Pennsylvania in feet.\n# Drop shape when finished creating geometry.\nresidential_prop_gdf &lt;- residential_prop %&gt;%\n  mutate(geometry = st_as_sfc(shape)) %&gt;%\n  st_as_sf(crs = 2272) %&gt;%\n  rename(geometry_point = geometry) %&gt;%\n  select(-c(shape))"
  },
  {
    "objectID": "assignments/assignment_3/Luu_Jun_Appendix.html#spatial-data-opendataphilly",
    "href": "assignments/assignment_3/Luu_Jun_Appendix.html#spatial-data-opendataphilly",
    "title": "Philadelphia Housing Model—Technical Appendix",
    "section": "Spatial Data (OpenDataPhilly)",
    "text": "Spatial Data (OpenDataPhilly)\n\n\nCode\n# Read in Philadelpha census tracts.\nphilly_tracts_path &lt;- \"data/philly_tracts/philly_tracts.shp\"\nphilly_tracts &lt;- st_read(philly_tracts_path)\n\n\nReading layer `philly_tracts' from data source \n  `/Users/jenniferluu/Documents/5080portfolio-setup-jenniferluu6/assignments/assignment_3/data/philly_tracts/philly_tracts.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3446 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -80.51985 ymin: 39.7198 xmax: -74.68956 ymax: 42.51607\nGeodetic CRS:  NAD83\n\n\nCode\n# Match CRS.\nphilly_tracts &lt;- st_transform(philly_tracts, crs = 2272)\n\n# Left spatial join.\nresidential_points &lt;- st_join(residential_prop_gdf, philly_tracts)\n\n# Drop unnecessary columns and remove incomplete observations (rows) for upcoming spatial feature computations.\nresidential_points &lt;- residential_points %&gt;%\n  select(-c(FUNCSTAT, MTFCC, NAMELSAD, NAME,\n            STATEFP, COUNTYFP, TRACTCE)) %&gt;%\n  na.omit(.)\n\n# Remove unneeded datasets for housekeeping and call garbage collector to reduce memory.\nrm(properties, residential_prop, residential_prop_gdf)\ngc()\n\n\n           used  (Mb) gc trigger  (Mb) limit (Mb)  max used   (Mb)\nNcells  3562911 190.3   11772104 628.7         NA  10689264  570.9\nVcells 12820016  97.9  112059716 855.0      16384 140027659 1068.4\n\n\n\n\nCode\n# Proximity to downtown.\n\n# Decided on Euclidean distance because network proximity computation is demanding on thousands of points, even with parallel programming.\n\n# Create single Center City point feature based on City Hall coordinates.\ncenter_city &lt;- st_sfc(st_point(c(-75.163500, 39.952800)), crs = 4326) %&gt;%\n  st_transform(crs = 2272)\n\n# Need to add mile units for operations. Then remove units object for easier plotting.\nresidential_points$city_dist_mi &lt;- (st_distance(residential_points, st_union(center_city))) %&gt;%\n  set_units(\"mi\") %&gt;%\n  drop_units() %&gt;%\n  as.numeric()\n\n# Log transform because distance benefit diminishes, for potential use.\nresidential_points$ln_city_dist &lt;- log(residential_points$city_dist_mi + 0.1)\n\n\n\n\nCode\n# Transit proximity.\n# Major cities could be distance to nearest transit like metro/rail stations, but suburban and rural areas might be better served by distance to nearest major highway.\n# Read in SEPTA stops.\nsepta_stops_path &lt;- \"data/septa_stops.csv\"\n\nsepta_stops_df &lt;- read.csv(septa_stops_path)\n\n# Make csv a geodataframe.\nsepta_stops &lt;- septa_stops_df %&gt;%\n  st_as_sf(., coords = c(\"Lon\", \"Lat\"), crs = 4326)\n\n# Match CRS.\nsepta_stops &lt;- septa_stops %&gt;%\n  st_transform(., crs = 2272)\n\n# Stops are duplicated for the same station because the data includes directions for all cardinal directions as well as bus, rail, and trolley for the same location. This means a single station could have more than one point representing a single location residents go to commute.\n# Create new column with stop name without the cardinal suffixes and keep only the unique station values.\nsepta_stops &lt;- septa_stops %&gt;%\n  mutate(stations = if_else(\n    str_detect(StopAbbr, \"NO$|SO$|EA$|WE$|NE$|NW$|SE$|SW$\"),\n    str_sub(StopAbbr, end = -3),\n    str_sub(StopAbbr)\n    )\n  ) %&gt;%\n  distinct(stations, .keep_all = TRUE)\n\n# Create buffer zone for stops within a half mile. This is ~10 minute walk, depending on topography.\n# Note: EPSG 2272 is measured in feet, not miles.\nsepta_distance &lt;- st_buffer(residential_points, 2640)\n\n# Create number of stops in the buffer zone.\nsepta_stations &lt;- st_intersects(septa_distance, septa_stops)\n\n# Append buffer zone counts and put into main tract data. Create a logged version for potential use as well because distance benefit tapers off.\nresidential_points &lt;- residential_points %&gt;%\n  mutate(\n    septa_half_mi = lengths(septa_stations),\n    ln_septa_half_mi = log(septa_half_mi + 0.1)\n  )\n\n\n\n\nCode\n# Park proximity / size. Measuring distance is important for accessibility, but the size of the park often matters because a property near a block-sized pocket of green space is not equivalent to being near a large one like Wissahickon Valley Park.\n\n# Read in geojson data.\nparks_path &lt;- \"data/parks.geojson\"\n\nparks &lt;- st_read(parks_path)\n\n\nReading layer `parks' from data source \n  `/Users/jenniferluu/Documents/5080portfolio-setup-jenniferluu6/assignments/assignment_3/data/parks.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 63 features and 18 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -75.2837 ymin: 39.87048 xmax: -74.95865 ymax: 40.13191\nGeodetic CRS:  WGS 84\n\n\nCode\n# Match CRS and filter by parks.\nparks &lt;- parks %&gt;%\n  st_transform(., crs = 2272) %&gt;%\n  filter(str_detect(USE_, \"PARK\"))\n\n# Get distance to the edge of the nearest park.\n# Note: Don't try to do spatial operations in apply() and mutate().\n# Distance matrix of residential properties to parks.\nparks_matrix &lt;- st_distance(residential_points, parks)\n\n# Get the nearest distance for each point.\nresidential_points$parks_mile &lt;- apply(parks_matrix, 1, min)\n\n# Convert to miles.\nresidential_points$parks_mile &lt;- as.numeric(residential_points$parks_mile) / 5280\n\n# Log parks data for potential use because of diminishing distance benefits.\nresidential_points$ln_park_dist &lt;- as.numeric(log(residential_points$parks_mile + 0.1))\n\n\n\n\nCode\n# Convenience/Food points of interest. Using kNN to measure the density of these amenities rather than nearest amenity point.\namenities_path &lt;- \"data/osm_pois/osm_pois.shp\"\namenities &lt;- st_read(amenities_path)\n\n\nReading layer `osm_pois' from data source \n  `/Users/jenniferluu/Documents/5080portfolio-setup-jenniferluu6/assignments/assignment_3/data/osm_pois/osm_pois.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 65127 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.52111 ymin: 39.71816 xmax: -74.69473 ymax: 42.25797\nGeodetic CRS:  WGS 84\n\n\nCode\n# Filter amenities by convenience and food.\namenities &lt;- amenities %&gt;%\n  filter(fclass %in% c(\n    \"atm\", \"bakery\", \"bank\", \"bar\", \"beauty_shop\", \"biergarten\", \"bookshop\",\n    \"butcher\", \"cafe\", \"convenience\", \"department_store\", \"fast_food\", \"food_court\",\n    \"greengrocer\", \"hairdresser\", \"kiosk\", \"laundry\", \"market_place\", \"pharmacy\",\n    \"mall\", \"pub\", \"restaurant\", \"supermarket\"\n  )\n         ) %&gt;%\n  st_transform(., crs = 2272)\n\n# Distance matrix of residential properties to amenities.\namenities_matrix &lt;- st_distance(residential_points, amenities)\n\n\n\n\nCode\n# k-Nearest Neighbors (kNN) function.\nknn_distance &lt;- function(distance_matrix, k) {\n  apply(distance_matrix, 1, function(distances){\n    mean(as.numeric(sort(distances)[1:k]))\n  })\n}\n\n# Create kNN feature for amenities. k = 4 to balance for urban and suburban areas, probably not as representative of rural areas.\nresidential_points &lt;- residential_points %&gt;%\n  mutate(\n    knn_amenity_mi = as.numeric(knn_distance(amenities_matrix, k = 4))\n  )\n\n# Convert to miles.\nresidential_points$knn_amenity_mi &lt;- as.numeric(residential_points$knn_amenity_mi / 5280)\n\n\n\n\nCode\n# Fixed effect neighborhoods.\nneighborhoods_path &lt;- \"data/philadelphia_neighborhoods/philadelphia_neighborhoods.shp\"\nphilly_neighborhoods &lt;- st_read(neighborhoods_path)\n\n\nReading layer `philadelphia_neighborhoods' from data source \n  `/Users/jenniferluu/Documents/5080portfolio-setup-jenniferluu6/assignments/assignment_3/data/philadelphia_neighborhoods/philadelphia_neighborhoods.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 159 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -75.28026 ymin: 39.86701 xmax: -74.95576 ymax: 40.13799\nGeodetic CRS:  WGS 84\n\n\nCode\n# Match CRS.\nphilly_neighborhoods &lt;- philly_neighborhoods %&gt;%\n  st_transform(., crs = 2272)\n\n# Join to residential points and rename to neighborhoods.\nresidential_points &lt;- residential_points %&gt;%\n  st_join(., philly_neighborhoods) %&gt;%\n  rename(neighborhood_fe = MAPNAME)\n\n# Make the neighborhoods a factor.\nresidential_points$neighborhood_fe &lt;- relevel(factor(residential_points$neighborhood_fe), ref = \"East Falls\")\n\n# Place neighborhoods with 10 or less sales into a small neighborhoods category.\nresidential_points &lt;- residential_points %&gt;%\n  mutate(neighborhood_fe = fct_lump_min(neighborhood_fe, min = 11, other_level = \"Small Neighborhoods\"))\n\n\n\n\nCode\n# Capture spatial feature dimensions.\nafter_feat_eng_dimension &lt;- dim(residential_points)\n\n\n\n\nCode\n# Spatial feature creation table, select spatial features into a separate data frame and drop geometry.\nspatial_feature_df &lt;- residential_points %&gt;%\n  select(c(city_dist_mi, ln_city_dist, septa_half_mi, ln_septa_half_mi,\n           parks_mile, ln_park_dist, knn_amenity_mi)) %&gt;%\n  na.omit(.) %&gt;%\n  st_drop_geometry()\n\n# Create a tibble from the selected spatial features.\nspatial_summary &lt;- tibble(\n  \"Spatial Features\" = names(spatial_feature_df),\n  \"Description\" = c(\"Distance from city (mi).\", \"Log of distance from city.\", \"Within 0.5mi of SEPTA station.\",\n                    \"Log of 0.5 SEPTA station.\", \"Distance from nearest park (mi).\",\n                    \"Log of distance from nearest park.\", \"k-Nearest Neighbors convenience and food amenities.\")\n)\n\n# Make Kable of spatial features.\nspatial_kable &lt;- kable(spatial_summary,\n                       caption = \"Feature Engineered Variables\",\n                       format.args = list(big.mark = \",\")\n) %&gt;%\n  kable_styling(latex_options = \"striped\",\n                full_width = FALSE) %&gt;%\n  column_spec(1, bold = TRUE, width = \"5cm\") %&gt;%\n  row_spec(0, color = \"#f5f4f0\", background = \"#ff4100\", bold = TRUE)\n\nspatial_kable\n\n\n\nFeature Engineered Variables\n\n\nSpatial Features\nDescription\n\n\n\n\ncity_dist_mi\nDistance from city (mi).\n\n\nln_city_dist\nLog of distance from city.\n\n\nsepta_half_mi\nWithin 0.5mi of SEPTA station.\n\n\nln_septa_half_mi\nLog of 0.5 SEPTA station.\n\n\nparks_mile\nDistance from nearest park (mi).\n\n\nln_park_dist\nLog of distance from nearest park.\n\n\nknn_amenity_mi\nk-Nearest Neighbors convenience and food amenities.\n\n\n\n\n\nPrimary:\nFrom the CSV, we are analyzing the conditions of basements, number of fireplaces, garage spaces, number of bathrooms, number of stories, the total livable area in square feet, the existence of central air, and type of fuel used on the property. We filtered residential category code with the sale dates between 2023 and 2024. We eliminated property prices &lt;10k. Rather than adhering to the percentage of assessed value as a guide for this filter, for it could incorporate marginalized bias, filtering the property prices removes non-market transactions but still incorporates a wide diversity of communities.\nThe forced the central air characteristic to become binary rather than “Y” and “N” and made sure to turn the categorical data to factors. The reference categories for types of basements is “None” and for fuel type, it’s “Natural Gas”. We including a building age category computed from the year built data. We logged the square footage and the sale price to correct for right-skewedness.\nSpatial:\nWe inserted the Philadelphia census tracts, changing the CRS to 2272, the ideal projection for SE Pennsylvania analysis. We decided to perform a log transformation on the following variables - city_dist (distance from City Hall in Center City), septa_half_mi (half mile buffer zone from all septa stops within the city geometry), and parks_dist (distance to edge of nearest park in miles) - because their effects on housing prices were non-linear. This transformation ensures that changes in proximity are measured more consistently across the range of distances, rather than being dominated by properties very close to these features.\nRegarding amenities, we used k-NN (k nearest neighbors) to measure the density of amenity accessibility rather than individual point data. The amenities are as follows: ATM, bakery, bank, bar, beauty shop, biergarten, bookshop, butcher, café, convenience, department store, fast food, food court, greengrocer, hairdresser, kiosk, laundry, marketplace, pharmacy, mall, pub, restaurant, supermarket. We filtered by convenience and food, transformed the CRS to 2272 for consistency, and then developed a matrix. The distance was inverted, log-transformed to account for diminishing returns, and scaled it to produce a single numeric value in which higher, positive values indicate greater accessibility to amenities.\nWe included neighborhoods as fixed effects to help explain unknown, unquantifiable factors like cultural atmosphere and other neighborhood-specific factors we cannot statistically account for that may influence housing prices. It was converted into a factor so each neighborhood can receive its own baseline model. Fiscal quarters were also introduced as fixed effects; splitting a year into 4 quarters for unknown factors when it comes to purchasing property (e.g. people are more likely to buy real estate in spring and summer).\n\nCensus Data (TidyCensus)\n\n\nCode\n# Open tidycensus data. Using 2023 data, because we are looking at sales 2023-2024\nacs_vars &lt;- load_variables(2023, \"acs5\", cache = TRUE)\n\n# Get acs dimensions.\nog_acs_dimension &lt;- dim(acs_vars)\n\n# The variables that we want from tidycensus\nvariables &lt;- c(\n  median_household_income = \"B19013_001\",\n  total_pop = \"B01003_001\",\n  poverty_white = \"B17001A_001\", # To get poverty percentage\n  poverty_black = \"B17001B_001\",\n  poverty_native = \"B17001C_001\",\n  poverty_asian = \"B17001D_001\",\n  poverty_islander = \"B17001E_001\",\n  poverty_other = \"B17001F_001\",\n  poverty_multiracial = \"B17001G_001\",\n  male_18_24_bach = \"B15001_009\", # Tracts only show bachelor's degrees, unless we want to look at only people 25+\n  male_25_34_bach = \"B15001_017\",\n  male_35_44_bach = \"B15001_025\",\n  male_45_64_bach = \"B15001_033\",\n  male_65plus_bach = \"B15001_041\",\n  female_18_24_bach = \"B15001_050\",\n  female_25_34_bach = \"B15001_058\",\n  female_35_44_bach = \"B15001_066\",\n  female_45_64_bach = \"B15001_074\",\n  female_65plus_bach = \"B15001_082\",\n  total_vacant = \"B25005_001\", # To get vacancy percentage\n  white_total_units = \"B25032A_001\", # Need total units to get percentage of single, detached units and vacant units.\n  white_single_family = \"B25032A_002\",\n  black_total_units = \"B25032B_001\",\n  black_single_family = \"B25032B_002\",\n  native_total_units = \"B25032C_001\",\n  native_single_family = \"B25032C_002\",\n  asian_total_units = \"B25032D_001\",\n  asian_single_family = \"B25032D_002\",\n  islander_total_units = \"B25032E_001\",\n  islander_single_family = \"B25032E_002\",\n  other_total_units = \"B25032F_001\",\n  other_single_family = \"B25032F_002\",\n  multiracial_total_units = \"B25032G_001\",\n  multiracial_single_family = \"B25032G_002\",\n  medhhinc_white = \"B19013A_001\", # Median Household Income\n  medhhinc_black = \"B19013B_001\",\n  medhhinc_native = \"B19013C_001\", \n  medhhinc_asian = \"B19013D_001\", \n  medhhinc_other = \"B19013F_001\",  # There is no tract data for native hawiian/pacific islander, I'm including it with other\n  medhhinc_multiracial = \"B19013G_001\", \n  white_pop = \"B01001A_001\",\n  black_pop = \"B01001B_001\",\n  native_pop = \"B01001C_001\",\n  asian_pop = \"B01001D_001\",\n  islander_pop = \"B01001E_001\",\n  other_pop = \"B01001F_001\",\n  multiracial_pop = \"B01001G_001\"\n)\n\n# We are grouping our data by tracts\nphilly_tract_acs &lt;- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  variables = variables,\n  year = 2022,\n  survey = \"acs5\",\n  cache_table = TRUE, \n  output = \"wide\"\n)\n\n\n\n\nCode\n# Summing up the variables that we need to create our percentage variables\nphilly_tract_acs &lt;- philly_tract_acs %&gt;%\n  mutate(\n    total_poverty = poverty_whiteE + poverty_blackE + poverty_nativeE + poverty_asianE + poverty_islanderE + poverty_otherE + poverty_multiracialE, # Adding all poverty populations together \n    \n    total_bach = male_18_24_bachE + male_25_34_bachE + male_35_44_bachE + male_45_64_bachE + male_65plus_bachE + female_18_24_bachE + female_25_34_bachE + female_35_44_bachE + female_45_64_bachE + female_65plus_bachE, #Adding all bachelors degrees together\n    \n    total_units = white_total_unitsE + black_total_unitsE + native_total_unitsE + asian_total_unitsE + islander_total_unitsE + other_total_unitsE + multiracial_total_unitsE, # Total housing units\n    \n    total_single_family = white_single_familyE + black_single_familyE + native_single_familyE + asian_single_familyE + islander_single_familyE + other_single_familyE + multiracial_single_familyE # Total single family homes\n  )\n\n\n\n\nCode\n# Creating our variables that we are going to analyze\nphilly_tract_acs &lt;- philly_tract_acs %&gt;%\n  mutate(\n    pct_poverty = (total_poverty/total_popE)*100, # Divide total poverty population by total population\n\n    pct_bach = (total_bach/total_popE)*100, # Divide bachelor degree holders by total population\n    \n    pct_vacant = (total_vacantE/total_units)*100, # Divide vacant units by total housing units\n    pct_vacant = ifelse(is.infinite(pct_vacant) | total_vacantE &gt; total_units, 100, pct_vacant), # Fixing errors when units equal zero or high MOE\n    \n    pct_single_family = (total_single_family/total_units)*100, # Divide single family homes by total housing units\n    \n    medhhinc = \n  (ifelse(is.na(medhhinc_whiteE), 0, medhhinc_whiteE) * white_popE +\n   ifelse(is.na(medhhinc_blackE), 0, medhhinc_blackE) * black_popE +\n   ifelse(is.na(medhhinc_nativeE), 0, medhhinc_nativeE) * native_popE +\n   ifelse(is.na(medhhinc_asianE), 0, medhhinc_asianE) * asian_popE +\n   ifelse(is.na(medhhinc_otherE), 0, medhhinc_otherE) * (islander_popE + other_popE) +\n   ifelse(is.na(medhhinc_multiracialE), 0, medhhinc_multiracialE) * multiracial_popE) / total_popE)\n# For median household income, I had to turn all median household incomes that were NA to 0, so that it would not mess up the formula. \n# Multiplying median household income times population by race. There was no islander median household income, so I included it in other. All divided by the total population, to get the total median household income. \n\n\n\n\nCode\n# Creating a summary table \nphilly_acs_summary &lt;- philly_tract_acs %&gt;%\n  select(\n    GEOID, \n    NAME,\n    pct_poverty,\n    pct_bach,\n    pct_vacant,\n    pct_single_family,\n    medhhinc\n  )\n\n# Get after acs dimension.\nafter_acs_dimension &lt;- dim(philly_acs_summary)\n\n\n\n\nCode\n# Join primary and census data.\nfinal_data &lt;- residential_points %&gt;%\n  left_join(philly_acs_summary, by = \"GEOID\") %&gt;%\n  select(-c(sale_date, year_built, ALAND, AWATER,\n            INTPTLAT, INTPTLON, NAME.x, LISTNAME, NAME.y,\n            Shape_Leng, Shape_Area)\n         )\n\n\n\n\nCode\n# Create key variables list.\nkey_columns &lt;- c(\"sale_price\", \"ln_sale_price\", \"square_feet\", \"ln_square_feet\",\n                 \"bath_num\", \"fireplace_num\", \"garage_num\", \"ac_binary\",\n                 \"story_num\", \"age\", \"city_dist_mi\", \"ln_city_dist\",\n                 \"septa_half_mi\", \"ln_septa_half_mi\", \"parks_mile\", \"ln_park_dist\",\n                 \"knn_amenity_mi\", \"pct_poverty\", \"pct_bach\",\n                 \"pct_vacant\", \"pct_single_family\", \"medhhinc\",\n                 \"basement_type\", \"fuel_type\", \"neighborhood_fe\", \"quarters_fe\")\n\n# Reorder for key columns first and drop all rows with NA because OLS needs complete observations.\nfinal_data &lt;- final_data %&gt;%\n  select(any_of(key_columns), everything()) %&gt;%\n  na.omit(.)\n\n# Get final dimension.\nfinal_dimension &lt;- dim(final_data)\n\n\n\n\nCode\n# Separate before/after dimensions for data.\ndimensions &lt;- data.frame(\n  rows_columns = c(\"Rows\", \"Columns\"),\n  \"Property Data Before\" = og_property_dimension,\n  \"Property Data After\" = after_property_dimension,\n  \"Property Data After Feature Engineering\" = after_feat_eng_dimension,\n  \"ACS Data Before\" = og_acs_dimension,\n  \"ACS Data After\" = after_acs_dimension,\n  \"Final Data\" = final_dimension\n)\n\n# Make Kable of dimensions.\ndimensions_kable &lt;- kable(dimensions,\n                          col.names = c(\"Dimensions\", \"Property Data Before\", \"Property Data After\",\n                                        \"Property Data After Feature Engineering\",\n                                        \"ACS Data Before\", \"ACS Data After\", \"Final Data\"),\n                          digits = 2,\n                          caption = \"Before and After Data Dimensions\",\n                          format.args = list(big.mark = \",\")\n) %&gt;%\n  kable_styling(latex_options = \"striped\",\n                full_width = FALSE) %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  row_spec(0, color = \"#f5f4f0\", background = \"#ff4100\", bold = TRUE)\n\ndimensions_kable\n\n\n\nBefore and After Data Dimensions\n\n\nDimensions\nProperty Data Before\nProperty Data After\nProperty Data After Feature Engineering\nACS Data Before\nACS Data After\nFinal Data\n\n\n\n\nRows\n583,776\n26,344\n13,884\n28,261\n3,446\n13,883\n\n\nColumns\n79\n16\n33\n4\n7\n28\n\n\n\n\n\nCensus:\nUsing tidycensus, we imported all variables that aligned with our structural data from 2023 ACS data by tracts: median household income, total population, poverty by ethnicity (White, Black, Native American, Asian, Pacific Islander, “Other,” Multiracial), males and females aged 18–65+ with bachelor’s degrees or higher, total vacancy, and total housing units by ethnicity, as well as single-family households and median household income per ethnic group. We compiled the individual poverty, bachelor’s degree, unit, and single-family household counts by ethnicity to form the following percentage variables: total_poverty, total_bach, total_units, and total_single_family.\nFrom this, we calculated pct_poverty, pct_bach, pct_vacant (accounting for ACS errors), pct_single_family, and medhhinc, transforming NAs to 0 for regression analysis.\nUsing our residential property vector data (which includes structural, spatial, and feature-engineered variables), we performed a left join on the cleaned ACS summary data by GEOID.\nAfter organizing the final dataset so key variables appear first, we generated a kable summarizing the workflow. The final dataset contains 26,344 observations and 29 columns."
  },
  {
    "objectID": "assignments/assignment_3/Luu_Jun_Appendix.html#model-building-progression",
    "href": "assignments/assignment_3/Luu_Jun_Appendix.html#model-building-progression",
    "title": "Philadelphia Housing Model—Technical Appendix",
    "section": "Model Building Progression",
    "text": "Model Building Progression\nCheck for multicollinearity:\n\n\nCode\n# VIF check for multicollinearity.\nvif_check &lt;- lm(ln_sale_price ~ ln_square_feet + bath_num + ac_binary + fireplace_num + story_num  + garage_num + ln_septa_half_mi + ln_park_dist + ln_city_dist + basement_type + fuel_type, data = residential_points)\n\nvif(vif_check)\n\n\n                     GVIF Df GVIF^(1/(2*Df))\nln_square_feet   2.135764  1        1.461425\nbath_num         1.898126  1        1.377725\nac_binary        1.326611  1        1.151786\nfireplace_num    1.254647  1        1.120110\nstory_num        1.480500  1        1.216758\ngarage_num       2.278251  1        1.509387\nln_septa_half_mi 3.022487  1        1.738530\nln_park_dist     1.042356  1        1.020958\nln_city_dist     3.366197  1        1.834720\nbasement_type    3.098173 10        1.058170\nfuel_type        1.050158  3        1.008190\n\n\n\n\nCode\n# Build Model step by step\n# First Model (structural features only)\n\nfirst_model &lt;- lm(sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2), # Age polynomial.\n                    data = final_data)\n\nsummary(first_model)\n\n\n\nCall:\nlm(formula = sale_price ~ ln_square_feet + bath_num + ac_binary + \n    fireplace_num + story_num + garage_num + basement_type + \n    fuel_type + age + I(age^2), data = final_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-948550  -77612  -12427   56061 4967680 \n\nCoefficients:\n                                          Estimate    Std. Error t value\n(Intercept)                          -1573115.2430    47184.2963 -33.340\nln_square_feet                         265324.4547     6777.4520  39.148\nbath_num                                50976.0380     2850.5451  17.883\nac_binary                               93358.6805     3519.3817  26.527\nfireplace_num                          139754.2587     4574.3312  30.552\nstory_num                               36185.3161     3367.8324  10.744\ngarage_num                              73507.4314     4149.3241  17.716\nbasement_typeFull Finished             -53619.3219    10227.7625  -5.243\nbasement_typeFull Semi-Finished        -64747.7385    11812.4468  -5.481\nbasement_typeFull Unfinished           -69284.7145    10632.8706  -6.516\nbasement_typeFull Unknown Finish       -85987.4342    10988.0644  -7.826\nbasement_typePartial Finished         -119781.8006    10844.6785 -11.045\nbasement_typePartial Semi-Finished    -130192.6848    11305.7582 -11.516\nbasement_typePartial Unfinished       -124518.1899    13722.2343  -9.074\nbasement_typePartial Unknown Finish   -131660.7994    13342.8310  -9.868\nbasement_typeUnknown Size Finished      64719.7760    49966.7638   1.295\nbasement_typeUnknown Size Unfinished   -25066.6493    38151.1732  -0.657\nfuel_typeElectric                      -10365.3404    10223.5159  -1.014\nfuel_typeOil Heat                       -2321.1682    20691.7649  -0.112\nfuel_typeOther                         261307.5132    62624.4546   4.173\nage                                     -4531.8642      146.4668 -30.941\nI(age^2)                                   26.6178        0.8083  32.929\n                                                 Pr(&gt;|t|)    \n(Intercept)                          &lt; 0.0000000000000002 ***\nln_square_feet                       &lt; 0.0000000000000002 ***\nbath_num                             &lt; 0.0000000000000002 ***\nac_binary                            &lt; 0.0000000000000002 ***\nfireplace_num                        &lt; 0.0000000000000002 ***\nstory_num                            &lt; 0.0000000000000002 ***\ngarage_num                           &lt; 0.0000000000000002 ***\nbasement_typeFull Finished            0.00000016071883817 ***\nbasement_typeFull Semi-Finished       0.00000004295551073 ***\nbasement_typeFull Unfinished          0.00000000007465800 ***\nbasement_typeFull Unknown Finish      0.00000000000000542 ***\nbasement_typePartial Finished        &lt; 0.0000000000000002 ***\nbasement_typePartial Semi-Finished   &lt; 0.0000000000000002 ***\nbasement_typePartial Unfinished      &lt; 0.0000000000000002 ***\nbasement_typePartial Unknown Finish  &lt; 0.0000000000000002 ***\nbasement_typeUnknown Size Finished                  0.195    \nbasement_typeUnknown Size Unfinished                0.511    \nfuel_typeElectric                                   0.311    \nfuel_typeOil Heat                                   0.911    \nfuel_typeOther                        0.00003029641753816 ***\nage                                  &lt; 0.0000000000000002 ***\nI(age^2)                             &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 176400 on 13861 degrees of freedom\nMultiple R-squared:  0.5896,    Adjusted R-squared:  0.5889 \nF-statistic: 948.1 on 21 and 13861 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nCode\n# Build Model step by step\n# Second Model (structural features + census features)\n\nsecond_model &lt;- lm(sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2) + # Age polynomial.\n                    medhhinc + pct_vacant + pct_single_family, # Census feature.\n                    data = final_data)\n\nsummary(second_model)\n\n\n\nCall:\nlm(formula = sale_price ~ ln_square_feet + bath_num + ac_binary + \n    fireplace_num + story_num + garage_num + basement_type + \n    fuel_type + age + I(age^2) + medhhinc + pct_vacant + pct_single_family, \n    data = final_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-988077  -66190   -4479   51392 5089350 \n\nCoefficients:\n                                           Estimate     Std. Error t value\n(Intercept)                          -1631653.69177    43478.62618 -37.528\nln_square_feet                         249024.29196     6287.93299  39.604\nbath_num                                57294.02210     2625.66512  21.821\nac_binary                               51858.97037     3354.26227  15.461\nfireplace_num                          123699.62363     4248.12112  29.119\nstory_num                                8121.18745     3187.48631   2.548\ngarage_num                              78147.86081     3858.85587  20.252\nbasement_typeFull Finished             -11441.07419     9433.38338  -1.213\nbasement_typeFull Semi-Finished        -28190.97570    10881.72867  -2.591\nbasement_typeFull Unfinished           -26373.93425     9803.25374  -2.690\nbasement_typeFull Unknown Finish       -35981.99248    10140.98926  -3.548\nbasement_typePartial Finished          -81503.33848    10010.20710  -8.142\nbasement_typePartial Semi-Finished     -80042.76840    10459.75246  -7.652\nbasement_typePartial Unfinished        -73358.61614    12658.62517  -5.795\nbasement_typePartial Unknown Finish    -84936.85419    12306.96737  -6.902\nbasement_typeUnknown Size Finished     100022.70014    45911.66164   2.179\nbasement_typeUnknown Size Unfinished    18865.34248    35068.79357   0.538\nfuel_typeElectric                        3565.49288     9416.44401   0.379\nfuel_typeOil Heat                       -9369.15017    19014.09219  -0.493\nfuel_typeOther                         149197.56776    57590.28952   2.591\nage                                     -3249.53619      137.00469 -23.718\nI(age^2)                                   20.10269        0.75712  26.552\nmedhhinc                                    2.64611        0.05506  48.061\npct_vacant                                383.54766      219.97666   1.744\npct_single_family                       -1835.25686      167.77302 -10.939\n                                                 Pr(&gt;|t|)    \n(Intercept)                          &lt; 0.0000000000000002 ***\nln_square_feet                       &lt; 0.0000000000000002 ***\nbath_num                             &lt; 0.0000000000000002 ***\nac_binary                            &lt; 0.0000000000000002 ***\nfireplace_num                        &lt; 0.0000000000000002 ***\nstory_num                                        0.010850 *  \ngarage_num                           &lt; 0.0000000000000002 ***\nbasement_typeFull Finished                       0.225216    \nbasement_typeFull Semi-Finished                  0.009589 ** \nbasement_typeFull Unfinished                     0.007147 ** \nbasement_typeFull Unknown Finish                 0.000389 ***\nbasement_typePartial Finished        0.000000000000000422 ***\nbasement_typePartial Semi-Finished   0.000000000000021017 ***\nbasement_typePartial Unfinished      0.000000006974481709 ***\nbasement_typePartial Unknown Finish  0.000000000005368236 ***\nbasement_typeUnknown Size Finished               0.029379 *  \nbasement_typeUnknown Size Unfinished             0.590619    \nfuel_typeElectric                                0.704957    \nfuel_typeOil Heat                                0.622199    \nfuel_typeOther                                   0.009589 ** \nage                                  &lt; 0.0000000000000002 ***\nI(age^2)                             &lt; 0.0000000000000002 ***\nmedhhinc                             &lt; 0.0000000000000002 ***\npct_vacant                                       0.081254 .  \npct_single_family                    &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 162000 on 13858 degrees of freedom\nMultiple R-squared:  0.6538,    Adjusted R-squared:  0.6532 \nF-statistic:  1091 on 24 and 13858 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nCode\n# Build Model step by step\n# Third Model (structural features + census features + spatial features)\n\nthird_model &lt;- lm(sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2) + # Age polynomial.\n                    medhhinc + pct_vacant + pct_single_family + # Census feature.\n                    city_dist_mi + septa_half_mi + ln_park_dist + knn_amenity_mi, # Spatial \n                    data = final_data)\n\nsummary(third_model)\n\n\n\nCall:\nlm(formula = sale_price ~ ln_square_feet + bath_num + ac_binary + \n    fireplace_num + story_num + garage_num + basement_type + \n    fuel_type + age + I(age^2) + medhhinc + pct_vacant + pct_single_family + \n    city_dist_mi + septa_half_mi + ln_park_dist + knn_amenity_mi, \n    data = final_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-817667  -62638   -2348   51585 5096901 \n\nCoefficients:\n                                          Estimate    Std. Error t value\n(Intercept)                          -1755937.2954    42209.8105 -41.600\nln_square_feet                         259188.8862     6030.4108  42.980\nbath_num                                51003.1282     2524.1100  20.206\nac_binary                               46002.9566     3224.9122  14.265\nfireplace_num                          127882.4271     4065.5162  31.455\nstory_num                               -3293.9735     3080.5023  -1.069\ngarage_num                              87635.7897     3702.7940  23.667\nbasement_typeFull Finished               9025.2270     9060.4446   0.996\nbasement_typeFull Semi-Finished          -655.1732    10446.8183  -0.063\nbasement_typeFull Unfinished            -6175.5266     9422.9158  -0.655\nbasement_typeFull Unknown Finish       -15053.7679     9744.2956  -1.545\nbasement_typePartial Finished          -37741.0741     9667.9264  -3.904\nbasement_typePartial Semi-Finished     -34526.5862    10142.6823  -3.404\nbasement_typePartial Unfinished        -43195.2313    12139.1172  -3.558\nbasement_typePartial Unknown Finish    -52068.7616    11806.6007  -4.410\nbasement_typeUnknown Size Finished     125455.6640    43907.0332   2.857\nbasement_typeUnknown Size Unfinished    36845.8296    33537.6606   1.099\nfuel_typeElectric                        2967.6188     9004.5144   0.330\nfuel_typeOil Heat                        -796.3198    18184.1313  -0.044\nfuel_typeOther                         104285.2749    55084.8694   1.893\nage                                     -2593.0061      132.4070 -19.584\nI(age^2)                                   14.7489        0.7398  19.935\nmedhhinc                                    2.1122        0.0560  37.717\npct_vacant                              -1163.6035      224.4922  -5.183\npct_single_family                         724.0007      182.9895   3.957\ncity_dist_mi                            -2507.4907      766.2599  -3.272\nsepta_half_mi                            1848.1046       70.2789  26.297\nln_park_dist                           -16888.4011     2091.3400  -8.075\nknn_amenity_mi                         -12260.4575     8546.2417  -1.435\n                                                 Pr(&gt;|t|)    \n(Intercept)                          &lt; 0.0000000000000002 ***\nln_square_feet                       &lt; 0.0000000000000002 ***\nbath_num                             &lt; 0.0000000000000002 ***\nac_binary                            &lt; 0.0000000000000002 ***\nfireplace_num                        &lt; 0.0000000000000002 ***\nstory_num                                        0.284954    \ngarage_num                           &lt; 0.0000000000000002 ***\nbasement_typeFull Finished                       0.319213    \nbasement_typeFull Semi-Finished                  0.949994    \nbasement_typeFull Unfinished                     0.512238    \nbasement_typeFull Unknown Finish                 0.122398    \nbasement_typePartial Finished        0.000095166072865703 ***\nbasement_typePartial Semi-Finished               0.000666 ***\nbasement_typePartial Unfinished                  0.000374 ***\nbasement_typePartial Unknown Finish  0.000010408240366613 ***\nbasement_typeUnknown Size Finished               0.004279 ** \nbasement_typeUnknown Size Unfinished             0.271944    \nfuel_typeElectric                                0.741730    \nfuel_typeOil Heat                                0.965071    \nfuel_typeOther                                   0.058355 .  \nage                                  &lt; 0.0000000000000002 ***\nI(age^2)                             &lt; 0.0000000000000002 ***\nmedhhinc                             &lt; 0.0000000000000002 ***\npct_vacant                           0.000000221096385096 ***\npct_single_family                    0.000076430420795608 ***\ncity_dist_mi                                     0.001069 ** \nsepta_half_mi                        &lt; 0.0000000000000002 ***\nln_park_dist                         0.000000000000000728 ***\nknn_amenity_mi                                   0.151423    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 154900 on 13854 degrees of freedom\nMultiple R-squared:  0.6837,    Adjusted R-squared:  0.6831 \nF-statistic:  1069 on 28 and 13854 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nCode\n# Build Model step by step\n# Final Model \n## (structural features + census features + spatial features + interactions and fixed effects)\n\nfinal_model &lt;- lm(sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2) + # Age polynomial.\n                    medhhinc + pct_vacant + pct_single_family + # Census feature.\n                    city_dist_mi + septa_half_mi + ln_park_dist + knn_amenity_mi + # Spatial \n                    knn_amenity_mi * city_dist_mi + septa_half_mi * city_dist_mi + # Interaction between amenities and downtown distance.\n                    neighborhood_fe + quarters_fe, # Fixed effect \n                    data = final_data)\n\nsummary(final_model)\n\n\n\nCall:\nlm(formula = sale_price ~ ln_square_feet + bath_num + ac_binary + \n    fireplace_num + story_num + garage_num + basement_type + \n    fuel_type + age + I(age^2) + medhhinc + pct_vacant + pct_single_family + \n    city_dist_mi + septa_half_mi + ln_park_dist + knn_amenity_mi + \n    knn_amenity_mi * city_dist_mi + septa_half_mi * city_dist_mi + \n    neighborhood_fe + quarters_fe, data = final_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-798051  -47316    2231   44345 5123884 \n\nCoefficients:\n                                                 Estimate     Std. Error\n(Intercept)                                -1657940.22470    45493.46650\nln_square_feet                               261653.15436     5664.41060\nbath_num                                      43843.65408     2308.64344\nac_binary                                     38117.05447     3019.61163\nfireplace_num                                 88491.39266     3861.20497\nstory_num                                     -3914.30380     2936.48610\ngarage_num                                    77030.43653     3411.56673\nbasement_typeFull Finished                   101483.65131     8830.21379\nbasement_typeFull Semi-Finished               92361.27832     9993.78345\nbasement_typeFull Unfinished                  81610.24126     9130.96925\nbasement_typeFull Unknown Finish              72063.11376     9381.23915\nbasement_typePartial Finished                 49757.63088     9336.90901\nbasement_typePartial Semi-Finished            45706.97947     9840.50165\nbasement_typePartial Unfinished               38542.17995    11431.04311\nbasement_typePartial Unknown Finish           32484.57614    11155.03267\nbasement_typeUnknown Size Finished           168383.76890    39699.41756\nbasement_typeUnknown Size Unfinished         130060.38774    30365.34179\nfuel_typeElectric                             -9782.82669     8165.97207\nfuel_typeOil Heat                              3778.65607    16324.04269\nfuel_typeOther                                 8483.98198    49580.06057\nage                                           -1482.81195      130.24128\nI(age^2)                                          5.87768        0.74553\nmedhhinc                                          0.62950        0.08448\npct_vacant                                    -1005.64567      291.81015\npct_single_family                               219.33928      214.90003\ncity_dist_mi                                  -4819.21361     3219.40481\nsepta_half_mi                                  1424.38286      194.01801\nln_park_dist                                  -6234.17054     4838.09208\nknn_amenity_mi                               -20833.95786    24021.24910\nneighborhood_feAcademy Gardens                37183.75011    29063.24192\nneighborhood_feAllegheny West                -81178.66645    19747.80365\nneighborhood_feAndorra                       -36899.86992    30639.87807\nneighborhood_feAston-Woodbridge               37471.10986    33193.50178\nneighborhood_feBella Vista                    90030.05119    24534.03655\nneighborhood_feBelmont                       -73260.20102    41183.35267\nneighborhood_feBrewerytown                   -70887.58170    18851.85190\nneighborhood_feBridesburg                      8200.62849    22173.10061\nneighborhood_feBurholme                      -11020.59219    36141.98192\nneighborhood_feBustleton                      26845.36972    24665.90193\nneighborhood_feCarroll Park                  -79485.63764    20533.40133\nneighborhood_feCedar Park                     50903.74805    21594.31513\nneighborhood_feCedarbrook                    -18733.12094    23527.99233\nneighborhood_feChestnut Hill                 405617.68702    20989.45599\nneighborhood_feClearview                     -84924.33030    32006.87846\nneighborhood_feCobbs Creek                   -77562.70250    16239.01668\nneighborhood_feDickinson Narrows             -46228.73425    20672.82929\nneighborhood_feDunlap                       -149077.53577    34850.29349\nneighborhood_feEast Germantown               -55725.61363    22284.70084\nneighborhood_feEast Kensington               -24931.83758    18649.97574\nneighborhood_feEast Mount Airy               -39672.38823    18326.30068\nneighborhood_feEast Oak Lane                -110720.66613    29189.26470\nneighborhood_feEast Parkside                 -96875.29736    43009.02090\nneighborhood_feEast Passyunk                  18548.68326    19771.17863\nneighborhood_feEastwick                      -65187.86687    34422.87528\nneighborhood_feElmwood                       -65408.97379    18135.99045\nneighborhood_feFairhill                      -71704.26714    37417.40279\nneighborhood_feFairmount                      79935.76169    18138.51850\nneighborhood_feFeltonville                   -64193.35248    19842.10485\nneighborhood_feFern Rock                     -80858.30468    34391.53574\nneighborhood_feFishtown - Lower Kensington     9753.74777    15479.96362\nneighborhood_feFitler Square                 433898.63083    30666.44346\nneighborhood_feFox Chase                      -1191.12744    21434.30138\nneighborhood_feFrancisville                  -55199.70458    23448.72689\nneighborhood_feFrankford                     -65752.33751    18632.41316\nneighborhood_feFranklinville                -135326.00022    29388.65347\nneighborhood_feGermantown - Morton           -73568.42918    26505.05993\nneighborhood_feGermantown - Penn Knox       -136518.39681    44353.19600\nneighborhood_feGermantown - Westside         -98563.13632    39334.28353\nneighborhood_feGermany Hill                   10231.17331    26115.45737\nneighborhood_feGirard Estates                -30603.49061    18939.42860\nneighborhood_feGlenwood                     -119474.06331    28556.83370\nneighborhood_feGraduate Hospital              97520.93433    19973.04887\nneighborhood_feGrays Ferry                   -75481.41036    17444.55221\nneighborhood_feGreenwich                     -64175.19929    25230.25907\nneighborhood_feHaddington                    -91270.64590    18398.19928\nneighborhood_feHarrowgate                    -62121.18902    18977.91421\nneighborhood_feHartranft                    -131582.73560    22639.15057\nneighborhood_feHawthorne                      13169.95713    28842.84687\nneighborhood_feHolmesburg                    -17670.00910    20551.32095\nneighborhood_feHunting Park                  -53147.58934    22061.51403\nneighborhood_feJuniata Park                  -49408.97887    17918.47259\nneighborhood_feKingsessing                  -100245.71687    17213.22477\nneighborhood_feLawndale                      -38945.07939    17685.32938\nneighborhood_feLexington Park                 23243.39571    28523.99591\nneighborhood_feLogan                        -102555.05340    19864.76119\nneighborhood_feLogan Square                  379046.61233    30173.19574\nneighborhood_feLower Moyamensing             -65365.63965    17491.07739\nneighborhood_feManayunk                       28356.63404    18199.96310\nneighborhood_feMantua                        -92910.84396    27416.06170\nneighborhood_feMayfair                       -11299.33352    18328.17970\nneighborhood_feMelrose Park Gardens          -68278.66749    32017.80130\nneighborhood_feMill Creek                   -108864.06595    26183.51708\nneighborhood_feMillbrook                      15534.97223    30551.88468\nneighborhood_feModena                         55854.04856    28537.16201\nneighborhood_feMorrell Park                   25251.89869    27932.79846\nneighborhood_feNewbold                       -54636.97578    21808.17215\nneighborhood_feNicetown                      -66893.42947    36298.67571\nneighborhood_feNormandy Village              155197.36009    49122.79282\nneighborhood_feNorth Central                -101900.04242    22183.92696\nneighborhood_feNorthern Liberties              2692.49730    19255.79096\nneighborhood_feNorthwood                     -96763.98083    23348.68911\nneighborhood_feOgontz                        -38842.22862    20138.09837\nneighborhood_feOld City                      443334.48569    45204.28297\nneighborhood_feOld Kensington                -46454.88136    20353.62973\nneighborhood_feOlney                         -68389.05352    17153.25904\nneighborhood_feOverbrook                     -93077.05144    16987.27692\nneighborhood_feOxford Circle                 -22701.66053    17820.65848\nneighborhood_fePacker Park                     9689.89314    25692.21263\nneighborhood_feParkwood Manor                 51865.84423    29534.39315\nneighborhood_fePaschall                      -77498.68652    19758.48273\nneighborhood_fePassyunk Square                 4243.48331    20608.82553\nneighborhood_fePennsport                     -32232.31694    19668.11315\nneighborhood_fePennypack                       -229.17130    23906.35132\nneighborhood_fePennypack Woods                10601.67627    46721.77819\nneighborhood_fePenrose                       -58590.52553    30805.52778\nneighborhood_fePoint Breeze                  -63076.08386    17718.03825\nneighborhood_feQueen Village                 110051.36389    21297.63346\nneighborhood_feRhawnhurst                     13210.76897    21751.53460\nneighborhood_feRichmond                      -28186.33567    15369.92197\nneighborhood_feRittenhouse                   428527.90443    26394.39500\nneighborhood_feRoxborough                     10264.37966    16135.89529\nneighborhood_feRoxborough Park               -33966.75004    32811.40721\nneighborhood_feSharswood                     -96773.71513    41275.66768\nneighborhood_feSociety Hill                  350959.85928    26133.87223\nneighborhood_feSomerton                       57933.82996    29085.20858\nneighborhood_feSouthwest Germantown         -105279.23491    24716.20835\nneighborhood_feSouthwest Schuylkill          -90431.06270    20983.94325\nneighborhood_feSpring Garden                 178866.21603    25856.32707\nneighborhood_feSpruce Hill                   134662.38807    30063.65847\nneighborhood_feStadium District              -23294.11604    21985.18636\nneighborhood_feStanton                      -147735.64393    20701.69637\nneighborhood_feStrawberry Mansion           -107463.96962    20368.92240\nneighborhood_feSummerdale                    -40151.90279    21740.17259\nneighborhood_feTacony                        -38167.86864    19893.82475\nneighborhood_feTioga                        -119532.35201    23356.75045\nneighborhood_feTorresdale                    -23879.53124    27055.42084\nneighborhood_feUpper Kensington              -95731.86958    18344.45529\nneighborhood_feUpper Roxborough              -38230.35488    20207.99113\nneighborhood_feWalnut Hill                   -59138.01438    30653.12012\nneighborhood_feWashington Square West         88690.10153    29937.37012\nneighborhood_feWest Central Germantown        16807.75200    27884.51013\nneighborhood_feWest Kensington               -68127.87269    20317.08101\nneighborhood_feWest Mount Airy                38179.51090    18510.39444\nneighborhood_feWest Oak Lane                 -45315.59922    18269.26015\nneighborhood_feWest Passyunk                 -79646.98859    19772.56079\nneighborhood_feWest Poplar                  -110299.67792    43554.69769\nneighborhood_feWest Powelton                -104040.85024    34399.77899\nneighborhood_feWhitman                       -42920.67535    18272.47676\nneighborhood_feWinchester Park                38808.93027    34718.30402\nneighborhood_feWissahickon                    -5389.48604    20201.84628\nneighborhood_feWissahickon Hills              12442.07991    32378.82072\nneighborhood_feWissinoming                   -23043.18919    18367.46550\nneighborhood_feWister                        -68117.95065    31454.19955\nneighborhood_feWynnefield                    -99224.93028    20126.46981\nneighborhood_feSmall Neighborhoods           -26240.56901    19047.76223\nquarters_fe2                                   3325.18036     3378.65124\nquarters_fe3                                   4564.25634     3438.64092\nquarters_fe4                                   5216.49275     3566.47726\ncity_dist_mi:knn_amenity_mi                    1654.98301     2642.51941\ncity_dist_mi:septa_half_mi                     -332.55888       41.45544\n                                           t value             Pr(&gt;|t|)    \n(Intercept)                                -36.443 &lt; 0.0000000000000002 ***\nln_square_feet                              46.192 &lt; 0.0000000000000002 ***\nbath_num                                    18.991 &lt; 0.0000000000000002 ***\nac_binary                                   12.623 &lt; 0.0000000000000002 ***\nfireplace_num                               22.918 &lt; 0.0000000000000002 ***\nstory_num                                   -1.333             0.182558    \ngarage_num                                  22.579 &lt; 0.0000000000000002 ***\nbasement_typeFull Finished                  11.493 &lt; 0.0000000000000002 ***\nbasement_typeFull Semi-Finished              9.242 &lt; 0.0000000000000002 ***\nbasement_typeFull Unfinished                 8.938 &lt; 0.0000000000000002 ***\nbasement_typeFull Unknown Finish             7.682  0.00000000000001677 ***\nbasement_typePartial Finished                5.329  0.00000010024344867 ***\nbasement_typePartial Semi-Finished           4.645  0.00000343597723092 ***\nbasement_typePartial Unfinished              3.372             0.000749 ***\nbasement_typePartial Unknown Finish          2.912             0.003596 ** \nbasement_typeUnknown Size Finished           4.241  0.00002235191311615 ***\nbasement_typeUnknown Size Unfinished         4.283  0.00001854901682209 ***\nfuel_typeElectric                           -1.198             0.230938    \nfuel_typeOil Heat                            0.231             0.816947    \nfuel_typeOther                               0.171             0.864134    \nage                                        -11.385 &lt; 0.0000000000000002 ***\nI(age^2)                                     7.884  0.00000000000000341 ***\nmedhhinc                                     7.452  0.00000000000009754 ***\npct_vacant                                  -3.446             0.000570 ***\npct_single_family                            1.021             0.307435    \ncity_dist_mi                                -1.497             0.134435    \nsepta_half_mi                                7.341  0.00000000000022310 ***\nln_park_dist                                -1.289             0.197573    \nknn_amenity_mi                              -0.867             0.385785    \nneighborhood_feAcademy Gardens               1.279             0.200775    \nneighborhood_feAllegheny West               -4.111  0.00003966371497638 ***\nneighborhood_feAndorra                      -1.204             0.228491    \nneighborhood_feAston-Woodbridge              1.129             0.258973    \nneighborhood_feBella Vista                   3.670             0.000244 ***\nneighborhood_feBelmont                      -1.779             0.075282 .  \nneighborhood_feBrewerytown                  -3.760             0.000170 ***\nneighborhood_feBridesburg                    0.370             0.711503    \nneighborhood_feBurholme                     -0.305             0.760428    \nneighborhood_feBustleton                     1.088             0.276456    \nneighborhood_feCarroll Park                 -3.871             0.000109 ***\nneighborhood_feCedar Park                    2.357             0.018424 *  \nneighborhood_feCedarbrook                   -0.796             0.425926    \nneighborhood_feChestnut Hill                19.325 &lt; 0.0000000000000002 ***\nneighborhood_feClearview                    -2.653             0.007980 ** \nneighborhood_feCobbs Creek                  -4.776  0.00000180380477018 ***\nneighborhood_feDickinson Narrows            -2.236             0.025354 *  \nneighborhood_feDunlap                       -4.278  0.00001901511843807 ***\nneighborhood_feEast Germantown              -2.501             0.012409 *  \nneighborhood_feEast Kensington              -1.337             0.181300    \nneighborhood_feEast Mount Airy              -2.165             0.030422 *  \nneighborhood_feEast Oak Lane                -3.793             0.000149 ***\nneighborhood_feEast Parkside                -2.252             0.024310 *  \nneighborhood_feEast Passyunk                 0.938             0.348175    \nneighborhood_feEastwick                     -1.894             0.058281 .  \nneighborhood_feElmwood                      -3.607             0.000311 ***\nneighborhood_feFairhill                     -1.916             0.055343 .  \nneighborhood_feFairmount                     4.407  0.00001056260193703 ***\nneighborhood_feFeltonville                  -3.235             0.001218 ** \nneighborhood_feFern Rock                    -2.351             0.018732 *  \nneighborhood_feFishtown - Lower Kensington   0.630             0.528647    \nneighborhood_feFitler Square                14.149 &lt; 0.0000000000000002 ***\nneighborhood_feFox Chase                    -0.056             0.955684    \nneighborhood_feFrancisville                 -2.354             0.018584 *  \nneighborhood_feFrankford                    -3.529             0.000419 ***\nneighborhood_feFranklinville                -4.605  0.00000416765856661 ***\nneighborhood_feGermantown - Morton          -2.776             0.005517 ** \nneighborhood_feGermantown - Penn Knox       -3.078             0.002088 ** \nneighborhood_feGermantown - Westside        -2.506             0.012230 *  \nneighborhood_feGermany Hill                  0.392             0.695236    \nneighborhood_feGirard Estates               -1.616             0.106147    \nneighborhood_feGlenwood                     -4.184  0.00002885494372713 ***\nneighborhood_feGraduate Hospital             4.883  0.00000105860760820 ***\nneighborhood_feGrays Ferry                  -4.327  0.00001522694286549 ***\nneighborhood_feGreenwich                    -2.544             0.010983 *  \nneighborhood_feHaddington                   -4.961  0.00000071026962559 ***\nneighborhood_feHarrowgate                   -3.273             0.001065 ** \nneighborhood_feHartranft                    -5.812  0.00000000630356266 ***\nneighborhood_feHawthorne                     0.457             0.647958    \nneighborhood_feHolmesburg                   -0.860             0.389915    \nneighborhood_feHunting Park                 -2.409             0.016007 *  \nneighborhood_feJuniata Park                 -2.757             0.005833 ** \nneighborhood_feKingsessing                  -5.824  0.00000000588257898 ***\nneighborhood_feLawndale                     -2.202             0.027674 *  \nneighborhood_feLexington Park                0.815             0.415160    \nneighborhood_feLogan                        -5.163  0.00000024686521012 ***\nneighborhood_feLogan Square                 12.562 &lt; 0.0000000000000002 ***\nneighborhood_feLower Moyamensing            -3.737             0.000187 ***\nneighborhood_feManayunk                      1.558             0.119242    \nneighborhood_feMantua                       -3.389             0.000704 ***\nneighborhood_feMayfair                      -0.617             0.537574    \nneighborhood_feMelrose Park Gardens         -2.133             0.032982 *  \nneighborhood_feMill Creek                   -4.158  0.00003233743685558 ***\nneighborhood_feMillbrook                     0.508             0.611126    \nneighborhood_feModena                        1.957             0.050340 .  \nneighborhood_feMorrell Park                  0.904             0.365999    \nneighborhood_feNewbold                      -2.505             0.012245 *  \nneighborhood_feNicetown                     -1.843             0.065371 .  \nneighborhood_feNormandy Village              3.159             0.001585 ** \nneighborhood_feNorth Central                -4.593  0.00000439924270724 ***\nneighborhood_feNorthern Liberties            0.140             0.888798    \nneighborhood_feNorthwood                    -4.144  0.00003428980342821 ***\nneighborhood_feOgontz                       -1.929             0.053777 .  \nneighborhood_feOld City                      9.807 &lt; 0.0000000000000002 ***\nneighborhood_feOld Kensington               -2.282             0.022482 *  \nneighborhood_feOlney                        -3.987  0.00006727698685702 ***\nneighborhood_feOverbrook                    -5.479  0.00000004347320880 ***\nneighborhood_feOxford Circle                -1.274             0.202722    \nneighborhood_fePacker Park                   0.377             0.706066    \nneighborhood_feParkwood Manor                1.756             0.079091 .  \nneighborhood_fePaschall                     -3.922  0.00008813490635267 ***\nneighborhood_fePassyunk Square               0.206             0.836867    \nneighborhood_fePennsport                    -1.639             0.101276    \nneighborhood_fePennypack                    -0.010             0.992352    \nneighborhood_fePennypack Woods               0.227             0.820496    \nneighborhood_fePenrose                      -1.902             0.057199 .  \nneighborhood_fePoint Breeze                 -3.560             0.000372 ***\nneighborhood_feQueen Village                 5.167  0.00000024082503649 ***\nneighborhood_feRhawnhurst                    0.607             0.543629    \nneighborhood_feRichmond                     -1.834             0.066696 .  \nneighborhood_feRittenhouse                  16.236 &lt; 0.0000000000000002 ***\nneighborhood_feRoxborough                    0.636             0.524708    \nneighborhood_feRoxborough Park              -1.035             0.300588    \nneighborhood_feSharswood                    -2.345             0.019063 *  \nneighborhood_feSociety Hill                 13.429 &lt; 0.0000000000000002 ***\nneighborhood_feSomerton                      1.992             0.046406 *  \nneighborhood_feSouthwest Germantown         -4.260  0.00002062292616807 ***\nneighborhood_feSouthwest Schuylkill         -4.310  0.00001647368688246 ***\nneighborhood_feSpring Garden                 6.918  0.00000000000479383 ***\nneighborhood_feSpruce Hill                   4.479  0.00000755137153481 ***\nneighborhood_feStadium District             -1.060             0.289374    \nneighborhood_feStanton                      -7.136  0.00000000000100613 ***\nneighborhood_feStrawberry Mansion           -5.276  0.00000013413070739 ***\nneighborhood_feSummerdale                   -1.847             0.064783 .  \nneighborhood_feTacony                       -1.919             0.055058 .  \nneighborhood_feTioga                        -5.118  0.00000031349740898 ***\nneighborhood_feTorresdale                   -0.883             0.377460    \nneighborhood_feUpper Kensington             -5.219  0.00000018293556101 ***\nneighborhood_feUpper Roxborough             -1.892             0.058533 .  \nneighborhood_feWalnut Hill                  -1.929             0.053718 .  \nneighborhood_feWashington Square West        2.963             0.003057 ** \nneighborhood_feWest Central Germantown       0.603             0.546676    \nneighborhood_feWest Kensington              -3.353             0.000801 ***\nneighborhood_feWest Mount Airy               2.063             0.039170 *  \nneighborhood_feWest Oak Lane                -2.480             0.013134 *  \nneighborhood_feWest Passyunk                -4.028  0.00005651845672808 ***\nneighborhood_feWest Poplar                  -2.532             0.011338 *  \nneighborhood_feWest Powelton                -3.024             0.002495 ** \nneighborhood_feWhitman                      -2.349             0.018842 *  \nneighborhood_feWinchester Park               1.118             0.263662    \nneighborhood_feWissahickon                  -0.267             0.789641    \nneighborhood_feWissahickon Hills             0.384             0.700787    \nneighborhood_feWissinoming                  -1.255             0.209658    \nneighborhood_feWister                       -2.166             0.030357 *  \nneighborhood_feWynnefield                   -4.930  0.00000083160419737 ***\nneighborhood_feSmall Neighborhoods          -1.378             0.168343    \nquarters_fe2                                 0.984             0.325047    \nquarters_fe3                                 1.327             0.184417    \nquarters_fe4                                 1.463             0.143587    \ncity_dist_mi:knn_amenity_mi                  0.626             0.531135    \ncity_dist_mi:septa_half_mi                  -8.022  0.00000000000000112 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 138300 on 13723 degrees of freedom\nMultiple R-squared:  0.7501,    Adjusted R-squared:  0.7472 \nF-statistic:   259 on 159 and 13723 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nCode\n# Step model to check for most influential predictors.\nstep_model &lt;- step(final_model, direction = \"both\")\n\n\nStart:  AIC=328831.2\nsale_price ~ ln_square_feet + bath_num + ac_binary + fireplace_num + \n    story_num + garage_num + basement_type + fuel_type + age + \n    I(age^2) + medhhinc + pct_vacant + pct_single_family + city_dist_mi + \n    septa_half_mi + ln_park_dist + knn_amenity_mi + knn_amenity_mi * \n    city_dist_mi + septa_half_mi * city_dist_mi + neighborhood_fe + \n    quarters_fe\n\n                               Df      Sum of Sq             RSS    AIC\n- fuel_type                     3    29291306230 262526956909507 328827\n- quarters_fe                   3    48687286101 262546352889378 328828\n- city_dist_mi:knn_amenity_mi   1     7502864212 262505168467489 328830\n- pct_single_family             1    19926739927 262517592343204 328830\n- ln_park_dist                  1    31760364605 262529425967882 328831\n- story_num                     1    33988307880 262531653911157 328831\n&lt;none&gt;                                           262497665603277 328831\n- pct_vacant                    1   227177637523 262724843240800 328841\n- medhhinc                      1  1062196974143 263559862577420 328885\n- I(age^2)                      1  1188920399920 263686586003197 328892\n- city_dist_mi:septa_half_mi    1  1230978421267 263728644024543 328894\n- age                           1  2479426316334 264977091919610 328960\n- ac_binary                     1  3047985297556 265545650900832 328990\n- basement_type                10  5090476653267 267588142256544 329078\n- bath_num                      1  6898841085908 269396506689184 329189\n- garage_num                    1  9751992536383 272249658139660 329336\n- fireplace_num                 1 10046915002573 272544580605849 329351\n- ln_square_feet                1 40814913228775 303312578832052 330836\n- neighborhood_fe             126 54479351260674 316977016863950 331197\n\nStep:  AIC=328826.8\nsale_price ~ ln_square_feet + bath_num + ac_binary + fireplace_num + \n    story_num + garage_num + basement_type + age + I(age^2) + \n    medhhinc + pct_vacant + pct_single_family + city_dist_mi + \n    septa_half_mi + ln_park_dist + knn_amenity_mi + neighborhood_fe + \n    quarters_fe + city_dist_mi:knn_amenity_mi + city_dist_mi:septa_half_mi\n\n                               Df      Sum of Sq             RSS    AIC\n- quarters_fe                   3    47663097715 262574620007222 328823\n- city_dist_mi:knn_amenity_mi   1     7728079582 262534684989088 328825\n- pct_single_family             1    19707606054 262546664515561 328826\n- ln_park_dist                  1    32231192502 262559188102009 328827\n- story_num                     1    33640799011 262560597708518 328827\n&lt;none&gt;                                           262526956909507 328827\n+ fuel_type                     3    29291306230 262497665603277 328831\n- pct_vacant                    1   231494066009 262758450975516 328837\n- medhhinc                      1  1068136777060 263595093686567 328881\n- I(age^2)                      1  1187398099796 263714355009303 328887\n- city_dist_mi:septa_half_mi    1  1235288890463 263762245799970 328890\n- age                           1  2473620812909 265000577722415 328955\n- ac_binary                     1  3111535277414 265638492186920 328988\n- basement_type                10  5116255704022 267643212613529 329075\n- bath_num                      1  6880305741509 269407262651016 329184\n- garage_num                    1  9775449552879 272302406462386 329332\n- fireplace_num                 1 10086445343028 272613402252535 329348\n- ln_square_feet                1 41033877166244 303560834075751 330841\n- neighborhood_fe             126 54518512496951 317045469406458 331194\n\nStep:  AIC=328823.3\nsale_price ~ ln_square_feet + bath_num + ac_binary + fireplace_num + \n    story_num + garage_num + basement_type + age + I(age^2) + \n    medhhinc + pct_vacant + pct_single_family + city_dist_mi + \n    septa_half_mi + ln_park_dist + knn_amenity_mi + neighborhood_fe + \n    city_dist_mi:knn_amenity_mi + city_dist_mi:septa_half_mi\n\n                               Df      Sum of Sq             RSS    AIC\n- city_dist_mi:knn_amenity_mi   1     7640688841 262582260696063 328822\n- pct_single_family             1    20067833883 262594687841105 328822\n- ln_park_dist                  1    32005690287 262606625697509 328823\n- story_num                     1    33899752615 262608519759837 328823\n&lt;none&gt;                                           262574620007222 328823\n+ quarters_fe                   3    47663097715 262526956909507 328827\n+ fuel_type                     3    28267117844 262546352889378 328828\n- pct_vacant                    1   232781450230 262807401457452 328834\n- medhhinc                      1  1070794141050 263645414148272 328878\n- I(age^2)                      1  1188059138702 263762679145925 328884\n- city_dist_mi:septa_half_mi    1  1240320543243 263814940550465 328887\n- age                           1  2476861470042 265051481477264 328952\n- ac_binary                     1  3114048174200 265688668181422 328985\n- basement_type                10  5101378305872 267675998313094 329070\n- bath_num                      1  6879515547403 269454135554625 329180\n- garage_num                    1  9761217229099 272335837236321 329328\n- fireplace_num                 1 10086091959024 272660711966247 329345\n- ln_square_feet                1 41027723058193 303602343065415 330837\n- neighborhood_fe             126 54533360011622 317107980018844 331191\n\nStep:  AIC=328821.7\nsale_price ~ ln_square_feet + bath_num + ac_binary + fireplace_num + \n    story_num + garage_num + basement_type + age + I(age^2) + \n    medhhinc + pct_vacant + pct_single_family + city_dist_mi + \n    septa_half_mi + ln_park_dist + knn_amenity_mi + neighborhood_fe + \n    city_dist_mi:septa_half_mi\n\n                               Df      Sum of Sq             RSS    AIC\n- knn_amenity_mi                1    10258397228 262592519093292 328820\n- pct_single_family             1    19589044073 262601849740136 328821\n- ln_park_dist                  1    31831658301 262614092354365 328821\n- story_num                     1    32448748353 262614709444417 328821\n&lt;none&gt;                                           262582260696063 328822\n+ city_dist_mi:knn_amenity_mi   1     7640688841 262574620007222 328823\n+ quarters_fe                   3    47575706975 262534684989088 328825\n+ fuel_type                     3    28489340452 262553771355611 328826\n- pct_vacant                    1   228890973776 262811151669839 328832\n- medhhinc                      1  1123486534387 263705747230451 328879\n- I(age^2)                      1  1189659359701 263771920055764 328882\n- city_dist_mi:septa_half_mi    1  1431158316586 264013419012649 328895\n- age                           1  2484992093025 265067252789089 328950\n- ac_binary                     1  3117445785849 265699706481912 328984\n- basement_type                10  5100919334738 267683180030802 329069\n- bath_num                      1  6872409243178 269454669939241 329178\n- garage_num                    1  9754485171729 272336745867792 329326\n- fireplace_num                 1 10078758344826 272661019040890 329343\n- ln_square_feet                1 41039873091006 303622133787069 330836\n- neighborhood_fe             126 54611581815584 317193842511648 331193\n\nStep:  AIC=328820.3\nsale_price ~ ln_square_feet + bath_num + ac_binary + fireplace_num + \n    story_num + garage_num + basement_type + age + I(age^2) + \n    medhhinc + pct_vacant + pct_single_family + city_dist_mi + \n    septa_half_mi + ln_park_dist + neighborhood_fe + city_dist_mi:septa_half_mi\n\n                              Df      Sum of Sq             RSS    AIC\n- pct_single_family            1    17516329942 262610035423233 328819\n- story_num                    1    31989214042 262624508307333 328820\n- ln_park_dist                 1    33881262942 262626400356233 328820\n&lt;none&gt;                                          262592519093292 328820\n+ knn_amenity_mi               1    10258397228 262582260696063 328822\n+ quarters_fe                  3    47914194039 262544604899253 328824\n+ fuel_type                    3    28552031911 262563967061380 328825\n- pct_vacant                   1   228071955219 262820591048511 328830\n- medhhinc                     1  1154301728799 263746820822091 328879\n- I(age^2)                     1  1187867988165 263780387081456 328881\n- city_dist_mi:septa_half_mi   1  1428441824517 264020960917808 328894\n- age                          1  2483425354156 265075944447448 328949\n- ac_binary                    1  3115319852620 265707838945911 328982\n- basement_type               10  5093400845586 267685919938877 329067\n- bath_num                     1  6873267530482 269465786623774 329177\n- garage_num                   1  9744591721973 272337110815265 329324\n- fireplace_num                1 10068500036056 272661019129348 329341\n- ln_square_feet               1 41051759221484 303644278314775 330835\n- neighborhood_fe            126 54934695719999 317527214813291 331205\n\nStep:  AIC=328819.2\nsale_price ~ ln_square_feet + bath_num + ac_binary + fireplace_num + \n    story_num + garage_num + basement_type + age + I(age^2) + \n    medhhinc + pct_vacant + city_dist_mi + septa_half_mi + ln_park_dist + \n    neighborhood_fe + city_dist_mi:septa_half_mi\n\n                              Df      Sum of Sq             RSS    AIC\n- story_num                    1    33952275751 262643987698984 328819\n- ln_park_dist                 1    35780677027 262645816100260 328819\n&lt;none&gt;                                          262610035423233 328819\n+ pct_single_family            1    17516329942 262592519093292 328820\n+ knn_amenity_mi               1     8185683097 262601849740136 328821\n+ quarters_fe                  3    48218416368 262561817006865 328823\n+ fuel_type                    3    28307547225 262581727876008 328824\n- pct_vacant                   1   225660530490 262835695953723 328829\n- I(age^2)                     1  1189715385678 263799750808911 328880\n- medhhinc                     1  1212143043197 263822178466430 328881\n- city_dist_mi:septa_half_mi   1  1465288887215 264075324310448 328894\n- age                          1  2483423804818 265093459228052 328948\n- ac_binary                    1  3125197709254 265735233132488 328981\n- basement_type               10  5157061941754 267767097364987 329069\n- bath_num                     1  6886217998671 269496253421905 329177\n- garage_num                   1  9861971915501 272472007338734 329329\n- fireplace_num                1 10154625988422 272764661411656 329344\n- ln_square_feet               1 41239244968237 303849280391471 330842\n- neighborhood_fe            126 54917360702735 317527396125968 331203\n\nStep:  AIC=328819\nsale_price ~ ln_square_feet + bath_num + ac_binary + fireplace_num + \n    garage_num + basement_type + age + I(age^2) + medhhinc + \n    pct_vacant + city_dist_mi + septa_half_mi + ln_park_dist + \n    neighborhood_fe + city_dist_mi:septa_half_mi\n\n                              Df      Sum of Sq             RSS    AIC\n- ln_park_dist                 1    37088779964 262681076478948 328819\n&lt;none&gt;                                          262643987698984 328819\n+ story_num                    1    33952275751 262610035423233 328819\n+ pct_single_family            1    19479391651 262624508307333 328820\n+ knn_amenity_mi               1     7661975952 262636325723033 328821\n+ quarters_fe                  3    48485012070 262595502686914 328822\n+ fuel_type                    3    27891291636 262616096407348 328824\n- pct_vacant                   1   223078062952 262867065761936 328829\n- I(age^2)                     1  1171351745449 263815339444434 328879\n- medhhinc                     1  1206630741698 263850618440683 328881\n- city_dist_mi:septa_half_mi   1  1487117530887 264131105229872 328895\n- age                          1  2478986749323 265122974448307 328947\n- ac_binary                    1  3116812315900 265760800014885 328981\n- basement_type               10  5124404684650 267768392383634 329067\n- bath_num                     1  6865474227828 269509461926812 329175\n- garage_num                   1 10077721127271 272721708826256 329340\n- fireplace_num                1 10194706863620 272838694562604 329346\n- ln_square_feet               1 46784996837845 309428984536830 331093\n- neighborhood_fe            126 54884887831956 317528875530940 331202\n\nStep:  AIC=328819\nsale_price ~ ln_square_feet + bath_num + ac_binary + fireplace_num + \n    garage_num + basement_type + age + I(age^2) + medhhinc + \n    pct_vacant + city_dist_mi + septa_half_mi + neighborhood_fe + \n    city_dist_mi:septa_half_mi\n\n                              Df      Sum of Sq             RSS    AIC\n&lt;none&gt;                                          262681076478948 328819\n+ ln_park_dist                 1    37088779964 262643987698984 328819\n+ story_num                    1    35260378688 262645816100260 328819\n+ pct_single_family            1    21554786315 262659521692633 328820\n+ knn_amenity_mi               1     9414440263 262671662038685 328820\n+ quarters_fe                  3    48291130287 262632785348661 328822\n+ fuel_type                    3    28384069019 262652692409930 328823\n- pct_vacant                   1   233928798083 262915005277031 328829\n- I(age^2)                     1  1166982321537 263848058800485 328878\n- medhhinc                     1  1199519985435 263880596464384 328880\n- city_dist_mi:septa_half_mi   1  1547382207846 264228458686795 328898\n- age                          1  2471158435449 265152234914397 328947\n- ac_binary                    1  3093083166434 265774159645382 328979\n- basement_type               10  5119477792449 267800554271397 329067\n- bath_num                     1  6862714567634 269543791046582 329175\n- garage_num                   1 10067293974161 272748370453110 329339\n- fireplace_num                1 10167733347582 272848809826531 329344\n- ln_square_feet               1 46823860752575 309504937231523 331094\n- neighborhood_fe            126 56125236061007 318806312539955 331255\n\n\n\n\nCode\n# Compute TSS\ny &lt;- model.response(model.frame(step_model))\ntss &lt;- sum((y - mean(y))^2)\n\n# Sequential (Type I) Sum Sq → ΔR²\nanova_model &lt;- anova(step_model)\nanova_model &lt;- anova_model[!is.na(anova_model$\"Sum Sq\"), , drop = FALSE]\nseq_imp &lt;- transform(anova_model,\n                     term = rownames(anova_model),\n                     delta_R2 = `Sum Sq` / tss)\n\n# Get top 5 predictors\nseq_top4 &lt;- seq_imp[order(-seq_imp$delta_R2), c(\"term\", \"Df\", \"delta_R2\")]\nhead(seq_top4, 5)\n\n\n                           term    Df   delta_R2\nln_square_feet   ln_square_feet     1 0.40682553\nResiduals             Residuals 13734 0.25010793\nneighborhood_fe neighborhood_fe   126 0.06662910\nmedhhinc               medhhinc     1 0.06411606\nbath_num               bath_num     1 0.05055231\n\n\n\n\nCode\n# Residual map preparation\n\n# Match CRS\nphilly_neighborhoods &lt;- st_transform(philly_neighborhoods, st_crs(final_data))\n\n# Spatial join: assign each point to a neighborhood\npoints_with_neighborhood &lt;- st_join(final_data, philly_neighborhoods)\n\n# Add residual column\npoints_with_neighborhood$residuals &lt;- residuals(final_model)\n\n# Calculate average residual by neighborhood\nneighborhood_residuals &lt;- points_with_neighborhood %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(MAPNAME) %&gt;%\n  summarise(\n    mean_residual = mean(residuals, na.rm = TRUE),\n    median_residual = median(residuals, na.rm = TRUE),\n    n_sales = n()\n  )\n\n# Join to neighborhoods\nneighborhoods_with_residuals &lt;- philly_neighborhoods %&gt;%\n  left_join(neighborhood_residuals, by = \"MAPNAME\")\n\n\n\n\nCode\n# Map the averaged residuals\n\nneighborhood_map &lt;- ggplot(neighborhoods_with_residuals) +\n  geom_sf(aes(fill = mean_residual), color = \"black\", size = 0.3) +\n  scale_fill_gradient2(\n    low = \"blue2\", \n    mid = \"#f5f4f0\", \n    high = \"red2\",\n    midpoint = 0,\n    labels = scales::dollar,\n    name = \"Mean Residual ($)\"\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Average Model Residuals by Neighborhood\",\n    subtitle = \"Red = Under-Predicted | Blue = Over-Predicted\"\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank()\n  )\n\nneighborhood_map\n\n\n\n\n\n\n\n\n\nCode\n#ggsave(\"slide_images/residual-neighborhood.png\", plot = neighborhood_map, width = 8, height = 6, units = \"in\", dpi = 300)\n\n\nUniversity City is the hardest to predict, Penn owns a lot of property that doesn’t get taxed. And some less wealthy and disinvested neighborhoods are overvalued, like Parkside and Wynnefield in West Philadelphia, but the overall model predicts pretty accurately for most neighborhoods in Philadelphia."
  },
  {
    "objectID": "assignments/assignment_3/Luu_Jun_Appendix.html#fold-cross-validation",
    "href": "assignments/assignment_3/Luu_Jun_Appendix.html#fold-cross-validation",
    "title": "Philadelphia Housing Model—Technical Appendix",
    "section": "10-Fold Cross-Validation",
    "text": "10-Fold Cross-Validation\n\nLogged Price Response Variable\n\n\nCode\nlibrary(caret)\n# 10-fold cross-validation\n# first model\nset.seed(0)\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10, savePredictions=\"final\")\n\ncv_log_1 &lt;- train(ln_sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2), # Age polynomial.\n                    data = final_data,\n                    method = \"lm\",\n                    trControl = train_control)\n\ncv_log_1$results\n\n\n  intercept      RMSE  Rsquared       MAE      RMSESD  RsquaredSD       MAESD\n1      TRUE 0.4155534 0.5775606 0.3037167 0.005840741 0.007099066 0.004524254\n\n\n\n\nCode\nlibrary(caret)\n# 10-fold cross-validation\n# second model\nset.seed(0)\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10, savePredictions=\"final\")\n\ncv_log_2 &lt;- train(ln_sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2) + # Age polynomial.\n                    medhhinc + pct_vacant + pct_single_family, # census\n                    data = final_data,\n                    method = \"lm\",\n                    trControl = train_control)\n\ncv_log_2$results\n\n\n  intercept      RMSE  Rsquared       MAE     RMSESD RsquaredSD       MAESD\n1      TRUE 0.3439329 0.7108162 0.2432055 0.01002661 0.01197928 0.004981254\n\n\n\n\nCode\nlibrary(caret)\n# 10-fold cross-validation\n# third model\nset.seed(0)\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10, savePredictions=\"final\")\n\ncv_log_3 &lt;- train(ln_sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2) + # Age polynomial.\n                    medhhinc + pct_vacant + pct_single_family +\n                    city_dist_mi + septa_half_mi + ln_park_dist + knn_amenity_mi, # Spatial\n                    data = final_data,\n                    method = \"lm\",\n                    trControl = train_control)\n\ncv_log_3$results\n\n\n  intercept      RMSE  Rsquared       MAE     RMSESD RsquaredSD       MAESD\n1      TRUE 0.3298712 0.7339455 0.2311302 0.01037016 0.01242474 0.004006271\n\n\n\n\nCode\nlibrary(caret)\n# 10-fold cross-validation\n# third model\nset.seed(0)\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10, savePredictions=\"final\")\n\ncv_log_4 &lt;- train(ln_sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2) + # Age polynomial.\n                    medhhinc + pct_vacant + pct_single_family +\n                    city_dist_mi + septa_half_mi + ln_park_dist + knn_amenity_mi + # Spatial\n                    knn_amenity_mi * city_dist_mi + septa_half_mi * city_dist_mi + # Interaction between amenities and downtown distance.\n                    neighborhood_fe, # Fixed effect  \n                    data = final_data,\n                    method = \"lm\",\n                    trControl = train_control)\n\ncv_log_4$results\n\n\n  intercept      RMSE  Rsquared      MAE     RMSESD RsquaredSD       MAESD\n1      TRUE 0.2890608 0.7955765 0.193469 0.01314438 0.01571413 0.004562659\n\n\n\n\nCode\n# Compare all 4 models\n\n## Combine four models：cv_model_1, cv_model_2, cv_model_3, cv_model_4\nlog_compare &lt;- bind_rows(\n  cv_log_1$results %&gt;% \n    mutate(Model = \"Model 1: Structural\"),\n  cv_log_2$results %&gt;% \n    mutate(Model = \"Model 2: Structural + Census\"),\n  cv_log_3$results %&gt;% \n    mutate(Model = \"Model 3: Structural + Census + Spatial\"),\n  cv_log_4$results %&gt;% \n    mutate(Model = \"Model 4: Final Model\")\n) %&gt;%\n  select(Model, RMSE, Rsquared, MAE) %&gt;% \n  mutate(across(c(RMSE, Rsquared, MAE), round, 3))\n\n## Plot\nlog_kable &lt;- kable(log_compare,\n                     col.names = c(\"Model\", \"RMSE\", \"R²\", \"MAE\"),\n                     caption = \"Log Model Performance Comparison (10-Fold Cross-Validation)\",\n                     digits = 4,\n                     format.args = list(big.mark = \",\")\n) %&gt;%\n  kable_styling(latex_options = \"striped\", full_width = FALSE) %&gt;%\n  column_spec(1, bold = TRUE, width = \"10cm\") %&gt;%\n  row_spec(0, color = \"#f5f4f0\", background = \"#ff4100\")\n\nlog_kable\n\n\n\nLog Model Performance Comparison (10-Fold Cross-Validation)\n\n\nModel\nRMSE\nR²\nMAE\n\n\n\n\nModel 1: Structural\n0.416\n0.578\n0.304\n\n\nModel 2: Structural + Census\n0.344\n0.711\n0.243\n\n\nModel 3: Structural + Census + Spatial\n0.330\n0.734\n0.231\n\n\nModel 4: Final Model\n0.289\n0.796\n0.193\n\n\n\n\n\n\n\nUnlogged Price Response Variable\n\n\nCode\nlibrary(caret)\n# 10-fold cross-validation\n# first model\nset.seed(0)\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10, savePredictions=\"final\")\n\ncv_model_1 &lt;- train(sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2), # Age polynomial.\n                    data = final_data,\n                    method = \"lm\",\n                    trControl = train_control)\n\ncv_model_1$results\n\n\n  intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n1      TRUE 176344.9 0.5890063 102820.4 17348.48 0.04466116 3386.644\n\n\n\n\nCode\nlibrary(caret)\n# 10-fold cross-validation\n# second model\nset.seed(0)\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10, savePredictions=\"final\")\n\ncv_model_2 &lt;- train(sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2) + # Age polynomial.\n                    medhhinc + pct_vacant + pct_single_family, # census\n                    data = final_data,\n                    method = \"lm\",\n                    trControl = train_control)\n\ncv_model_2$results\n\n\n  intercept     RMSE  Rsquared      MAE  RMSESD RsquaredSD    MAESD\n1      TRUE 161630.5 0.6544361 88848.47 20418.2 0.05751453 3576.953\n\n\n\n\nCode\nlibrary(caret)\n# 10-fold cross-validation\n# third model\nset.seed(0)\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10, savePredictions=\"final\")\n\ncv_model_3 &lt;- train(sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2) + # Age polynomial.\n                    medhhinc + pct_vacant + pct_single_family +\n                    city_dist_mi + septa_half_mi + ln_park_dist + knn_amenity_mi, # Spatial\n                    data = final_data,\n                    method = \"lm\",\n                    trControl = train_control)\n\ncv_model_3$results\n\n\n  intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n1      TRUE 154443.5 0.6843236 84936.68 20818.25 0.05820868 3103.814\n\n\n\n\nCode\nlibrary(caret)\n# 10-fold cross-validation\n# third model\nset.seed(0)\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10, savePredictions=\"final\")\n\ncv_model_4 &lt;- train(sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2) + # Age polynomial.\n                    medhhinc + pct_vacant + pct_single_family +\n                    city_dist_mi + septa_half_mi + ln_park_dist + knn_amenity_mi + # Spatial\n                    knn_amenity_mi * city_dist_mi + septa_half_mi * city_dist_mi + # Interaction between amenities and downtown distance.\n                    neighborhood_fe, # Fixed effect  \n                    data = final_data,\n                    method = \"lm\",\n                    trControl = train_control)\n\ncv_model_4$results\n\n\n  intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n1      TRUE 138279.4 0.7459614 72567.76 22998.06 0.06401242 2898.615\n\n\n\n\nCode\n# Compare all 4 models\n\n## Combine four models：cv_model_1, cv_model_2, cv_model_3, cv_model_4\nmodel_compare &lt;- bind_rows(\n  cv_model_1$results %&gt;% \n    mutate(Model = \"Model 1: Structural\"),\n  cv_model_2$results %&gt;% \n    mutate(Model = \"Model 2: Structural + Census\"),\n  cv_model_3$results %&gt;% \n    mutate(Model = \"Model 3: Structural + Census + Spatial\"),\n  cv_model_4$results %&gt;% \n    mutate(Model = \"Model 4: Final Model\")\n) %&gt;%\n  select(Model, RMSE, Rsquared, MAE) %&gt;% \n  mutate(across(c(RMSE, Rsquared, MAE), round, 3))\n\n## Plot\nmodel_kable &lt;- kable(model_compare,\n                     col.names = c(\"Model\", \"RMSE ($)\", \"R²\", \"MAE ($)\"),\n                     caption = \"Model Performance Comparison (10-Fold Cross-Validation)\",\n                     digits = 4,\n                     format.args = list(big.mark = \",\")\n) %&gt;%\n  kable_styling(latex_options = \"striped\", full_width = FALSE) %&gt;%\n  column_spec(1, bold = TRUE, width = \"10cm\") %&gt;%\n  row_spec(0, color = \"#f5f4f0\", background = \"#ff4100\")\n\nmodel_kable\n\n\n\nModel Performance Comparison (10-Fold Cross-Validation)\n\n\nModel\nRMSE ($)\nR²\nMAE ($)\n\n\n\n\nModel 1: Structural\n176,344.9\n0.589\n102,820.38\n\n\nModel 2: Structural + Census\n161,630.5\n0.654\n88,848.47\n\n\nModel 3: Structural + Census + Spatial\n154,443.5\n0.684\n84,936.68\n\n\nModel 4: Final Model\n138,279.4\n0.746\n72,567.76\n\n\n\n\n\n\n\nCode\n# Create predicted vs. actual plot\n\npred_df &lt;- cv_model_4$pred\n\n# Plot Predicted vs Actual \npred_v_act &lt;- ggplot(pred_df, aes(x = obs, y = pred)) +\n  geom_point(alpha = 0.5, color = \"#2C7BB6\") +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Predicted vs. Actual Sale Price\",\n    x = \"Actual Sale Price ($)\",\n    y = \"Predicted Sale Price ($)\"\n  ) +\n  scale_x_continuous(labels = scales::dollar) +\n  scale_y_continuous(labels = scales::dollar) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14, margin = margin(b = 10)),\n    panel.grid.minor = element_blank(),\n    axis.title.x = element_text(margin = margin(t = 10)),\n    axis.title.y = element_text(margin = margin(r = 10))\n  )\n\npred_v_act"
  },
  {
    "objectID": "assignments/assignment_3/Luu_Jun_Appendix.html#check-assumptions",
    "href": "assignments/assignment_3/Luu_Jun_Appendix.html#check-assumptions",
    "title": "Philadelphia Housing Model—Technical Appendix",
    "section": "Check Assumptions",
    "text": "Check Assumptions\n\n\nCode\nresid_df &lt;- data.frame(\n  fitted = fitted(final_model),\n  residuals = resid(final_model)\n)\n\nresid_v_fit &lt;- ggplot(resid_df, aes(x = fitted, y = residuals)) +\n  geom_point(alpha = 0.5, color = \"#2C7BB6\") +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Residuals vs. Fitted Values\",\n    x = \"Fitted Values (Predicted Sale Price)\",\n    y = \"Residuals\"\n  ) +\n  scale_x_continuous(labels = scales::dollar) +\n  scale_y_continuous(labels = scales::dollar) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14, margin = margin(b = 10)),\n    panel.grid.minor = element_blank(),\n    axis.title.x = element_text(margin = margin(t = 10)),\n    axis.title.y = element_text(margin = margin(r = 10))\n  )\n\nresid_v_fit\n\n\n\n\n\n\n\n\n\nResiduals vs Fitted Values:\nThe residual plot shows that most residuals are centered around zero, but the spread of residuals increases as the fitted values grow. This “funnel-shaped” pattern suggests potential heteroskedasticity, meaning the variance of errors may increase for higher-priced properties. While the overall linearity assumption appears reasonable, the increasing dispersion indicates that the model’s prediction error is not constant across the price range.\n\n\nCode\nresid_df &lt;- data.frame(residuals = residuals(final_model))\n\n# Q-Q Plot\nqq_plot &lt;- ggplot(resid_df, aes(sample = residuals)) +\n  stat_qq(color = \"#2C7BB6\", alpha = 0.6, size = 2) +      \n  stat_qq_line(color = \"red\", linetype = \"dashed\") +        \n  labs(\n    title = \"Normal Q-Q Plot of Residuals\",\n    subtitle = \"Check for normality assumption.\",\n    x = \"Theoretical Quantiles (Normal Distribution)\",\n    y = \"Sample Quantiles (Residuals)\"\n  ) +\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::dollar) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14, margin = margin(b = 10)),\n    plot.subtitle = element_text(size = 11),\n    panel.grid.minor = element_blank(),\n    axis.title.x = element_text(margin = margin(t = 10)),\n    axis.title.y = element_text(margin = margin(r = 10))\n  )\n\nqq_plot\n\n\n\n\n\n\n\n\n\nNormal Q–Q Plot:\nThe Q–Q plot reveals that the residuals deviate from the reference line in both tails, especially in the upper tail. This pattern indicates that the residuals are right-skewed and not perfectly normally distributed. The deviation is mainly driven by a small number of very high sale-price observations, which pull the residual distribution upward. However, moderate departures from normality are common in housing price data and generally do not invalidate the model.\n\n\nCode\ncd &lt;- cooks.distance(final_model)\n\nused_row_idx &lt;- as.integer(rownames(model.frame(final_model)))\n\ncooks_df &lt;- tibble(\n  row_in_data  = used_row_idx,           \n  row_in_model = seq_along(cd),          \n  cooks_distance = as.numeric(cd)\n)\n\nn_used &lt;- length(cd)\nthreshold &lt;- 4 / n_used\n\ncooks_plot &lt;- ggplot(cooks_df, aes(x = row_in_model, y = cooks_distance)) +\n  geom_bar(stat = \"identity\", fill = \"#2C7BB6\", alpha = 0.6) +\n  geom_hline(yintercept = threshold, color = \"red\", linetype = \"dashed\") +\n  coord_cartesian(ylim = c(0, 0.02)) +  \n  labs(\n    title = \"Cook's Distance (Zoomed In)\",\n    subtitle = paste0(\"Red Dashed Line = 4/n ≈ \", round(threshold, 5)),\n    x = \"Observation Index (In-Model)\",\n    y = \"Cook's Distance\"\n  ) +\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14, margin = margin(b = 10)),\n    plot.subtitle = element_text(size = 11),\n    panel.grid.minor = element_blank(),\n    axis.title.x = element_text(margin = margin(t = 10)),\n    axis.title.y = element_text(margin = margin(r = 10))\n  )\n\ncooks_plot\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Most influential\ntop_influential &lt;- cooks_df %&gt;%\n  filter(cooks_distance &gt; threshold) %&gt;%\n  arrange(desc(cooks_distance)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(\n    sale_price = final_data$sale_price[row_in_data]   \n  ) %&gt;%\n  select(row_in_model, row_in_data, sale_price, cooks_distance)\n\ntop_influential\n\n\n# A tibble: 10 × 4\n   row_in_model row_in_data sale_price cooks_distance\n          &lt;int&gt;       &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n 1         6113        6113    3600000         0.197 \n 2         6383        6383    5477901         0.129 \n 3          198         198    3330400         0.0459\n 4         8210        8211     480000         0.0454\n 5         2496        2496    3995000         0.0395\n 6        10469       10470     281000         0.0356\n 7         7913        7914     330000         0.0325\n 8         2620        2620    3000000         0.0283\n 9         1921        1921     170000         0.0240\n10         5439        5439    3850000         0.0233\n\n\nCook’s Distance:\nThe Cook’s distance plot shows that almost all observations have very small influence values (below the 4/n threshold), indicating that the model is not dominated by a few extreme points. A few cases exhibit slightly higher Cook’s D values, suggesting the presence of mildly influential outliers, but none appear to exert excessive leverage on the regression coefficients."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Welcome to my MUSA 5080 Portfolio! Please follow me on my journey through Public Policy Analytics (MUSA 5080). Hopefully I will be able to analyze spatial data and create meaningful visualizations at the end of this course :O\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nAssignments: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\nHi, my name is Jun and I recently started my Masters of Urban Spatial Analytics at The University of Pennsylvania! My background is in affordable housing and community development construction engineering. Though I loved my work, I decided that I wanted to do research that would push new projects that I believe in instead of building the projects that had already been determined.\nTo learn more about my background, please refer to the “About” section located in the banner across the top of this page!\n\n\n\n\nEmail: jjluu@upenn.edu\nGitHub: jenniferluu6"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nAssignments: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Hi, my name is Jun and I recently started my Masters of Urban Spatial Analytics at The University of Pennsylvania! My background is in affordable housing and community development construction engineering. Though I loved my work, I decided that I wanted to do research that would push new projects that I believe in instead of building the projects that had already been determined.\nTo learn more about my background, please refer to the “About” section located in the banner across the top of this page!"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: jjluu@upenn.edu\nGitHub: jenniferluu6"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3 notes",
    "section": "",
    "text": "Anscombe’s Quartet and the limits of summary statistics\n- Depending on the way information is visualized, it can show completely different patterns.\n- “How to lie with stats/maps”\n- Outliers may represent important communities\n- Relationships aren’t always linear\nExploratory Data Analysis is detective work\nData → Aesthetics → Geometries → Visual\n- Data: Your data set (census data, survey responses, etc.)\n- Aesthetics: What variables map to visual properties (x, y, color, size)\n- Geometries: How to display the data (points, bars, lines)\n- Additional layers: Scales, themes, facets, annotations"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "title": "Week 3 notes",
    "section": "",
    "text": "Anscombe’s Quartet and the limits of summary statistics\n- Depending on the way information is visualized, it can show completely different patterns.\n- “How to lie with stats/maps”\n- Outliers may represent important communities\n- Relationships aren’t always linear\nExploratory Data Analysis is detective work\nData → Aesthetics → Geometries → Visual\n- Data: Your data set (census data, survey responses, etc.)\n- Aesthetics: What variables map to visual properties (x, y, color, size)\n- Geometries: How to display the data (points, bars, lines)\n- Additional layers: Scales, themes, facets, annotations"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#coding-techniques",
    "href": "weekly-notes/week-03-notes.html#coding-techniques",
    "title": "Week 3 notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\nggplot(data = your_data) +   aes(x = variable1, y = variable2) +   geom_something() +   additional_layers()\nAesthetics map data to visual properties:\n\nx, y - position\ncolor - point/line color\nfill - area fill color\nsize - point/line size\nshape - point shape\nalpha - transparency\nleft_join() - Keep all rows from left dataset\nright_join() - Keep all rows from right dataset\ninner_join() - Keep only rows that match in both\nfull_join() - Keep all rows from both datasets\n\nImportant: Aesthetics go inside aes(), constants go outside"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#questions-challenges",
    "href": "weekly-notes/week-03-notes.html#questions-challenges",
    "title": "Week 3 notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\nPattern: Smaller populations have higher uncertainty\nEthical implication: These communities might be systematically under counted\n\n\nReport the corresponding MOEs of ACS estimates - Always include margin of error values\nInclude a footnote when not reporting MOEs - Explicitly acknowledge omission\n\nProvide context for (un)reliability - Use coefficient of variation (CV):\n\nCV &lt; 12% = reliable (green coding)\nCV 12-40% = somewhat reliable (yellow)\nCV &gt; 40% = unreliable (red coding)\n\nReduce statistical uncertainty - Collapse data detail, aggregate geographies, use multi-year estimates\nAlways conduct statistical significance tests when comparing ACS estimates over time"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#connections-to-policy",
    "href": "weekly-notes/week-03-notes.html#connections-to-policy",
    "title": "Week 3 notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\nResearch finding: Only 27% of planners warn users about unreliable ACS data\n- Most planners don’t report margins of error\n- Many lack training on statistical uncertainty\nCommon problems in government data presentation:\n- Misleading scales or axes\n- Cherry-picked time periods\n- Hidden or ignored uncertainty\n- Missing context about data reliability"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#reflection",
    "href": "weekly-notes/week-03-notes.html#reflection",
    "title": "Week 3 notes",
    "section": "Reflection",
    "text": "Reflection\nReminder to study through definitions more closely."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 notes",
    "section": "",
    "text": "What is an algorithm?\nA set of rules or instructions for solving a problem or completing a task\n\nInputs (“features”, “predictors”, “independent variables”, “x”)\nOutputs (“labels”, “outcome”, “dependent variable”, “y”)\n\nData Science - Computer science/engineering focus on algorithms and methods\nData Analytics - Application of data science methods to other disciplines\nMachine Learning - Algorithms for classification & prediction that learn from data\nAI - Algorithms that adjust and improve across iterations (neural networks, etc.)\nTIGER/Line Files\n\nGeographic boundaries (shapefiles)\nCensus tracts, counties, states\nNow released as shapefiles (easier to use!)\n\nHistorical Data Sources:\n\nNHGIS (nhgis.org) - Historical census data\nNeighborhood Change Database\nLongitudinal Tract Database - Track changes over time"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 2 notes",
    "section": "",
    "text": "What is an algorithm?\nA set of rules or instructions for solving a problem or completing a task\n\nInputs (“features”, “predictors”, “independent variables”, “x”)\nOutputs (“labels”, “outcome”, “dependent variable”, “y”)\n\nData Science - Computer science/engineering focus on algorithms and methods\nData Analytics - Application of data science methods to other disciplines\nMachine Learning - Algorithms for classification & prediction that learn from data\nAI - Algorithms that adjust and improve across iterations (neural networks, etc.)\nTIGER/Line Files\n\nGeographic boundaries (shapefiles)\nCensus tracts, counties, states\nNow released as shapefiles (easier to use!)\n\nHistorical Data Sources:\n\nNHGIS (nhgis.org) - Historical census data\nNeighborhood Change Database\nLongitudinal Tract Database - Track changes over time"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(knitr)\n\ncase_when()\nget_acs()\nmutate()\nselect()\nstr_remove(), str_extract(), str_replace()\nkable()"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\nI need to finish Lab 0, so that I can practice the different coding functions."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nProxy: What would you use to stand in for what you want?\nBlind spot: What data gap or historical bias could skew results?\nHarm + Guardrail: Who could be harmed, and one simple safeguard?\n\n\nCriminal Justice - Recidivism risk scores for bail and sentencing decisions\nHousing & Finance - Mortgage lending and tenant screening algorithms\nHealthcare - Patient care prioritization and resource allocation\n\nLong history of government data collection:\n\nCivic registration systems\nCensus data\nAdministrative records\nOperations research (post-WWII)\n\nCensus data is the foundation for:\n\nUnderstanding community demographics\nAllocating government resources\n\nTracking neighborhood change"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 notes",
    "section": "Reflection",
    "text": "Reflection\nI think it is pretty nerve-racking thinking about how the data that I have/choose to use could affect the outcome of someone’s life. It makes me want to be as careful as possible to minimize bias as much as possible. This has made me very slow to answer data analysis questions in class."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Git - tracks changes, collaboration tool for teams\n\nrepo - folder containing your project files\npush - sends changes to GitHub\npull - get the latest changes fro GitHub\n\nGitHub - cloud hosting for Git repos, creates deployable websites to share projects\nGitHub Classroom- creates individual repos for students\nWhy Quarto:\n\nreproducible research:\n\ncode + explanation in one place\nothers can run your analysis\n\ncareer relevance:\n\nindustry standard\n\n\nWhy R:\n\nfree & open source\nreproducible research\nindustry standard\n\nTidyverse - uses “tibbles” (enhanced data frames)"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Git - tracks changes, collaboration tool for teams\n\nrepo - folder containing your project files\npush - sends changes to GitHub\npull - get the latest changes fro GitHub\n\nGitHub - cloud hosting for Git repos, creates deployable websites to share projects\nGitHub Classroom- creates individual repos for students\nWhy Quarto:\n\nreproducible research:\n\ncode + explanation in one place\nothers can run your analysis\n\ncareer relevance:\n\nindustry standard\n\n\nWhy R:\n\nfree & open source\nreproducible research\nindustry standard\n\nTidyverse - uses “tibbles” (enhanced data frames)"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\nConvert data frame to tibble:\n#Traditional Data Frame\nclass(data)\n# Convert to tibble\ncar_data &lt;- as_tibble(data)\nclass(car_data)\nCommonly used:\n\nselect() - choose columns\nfilter() - choose rows\n\nmutate() - create new variables\nsummarize() - calculate statistics\ngroup_by() - operate on groups"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNeed to practice using pipelines!"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\nThis semester we’ll use these skills for:\n\nCensus data analysis\nNeighborhood change studies\nPredictive modeling for resource allocation\nHousing market analysis\nTransportation equity assessment"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\nI am excited to learn how to use these things that we have learned on a project. I am interested in what a pipeline will look like for me."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html",
    "href": "weekly-notes/week-06-notes.html",
    "title": "Week 6 notes",
    "section": "",
    "text": "The (n-1) Rule: One neighborhood is automatically chosen as the reference category (omitted)! R picks the first alphabetically unless you specify otherwise.\n\n\nReference Category: Allston (automatically chosen - alphabetically first) Structural Variables: - Living Area: Each additional sq ft adds this amount (same for all neighborhoods) - Bedrooms: Effect of one more full bathroom (same for all neighborhoods) Neighborhood Dummies: - Positive coefficient = This neighborhood is MORE expensive than r ref_neighborhood - Negative coefficient = This neighborhood is LESS expensive than r ref_neighborhood - All else equal (same size, same bathrooms)\nSigns of Non-Linearity:\n\nCurved residual plots\nDiminishing returns\nAccelerating effects\nU-shaped or inverted-U patterns\nTheoretical reasons\n\n⚠️ Can’t Interpret Coefficients Directly!\nWith Age², the effect of age is no longer constant. You need to calculate the marginal effect. Marginal effect of Age = β₁ + 2×β₂×Age This means the effect changes at every age!\n\n\n\n\n\n\n\n\n\nNote“Everything is related to everything else, but near things are more related than distant things”\n\n\n\n- Waldo Tobler, 1970\n\n\n\nCrime nearby matters more than crime across the city\nParks within walking distance affect value\nYour immediate neighborhood defines your market\n\n\n\n\n\n\nCount or sum events within a defined distance\nExample: Number of crimes within 500 feet\n\n\n\nAverage distance to k closest events\nExample: Average distance to 3 nearest violent crimes\n\n\n\nStraight-line distance to important locations\nExample: Distance to downtown, nearest T station Fixed Effects = Categorical variables that capture all unmeasured characteristics of a group\nIn hedonic models:\n\nEach neighborhood gets its own dummy variable\nCaptures everything unique about that neighborhood we didn’t explicitly measure\n\nThree common validation approaches:\n\nTrain/Test Split - 80/20 split, simple but unstable\nk-Fold Cross-Validation - Split into k folds, train on k-1, test on 1, repeat\nLOOCV - Leave one observation out at a time (special case of k-fold)\n\nWhy CV?\n\nTells us how well model predicts NEW data\nMore honest than in-sample R²\nHelps detect overfitting\n\n\n\n\n\n\n\nWhat happened?\n\nRandom split created 10 folds\nAll “West End” sales ended up in ONE fold (the test fold)\nTraining folds never saw “West End”\nModel can’t predict for a category it never learned\n\n\n\n\n\n\n\nImportantThe Issue\n\n\n\nWhen neighborhoods have very few sales (&lt;10), random CV splits can put all instances in the same fold, breaking the model.\n\n\n\n\n\n4326 = WGS84 (lat/lon in degrees) - fine for display\nESRI:102286 = MA State Plane (feet) - good for distance calculations\n\n\n\n\n\n\n\n\nAdd 50 variables at once\nIgnore errors\nForget st_drop_geometry() for non-spatial operations\nSkip sparse category check\n\n\n\n\n\n\n\n\n\n“Factor has new levels” → Group sparse categories\n“Computationally singular” → Remove collinear variables\nVery high RMSE → Check outliers, scale\nCV takes forever → Simplify model or reduce folds\nNegative R² → Model worse than mean, rethink variables\n\n\n\nExample Scenarios: - Housing: Does square footage matter more in wealthy neighborhoods? - Education: Do tutoring effects vary by initial skill level? - Public Health: Do pollution effects differ by age?"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-06-notes.html#key-concepts-learned",
    "title": "Week 6 notes",
    "section": "",
    "text": "The (n-1) Rule: One neighborhood is automatically chosen as the reference category (omitted)! R picks the first alphabetically unless you specify otherwise.\n\n\nReference Category: Allston (automatically chosen - alphabetically first) Structural Variables: - Living Area: Each additional sq ft adds this amount (same for all neighborhoods) - Bedrooms: Effect of one more full bathroom (same for all neighborhoods) Neighborhood Dummies: - Positive coefficient = This neighborhood is MORE expensive than r ref_neighborhood - Negative coefficient = This neighborhood is LESS expensive than r ref_neighborhood - All else equal (same size, same bathrooms)\nSigns of Non-Linearity:\n\nCurved residual plots\nDiminishing returns\nAccelerating effects\nU-shaped or inverted-U patterns\nTheoretical reasons\n\n⚠️ Can’t Interpret Coefficients Directly!\nWith Age², the effect of age is no longer constant. You need to calculate the marginal effect. Marginal effect of Age = β₁ + 2×β₂×Age This means the effect changes at every age!\n\n\n\n\n\n\n\n\n\nNote“Everything is related to everything else, but near things are more related than distant things”\n\n\n\n- Waldo Tobler, 1970\n\n\n\nCrime nearby matters more than crime across the city\nParks within walking distance affect value\nYour immediate neighborhood defines your market\n\n\n\n\n\n\nCount or sum events within a defined distance\nExample: Number of crimes within 500 feet\n\n\n\nAverage distance to k closest events\nExample: Average distance to 3 nearest violent crimes\n\n\n\nStraight-line distance to important locations\nExample: Distance to downtown, nearest T station Fixed Effects = Categorical variables that capture all unmeasured characteristics of a group\nIn hedonic models:\n\nEach neighborhood gets its own dummy variable\nCaptures everything unique about that neighborhood we didn’t explicitly measure\n\nThree common validation approaches:\n\nTrain/Test Split - 80/20 split, simple but unstable\nk-Fold Cross-Validation - Split into k folds, train on k-1, test on 1, repeat\nLOOCV - Leave one observation out at a time (special case of k-fold)\n\nWhy CV?\n\nTells us how well model predicts NEW data\nMore honest than in-sample R²\nHelps detect overfitting\n\n\n\n\n\n\n\nWhat happened?\n\nRandom split created 10 folds\nAll “West End” sales ended up in ONE fold (the test fold)\nTraining folds never saw “West End”\nModel can’t predict for a category it never learned\n\n\n\n\n\n\n\nImportantThe Issue\n\n\n\nWhen neighborhoods have very few sales (&lt;10), random CV splits can put all instances in the same fold, breaking the model.\n\n\n\n\n\n4326 = WGS84 (lat/lon in degrees) - fine for display\nESRI:102286 = MA State Plane (feet) - good for distance calculations\n\n\n\n\n\n\n\n\nAdd 50 variables at once\nIgnore errors\nForget st_drop_geometry() for non-spatial operations\nSkip sparse category check\n\n\n\n\n\n\n\n\n\n“Factor has new levels” → Group sparse categories\n“Computationally singular” → Remove collinear variables\nVery high RMSE → Check outliers, scale\nCV takes forever → Simplify model or reduce folds\nNegative R² → Model worse than mean, rethink variables\n\n\n\nExample Scenarios: - Housing: Does square footage matter more in wealthy neighborhoods? - Education: Do tutoring effects vary by initial skill level? - Public Health: Do pollution effects differ by age?"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#three-approaches-to-spatial-features",
    "href": "weekly-notes/week-06-notes.html#three-approaches-to-spatial-features",
    "title": "Week 6 notes",
    "section": "",
    "text": "Count or sum events within a defined distance\nExample: Number of crimes within 500 feet\n\n\n\nAverage distance to k closest events\nExample: Average distance to 3 nearest violent crimes\n\n\n\nStraight-line distance to important locations\nExample: Distance to downtown, nearest T station Fixed Effects = Categorical variables that capture all unmeasured characteristics of a group\nIn hedonic models:\n\nEach neighborhood gets its own dummy variable\nCaptures everything unique about that neighborhood we didn’t explicitly measure\n\nThree common validation approaches:\n\nTrain/Test Split - 80/20 split, simple but unstable\nk-Fold Cross-Validation - Split into k folds, train on k-1, test on 1, repeat\nLOOCV - Leave one observation out at a time (special case of k-fold)\n\nWhy CV?\n\nTells us how well model predicts NEW data\nMore honest than in-sample R²\nHelps detect overfitting"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#the-problem-sparse-categories",
    "href": "weekly-notes/week-06-notes.html#the-problem-sparse-categories",
    "title": "Week 6 notes",
    "section": "",
    "text": "What happened?\n\nRandom split created 10 folds\nAll “West End” sales ended up in ONE fold (the test fold)\nTraining folds never saw “West End”\nModel can’t predict for a category it never learned\n\n\n\n\n\n\n\nImportantThe Issue\n\n\n\nWhen neighborhoods have very few sales (&lt;10), random CV splits can put all instances in the same fold, breaking the model.\n\n\n\n\n\n4326 = WGS84 (lat/lon in degrees) - fine for display\nESRI:102286 = MA State Plane (feet) - good for distance calculations\n\n\n\n\n\n\n\n\nAdd 50 variables at once\nIgnore errors\nForget st_drop_geometry() for non-spatial operations\nSkip sparse category check\n\n\n\n\n\n\n\n\n\n“Factor has new levels” → Group sparse categories\n“Computationally singular” → Remove collinear variables\nVery high RMSE → Check outliers, scale\nCV takes forever → Simplify model or reduce folds\nNegative R² → Model worse than mean, rethink variables\n\n\n\nExample Scenarios: - Housing: Does square footage matter more in wealthy neighborhoods? - Education: Do tutoring effects vary by initial skill level? - Public Health: Do pollution effects differ by age?"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#coding-techniques",
    "href": "weekly-notes/week-06-notes.html#coding-techniques",
    "title": "Week 6 notes",
    "section": "",
    "text": "4326 = WGS84 (lat/lon in degrees) - fine for display\nESRI:102286 = MA State Plane (feet) - good for distance calculations"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#questions-challenges",
    "href": "weekly-notes/week-06-notes.html#questions-challenges",
    "title": "Week 6 notes",
    "section": "",
    "text": "Add 50 variables at once\nIgnore errors\nForget st_drop_geometry() for non-spatial operations\nSkip sparse category check"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#reflection",
    "href": "weekly-notes/week-06-notes.html#reflection",
    "title": "Week 6 notes",
    "section": "",
    "text": "Example Scenarios: - Housing: Does square footage matter more in wealthy neighborhoods? - Education: Do tutoring effects vary by initial skill level? - Public Health: Do pollution effects differ by age?"
  }
]